{"documents": [{"url": "https://developer.apple.com/documentation/visionos", "title": "visionOS | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space  / visionOS API Changes: None visionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.  Developing for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app\u2019s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS. Start with a familiar window-based experience to introduce people to your content. From there, add SwiftUI scene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences. Build your app\u2019s 3D content with RealityKit and Reality Composer Pro, and display it with a RealityView. In an immersive experience, use ARKit to integrate your content with the person\u2019s surroundings.  People can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures. SwiftUI provides built-in support for these standard gestures, so rely on them for most of your app\u2019s input. When you want to go beyond the standard gestures, use ARKit to create custom gestures. Tap to select Pinch to rotate Manipulate objects Create custom gestures Explore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash. visionOS Overview Expand your app into immersive spaces Explore new kinds of interaction Dive into featured sample apps Topics Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos#Overview", "title": "visionOS | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space  / visionOS API Changes: None visionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.  Developing for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app\u2019s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS. Start with a familiar window-based experience to introduce people to your content. From there, add SwiftUI scene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences. Build your app\u2019s 3D content with RealityKit and Reality Composer Pro, and display it with a RealityView. In an immersive experience, use ARKit to integrate your content with the person\u2019s surroundings.  People can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures. SwiftUI provides built-in support for these standard gestures, so rely on them for most of your app\u2019s input. When you want to go beyond the standard gestures, use ARKit to create custom gestures. Tap to select Pinch to rotate Manipulate objects Create custom gestures Explore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash. visionOS Overview Expand your app into immersive spaces Explore new kinds of interaction Dive into featured sample apps Topics Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos#Expand-your-app-into-immersive-spaces", "title": "visionOS | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space  / visionOS API Changes: None visionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.  Developing for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app\u2019s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS. Start with a familiar window-based experience to introduce people to your content. From there, add SwiftUI scene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences. Build your app\u2019s 3D content with RealityKit and Reality Composer Pro, and display it with a RealityView. In an immersive experience, use ARKit to integrate your content with the person\u2019s surroundings.  People can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures. SwiftUI provides built-in support for these standard gestures, so rely on them for most of your app\u2019s input. When you want to go beyond the standard gestures, use ARKit to create custom gestures. Tap to select Pinch to rotate Manipulate objects Create custom gestures Explore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash. visionOS Overview Expand your app into immersive spaces Explore new kinds of interaction Dive into featured sample apps Topics Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos#Explore-new-kinds-of-interaction", "title": "visionOS | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space  / visionOS API Changes: None visionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.  Developing for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app\u2019s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS. Start with a familiar window-based experience to introduce people to your content. From there, add SwiftUI scene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences. Build your app\u2019s 3D content with RealityKit and Reality Composer Pro, and display it with a RealityView. In an immersive experience, use ARKit to integrate your content with the person\u2019s surroundings.  People can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures. SwiftUI provides built-in support for these standard gestures, so rely on them for most of your app\u2019s input. When you want to go beyond the standard gestures, use ARKit to create custom gestures. Tap to select Pinch to rotate Manipulate objects Create custom gestures Explore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash. visionOS Overview Expand your app into immersive spaces Explore new kinds of interaction Dive into featured sample apps Topics Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos#Dive-into-featured-sample-apps", "title": "visionOS | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space  / visionOS API Changes: None visionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.  Developing for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app\u2019s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS. Start with a familiar window-based experience to introduce people to your content. From there, add SwiftUI scene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences. Build your app\u2019s 3D content with RealityKit and Reality Composer Pro, and display it with a RealityView. In an immersive experience, use ARKit to integrate your content with the person\u2019s surroundings.  People can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures. SwiftUI provides built-in support for these standard gestures, so rely on them for most of your app\u2019s input. When you want to go beyond the standard gestures, use ARKit to create custom gestures. Tap to select Pinch to rotate Manipulate objects Create custom gestures Explore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash. visionOS Overview Expand your app into immersive spaces Explore new kinds of interaction Dive into featured sample apps Topics Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/world", "title": "Hello World | Apple Developer Documentation", "content": "App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space   You can use visionOS scene types and styles to share information in fun and compelling ways. Features like volumes and immersive spaces let you put interactive virtual objects into people\u2019s environments, or put people into a virtual environment. Hello World uses these tools to teach people about the Earth \u2014 the planet we call home. The app shows how the Earth\u2019s tilt creates the seasons, how objects move as they orbit the Earth, and how Earth appears from space. The app uses SwiftUI to define its interface, including both 2D and 3D elements. To create, customize, and manage 3D models and effects, it also relies on the RealityKit framework and Reality Composer Pro. Hello World constructs the scene that it displays at launch \u2014 the first scene that appears in the WorldApp structure \u2014 using a WindowGroup: Like other platforms \u2014 for example, macOS and iOS \u2014 visionOS displays a window group as a familiar-looking window. In visionOS, people can resize and move windows around the Shared Space. Even if your app offers a sophisticated 3D experience, a window is a great starting point for an app because it eases people into the experience. It\u2019s also a good place to provide instructions or controls. Tip This particular window group uses the plain window style to maintain control over the glass background effect that visionOS would otherwise automatically add. After you watch a brief introductory animation that shows the text Hello World typing in, the Modules view that defines the primary scene\u2019s content presents options to explore different aspects of the world. This view contains a table of contents at the root of a NavigationStack: A visionOS navigation stack has the same behavior that it has in other platforms. When it first appears, the stack displays its root view. When someone chooses an embedded NavigationLink, the stack draws a new view and displays a back button in the toolbar. When someone taps the back button, the stack restores the previous view.  The trailing closure of the navigationDestination(for:destination:) view modifier in the code above displays a view when someone activates a link based on a module input that comes from the corresponding link\u2019s initializer: The possible module values come from a custom Module enumeration: The globe module opens with a few facts about the Earth in the main window next to a decorative, flat image that supports the content. To help people understand even more, the module includes a button titled View Globe that opens a 3D interactive globe in a new window.  To be able to open multiple scene types, Hello World includes the UIApplicationSceneManifest key in its Information Property List file. The value for this key is a dictionary that includes the UIApplicationSupportsMultipleScenes key with a value of true: With the key in place, the app makes use of a second WindowGroup in its App declaration. This new window group uses the Globe view as its content: This window group creates a volume \u2014 which is a container that has three dimensions and behaves like a transparent box \u2014 because Hello World uses the volumetric window style scene modifier. People can move this box around the Shared Space like they move other window types, and the content remains fixed inside. The defaultSize(width:height:depth:in:) modifier specifies a size for the volume in meters, including a depth dimension. The Globe view inside the volume contains 3D content, but is still just a SwiftUI view. It contains two elements in a ZStack: a subview that draws a model of the Earth, and another that provides a control panel that people can use to configure the model\u2019s appearance. The globe module presents a View Globe button that people can tap to display or dismiss the volume, depending on the current state. Hello World achieves this behavior by creating a Toggle with the button style, and embedding it in a custom GlobeToggle view.  When someone taps the toggle, the isShowingGlobe state changes, and the onChange(of:initial:_:) modifier calls the openWindow or dismissWindow action to open or dismiss the volume, respectively. The view gets these actions from the environment and uses an identifier that matches the volume\u2019s identifier. You use windows in visionOS the same way you do in other platforms. But even 2D windows in visionOS provide a small amount of depth you can use to create 3D effects \u2014 like elements that appear in front of other elements. Hello World takes advantage of this depth to present small models inline with 2D content. The app\u2019s second module, Objects in Orbit, provides information about objects that go around the Earth, like the Moon and artificial satellites. To give a sense of what these objects look like, the module displays 3D models of these items directly inside the window.  Hello World loads these models from the asset bundle using a Model3D structure inside a custom ItemView. The view scales and positions the model to fit the available space, and applies optional orientation adjustments: The app uses this ItemView once for each model, placing each in an overlay that only becomes visible based on the current selection. For example, the following overlay displays the satellite model with a small amount of tilt in the x-axis and z-axis: The VStack that contains the models also contains a Picker that people use to select a model to view: When you add 3D effects to a 2D window, keep this guidance in mind: Don\u2019t overdo it. These kinds of effects add interest, but can unintentionally obscure important controls or information as people view the window from different directions. Ensure that elements don\u2019t exceed the available depth. Excess depth causes elements to clip. Account for any position or orientation changes that might occur after initial placement. Avoid models intersecting with the backing glass. Again, account for potential movement after initial placement. People can visualize how satellites move around the Earth because the app\u2019s orbit module displays the Earth, the Moon, and a communications satellite together as a single system. People can move the system anywhere in their environment or resize it using standard gestures. They can also move themselves around the system to get different perspectives.  Note To learn about designing with gestures in visionOS, read Gestures in Human Interface Guidelines. To create this visualization, the app displays the Orbit view \u2014 which contains a single RealityView that models the entire system \u2014 in an ImmersiveSpace scene with the mixed immersion style: As with any secondary scene in a visionOS app, this scene depends on having the UIApplicationSupportsMultipleScenes key in the Information Property List file. The app also opens and closes the space using a toggle view that resembles the one used for the globe: There are a few key differences from the version that appears in the section Open and dismiss the globe volume: OrbitToggle uses openImmersiveSpace and dismissImmersiveSpace from the environment, rather than the window equivalents. The dismiss action in this case doesn\u2019t require an identifier, because people can only open one space at a time, even across apps. The open and dismiss actions for spaces operate asynchronously, and so they appear inside a Task. The app\u2019s final module gives people a sense of the Earth\u2019s place in the solar system. Like other modules, this one includes information and a decorative image next to a button that leads to another visualization \u2014 in this case so people can experience Earth from space. When a person taps the button, the app takes over the entire display and shows stars in all directions. The Earth appears directly in front, the Moon to the right, and the Sun to the left. The main window also shows a small control panel that people can use to exit the fully immersive experience.  Tip People can always close the currently open immersive space by pressing the device\u2019s Digital Crown, but it\u2019s typically useful when you provide a built-in mechanism to maintain control of the experience within your app. The app uses another immersive space scene for this module, but here with the full immersion style that turns off the passthrough video: This scene depends on the same UIApplicationSupportsMultipleScenes key that other secondary scenes do, and is activated by a SolarSystemToggle that\u2019s similar to the ones that the app uses for the other scenes: This control appears in the main window to provide a way to begin the fully immersive experience, and separately in the control panel as a way to exit the experience. Because the app uses this control as two distinct buttons rather than as a toggle in one location, it\u2019s composed of a Button with behavior that changes depending on the app state rather than as a toggle with a button style. To reuse the main window for the solar system controls, Hello World places both the navigation stack and the controls in a ZStack, and then sets the opacity of each to ensure that only one appears at a time:\n\n\n\nWindowGroup(\"Hello World\", id: \"modules\") {\n    Modules()\n        .environment(model)\n}\n.windowStyle(.plain)\n\n\n\n\nNavigationStack(path: $model.navigationPath) {\n    TableOfContents()\n        .navigationDestination(for: Module.self) { module in\n            ModuleDetail(module: module)\n                .navigationTitle(module.eyebrow)\n        }\n}\n\n\n\n\nNavigationLink(value: module) { /* The link's label. */ }\n\n\n\n\nenum Module: String, Identifiable, CaseIterable, Equatable {\n    case globe, orbit, solar\n    // ...\n}\n\n\n\n\n<key>UIApplicationSceneManifest</key>\n<dict>\n    <key>UIApplicationSupportsMultipleScenes</key>\n    <true/>\n    <key>UISceneConfigurations</key>\n    <dict/>\n</dict>\n\n\n\n\nWindowGroup(id: Module.globe.name) {\n    Globe()\n        .environment(model)\n}\n.windowStyle(.volumetric)\n.defaultSize(width: 0.6, height: 0.6, depth: 0.6, in: .meters)\n\n\n\n\nstruct GlobeToggle: View {\n    @Environment(ViewModel.self) private var model\n    @Environment(\\.openWindow) private var openWindow\n    @Environment(\\.dismissWindow) private var dismissWindow\n\n\n    var body: some View {\n        @Bindable var model = model\n\n\n        Toggle(Module.globe.callToAction, isOn: $model.isShowingGlobe)\n            .onChange(of: model.isShowingGlobe) { _, isShowing in\n                if isShowing {\n                    openWindow(id: Module.globe.name)\n                } else {\n                    dismissWindow(id: Module.globe.name)\n                }\n            }\n            .toggleStyle(.button)\n    }\n}\n\n\n\n\nprivate struct ItemView: View {\n    var item: Item\n    var orientation: SIMD3<Double> = .zero\n\n\n    var body: some View {\n        Model3D(named: item.name, bundle: worldAssetsBundle) { model in\n            model.resizable()\n                .scaledToFit()\n                .rotation3DEffect(\n                    Rotation3D(\n                        eulerAngles: .init(angles: orientation, order: .xyz)\n                    )\n                )\n                .frame(depth: modelDepth)\n                .offset(z: -modelDepth / 2)\n        } placeholder: {\n            ProgressView()\n                .offset(z: -modelDepth * 0.75)\n        }\n    }\n}\n\n\n\n\n.overlay {\n    ItemView(item: .satellite, orientation: [0.15, 0, 0.15])\n        .opacity(selection == .satellite ? 1 : 0)\n}\n\n\n\n\nPicker(\"Satellite\", selection: $selection) {\n    ForEach(Item.allCases) { item in\n        Text(item.name)\n    }\n}\n.pickerStyle(.segmented)\n\n\n\n\nImmersiveSpace(id: Module.orbit.name) {\n    Orbit()\n        .environment(model)\n}\n.immersionStyle(selection: $orbitImmersionStyle, in: .mixed)\n\n\n\n\nstruct OrbitToggle: View {\n    @Environment(ViewModel.self) private var model\n    @Environment(\\.openImmersiveSpace) private var openImmersiveSpace\n    @Environment(\\.dismissImmersiveSpace) private var dismissImmersiveSpace\n\n\n    var body: some View {\n        @Bindable var model = model\n\n\n        Toggle(Module.orbit.callToAction, isOn: $model.isShowingOrbit)\n            .onChange(of: model.isShowingOrbit) { _, isShowing in\n                Task {\n                    if isShowing {\n                        await openImmersiveSpace(id: Module.orbit.name)\n                    } else {\n                        await dismissImmersiveSpace()\n                    }\n                }\n            }\n            .toggleStyle(.button)\n    }\n}\n\n\n\n\nImmersiveSpace(id: Module.solar.name) {\n    SolarSystem()\n        .environment(model)\n}\n.immersionStyle(selection: $solarImmersionStyle, in: .full)\n\n\n\n\nstruct SolarSystemToggle: View {\n    @Environment(ViewModel.self) private var model\n    @Environment(\\.openImmersiveSpace) private var openImmersiveSpace\n    @Environment(\\.dismissImmersiveSpace) private var dismissImmersiveSpace\n\n\n    var body: some View {\n        Button {\n            Task {\n                if model.isShowingSolar {\n                    await dismissImmersiveSpace()\n                } else {\n                    await openImmersiveSpace(id: Module.solar.name)\n                }\n            }\n        } label: {\n            if model.isShowingSolar {\n                Label(\n                    \"Exit the Solar System\",\n                    systemImage: \"arrow.down.right.and.arrow.up.left\")\n            } else {\n                Text(Module.solar.callToAction)\n            }\n        }\n    }\n}\n\n\n\n\nZStack {\n    SolarSystemControls()\n        .opacity(model.isShowingSolar ? 1 : 0)\n\n\n    NavigationStack(path: $model.navigationPath) {\n        // ...\n    }\n    .opacity(model.isShowingSolar ? 0 : 1)\n}\n.animation(.default, value: model.isShowingSolar)\n", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/destination-video", "title": "Destination Video | Apple Developer Documentation", "content": "App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space   Destination Video is a multiplatform video-playback app for visionOS, iOS, and tvOS. People get a familiar media-browsing experience navigating the library\u02bcs content and playing videos they find interesting. The app provides a similar experience on supported platforms, but leverages unique features of visionOS to create a novel, immersive playback experience. When you select a video in the library, Destination Video presents a view that displays additional details about the item. The view presents controls to play the video and specify whether to include it in your Up Next list. In visionOS, it also displays a video poster along its leading edge. Tapping the view\u2019s Preview button displays an inline preview of the video. When you present an AVPlayerViewController object\u2019s interface as a child of another view, inline controls display, for example, pause, skip, and seek. Showing standard playback controls in your app provides a familiar UI that automatically adapts its appearance to fit each platform, and is the recommended choice in most cases. Destination Video uses a simple UI for the inline player view: a single button that toggles state of playback. AVPlayerViewController doesn\u2019t provide this controls style, but the app uses it to display the video content without controls by setting the value of its showsPlaybackControls property to false. It then overlays the custom playback controls it requires. See Destination Video\u2019s InlinePlayerView type for details on how you can implement this. Note AVPlayerViewController only supports displaying 2D content when embedded inline. One of the most exciting features of visionOS is its ability to play 3D video along with Spatial Audio, which adds a deeper level of immersion to the viewing experience. Playing 3D content in your app requires that you display AVPlayerViewController full window. When you present the player this way, the system automatically docks it into the ideal viewing position, and presents streamlined playback controls that keep the person\u2019s focus on the content. Note In iOS or tvOS, you typically present video in a full-screen presentation using the fullScreenCover(isPresented:onDismiss:content:) modifier. This API is available in visionOS; however, the recommended way to present the player for full-window playback is to set it as the root view of your app\u2019s window group. Destination Video\u2019s ContentView displays the app\u2019s library by default. It observes changes to the player model\u2019s presentation property, which indicates whether the app requests inline or full-window playback. When the presentation state changes to fullWindow, the view redraws the UI to display the player view in place of the library. When someone selects the Play Video button on the detail view, the app calls the player model\u2019s loadVideo(_: presentation:) method requesting the fullWindow presentation option. After the player model successfully loads the video content for playback, it updates its presentation value to fullWindow, which causes the app to replace the library with PlayerView. To dismiss the full-window player in visionOS, people tap the Back button in the player UI. To handle this action, the app\u2019s PlayerViewControllerDelegate type defines an AVPlayerViewControllerDelegate object that handles the dismissal. When the delegate receives this call, it clears the media from the player model and resets the presentation state back to its default value, which results in the Destination Video app redisplaying the library view. Media playback apps require common configuration of their capabilities and audio session. In addition to performing the steps outlined in Configuring your app for media playback, Destination Video also adopts new AVAudioSession API to customize a person\u2019s Spatial Audio experience. After the app successfully loads a video for playback, it configures the Spatial Audio experience for the current presentation. For the inline player view, it sets the experience to a small, focused sound stage where the audio originates from the location of the view. When displaying a video full window, it sets the experience to a large, fully immersive sound stage. Building video playback apps for visionOS provides new opportunities to enhance the viewing experience beyond the bounds of the player window. To add a greater level of immersion, the sample presents an immersive space that displays a scene around a person as they watch the video. It defines the immersive space in the DestinationVideo app structure. The immersive space presents an instance of DestinationView, which maps a texture to the inside of a sphere that it displays around a person. The app presents it using the .progressive immersion style, which lets someone change their amount of immersion by turning the Digital Crown on the device. The Destination Video app automatically presents the immersive space when a person navigates to a video\u2019s detail view, and dismisses it when they return to the library. To monitor these events, the app observes its navigation path to determine when a navigation event occurs so it can show or dismiss the space. One of the best ways to enhance your app\u2019s playback experience is to make that experience shareable with others. You can use the AVFoundation and the Group Activities frameworks to build SharePlay experiences that bring people together even when they can\u2019t be in the same location. The Destination Video app creates an experience where people can watch videos with others across devices and platforms. It defines a group activity called VideoWatchingActivity that adopts the GroupActivity protocol.  When people have a FaceTime call active and they play a video in the full-window player, it becomes eligible for playback for everyone on the call. The app\u2019s VideoWatchingCoordinator actor manages Destination Video\u2019s SharePlay functionality. It observes the activation of new VideoWatchingActivity sessions and when one starts, it sets the GroupSession instance on the player object\u2019s AVPlaybackCoordinator. With the player configured to use the group session, when the app loads new videos, they become eligible to share with people in the FaceTime call.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/happybeam", "title": "Happy Beam | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space  / visionOS / Happy Beam API Changes: None In visionOS, you can create fun, dynamic games and apps using several different frameworks to create new kinds of spatial experiences: RealityKit, ARKit, SwiftUI, and Group Activities. This sample introduces Happy Beam, a game where you and your friends can hop on a FaceTime call and play together. You\u2019ll learn the mechanics of the game where grumpy clouds float around in the space, and people play by making a heart shape with their hands to project a beam. People aim the beam at the clouds to cheer them up, and a score counter keeps track of how well each player does cheering up the clouds. Most apps in visionOS launch as a window that opens different scene types depending on the needs of the app. Here you see how Happy Beam presents a fun interface to people by using several SwiftUI views that display a welcome screen, a coaching screen that gives instructions, a scoreboard, and a game-ending screen. Welcome window Instructions Scoreboard Ending window     The following shows you the primary view in the app that displays each phase of gameplay: When 3D content starts to appear, the game opens an immersive space to present content outside of the main window and in a person\u2019s surroundings. The HappyBeam container view declares a dependency on openImmersiveSpace: It later uses that dependency to open the space from the app\u2019s declaration when it\u2019s time to start showing 3D content: The Happy Beam app recognizes the central heart-shaped hands gesture using ARKit\u2019s support for 3D hand tracking in visionOS. Using hand tracking requires a running session and authorization from the wearer. It uses the NSHandsTrackingUsageDescription user info key to explain to players why the app requests permission for hand tracking.  Hand-tracking data isn\u2019t available when your app is only displaying a window or volume. Instead, it\u2019s available when you present an immersive space, as in the previous example. You can detect gestures using ARKit data with a level of accuracy that depends on your use case and intended experience. For example, Happy Beam could require strict positioning of finger joints to closely resemble a heart shape. Instead, however, it prompts people to make a heart shape and uses a heuristic to indicate when the gesture is close enough. The following checks whether a person\u2019s thumbs and index fingers are almost touching: To support accessibility features and general user preferences, include multiple kinds of input in an app that uses hand tracking as one form of input. Happy Beam supports several kinds of input: Interactive hands input from ARKit with the custom heart gesture. Drag gesture input to rotate the stationary beam on its platform. Accessibility components from RealityKit to support custom actions for cheering up the clouds. Game Controller support to make control over the beam more interactive from Switch Control. The 3D content in the app comes in the form of assets that you can export from Reality Composer Pro. You place each asset in the RealityView that represents your immersive space. The following shows how Happy Beam generates clouds when the game starts, as well as materials for the floor-based beam projector. Because the game uses collision detection to keep score \u2014 the beam cheers up grumpy clouds when they collide \u2014 you make collision shapes for each model that might be involved. You use the Group Activities framework in visionOS to support SharePlay during a FaceTime call. Happy Beam uses Group Activities to sync the score, active players list, and the position of each player\u2019s projected beam. Note Developers using the Apple Vision Pro developer kit can test spatial SharePlay experiences on-device by installing the Persona Preview Profile. Use a reliable channel to send information that\u2019s important to be correct, even if it can be slightly delayed as a result. The following shows how Happy Beam updates the game model\u2019s score state in response to a score message: Use an unreliable messenger for sending data with low-latency requirements. Because the delivery mode is unreliable, some messages might not make it. Happy Beam uses the unreliable mode to send live updates to the position of the beam when each participant in the call chooses the Spatial option in FaceTime. The following shows how Happy Beam serializes beam data for each message: Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC\n\n\n\nstruct HappyBeam: View {\n    @Environment(\\.openImmersiveSpace) private var openImmersiveSpace\n    @Environment(GameModel.self) var gameModel\n    \n    @State private var session: GroupSession<HeartProjection>? = nil\n    @State private var timer = Timer.publish(every: 1, on: .main, in: .common).autoconnect()\n    @State private var subscriptions = Set<AnyCancellable>()\n    \n    var body: some View {\n        let gameState = GameScreen.from(state: gameModel)\n        VStack {\n            Spacer()\n            Group {\n                switch gameState {\n                case .start:\n                    Start()\n                case .soloPlay:\n                    SoloPlay()\n                case .lobby:\n                    Lobby()\n                case .soloScore:\n                    SoloScore()\n                case .multiPlay:\n                    MultiPlay()\n                case .multiScore:\n                    MultiScore()\n                }\n            }\n            .glassBackgroundEffect(\n                in: RoundedRectangle(\n                    cornerRadius: 32,\n                    style: .continuous\n                )\n            )\n        }\n    }\n}\n\n\n\n@main\nstruct HappyBeamApp: App {\n    @State private var gameModel = GameModel()\n    @State private var immersionState: ImmersionStyle = .mixed\n    \n    var body: some SwiftUI.Scene {\n        WindowGroup(\"HappyBeam\", id: \"happyBeamApp\") {\n            HappyBeam()\n                .environmentObject(gameModel)\n        }\n        .windowStyle(.plain)\n        \n        ImmersiveSpace(id: \"happyBeam\") {\n            HappyBeamSpace(gestureModel: HeartGestureModelContainer.heartGestureModel)\n                .environmentObject(gameModel)\n        }\n        .immersionStyle(selection: $immersionState, in: .mixed)\n    }\n}\n\n\n\n@Environment(\\.openImmersiveSpace) private var openImmersiveSpace\n\n\n\nif gameModel.countDown == 0 {\n    Task {\n        await openImmersiveSpace(id: \"happyBeam\")\n    }\n}\n\n\n\nTask {\n    do {\n        try await session.run([handTrackingProvider])\n    } catch {\n        print(\"ARKitSession error:\", error)\n    }\n}\n\n\n\n// Get the position of all joints in world coordinates.\nlet originFromLeftHandThumbKnuckleTransform = matrix_multiply(\n    leftHandAnchor.originFromAnchorTransform, leftHandThumbKnuckle.anchorFromJointTransform\n).columns.3.xyz\nlet originFromLeftHandThumbTipTransform = matrix_multiply(\n    leftHandAnchor.originFromAnchorTransform, leftHandThumbTipPosition.anchorFromJointTransform\n).columns.3.xyz\nlet originFromLeftHandIndexFingerTipTransform = matrix_multiply(\n    leftHandAnchor.originFromAnchorTransform, leftHandIndexFingerTip.anchorFromJointTransform\n).columns.3.xyz\nlet originFromRightHandThumbKnuckleTransform = matrix_multiply(\n    rightHandAnchor.originFromAnchorTransform, rightHandThumbKnuckle.anchorFromJointTransform\n).columns.3.xyz\nlet originFromRightHandThumbTipTransform = matrix_multiply(\n    rightHandAnchor.originFromAnchorTransform, rightHandThumbTipPosition.anchorFromJointTransform\n).columns.3.xyz\nlet originFromRightHandIndexFingerTipTransform = matrix_multiply(\n    rightHandAnchor.originFromAnchorTransform, rightHandIndexFingerTip.anchorFromJointTransform\n).columns.3.xyz\n\n\nlet indexFingersDistance = distance(originFromLeftHandIndexFingerTipTransform, originFromRightHandIndexFingerTipTransform)\nlet thumbsDistance = distance(originFromLeftHandThumbTipTransform, originFromRightHandThumbTipTransform)\n\n\n// Heart gesture detection is true when the distance between the index finger tips centers\n// and the distance between the thumb tip centers is each less than four centimeters.\nlet isHeartShapeGesture = indexFingersDistance < 0.04 && thumbsDistance < 0.04\nif !isHeartShapeGesture {\n    return nil\n}\n\n\n// Compute a position in the middle of the heart gesture.\nlet halfway = (originFromRightHandIndexFingerTipTransform - originFromLeftHandThumbTipTransform) / 2\nlet heartMidpoint = originFromRightHandIndexFingerTipTransform - halfway\n\n\n// Compute the vector from left thumb knuckle to right thumb knuckle and normalize (X axis).\nlet xAxis = normalize(originFromRightHandThumbKnuckleTransform - originFromLeftHandThumbKnuckleTransform)\n\n\n// Compute the vector from right thumb tip to right index finger tip and normalize (Y axis).\nlet yAxis = normalize(originFromRightHandIndexFingerTipTransform - originFromRightHandThumbTipTransform)\n\n\nlet zAxis = normalize(cross(xAxis, yAxis))\n\n\n// Create the final transform for the heart gesture from the three axes and midpoint vector.\nlet heartMidpointWorldTransform = simd_matrix(\n    SIMD4(xAxis.x, xAxis.y, xAxis.z, 0),\n    SIMD4(yAxis.x, yAxis.y, yAxis.z, 0),\n    SIMD4(zAxis.x, zAxis.y, zAxis.z, 0),\n    SIMD4(heartMidpoint.x, heartMidpoint.y, heartMidpoint.z, 1)\n)\nreturn heartMidpointWorldTransform\n\n\n\n@MainActor\nfunc placeCloud(start: Point3D, end: Point3D, speed: Double) async throws -> Entity {\n    let cloud = await loadFromRealityComposerPro(\n        named: BundleAssets.cloudEntity,\n        fromSceneNamed: BundleAssets.cloudScene\n    )!\n        .clone(recursive: true)\n    \n    cloud.generateCollisionShapes(recursive: true)\n    cloud.components[PhysicsBodyComponent.self] = PhysicsBodyComponent()\n    \n    var accessibilityComponent = AccessibilityComponent()\n    accessibilityComponent.label = \"Cloud\"\n    accessibilityComponent.value = \"Grumpy\"\n    accessibilityComponent.isAccessibilityElement = true\n    accessibilityComponent.traits = [.button, .playsSound]\n    accessibilityComponent.systemActions = [.activate]\n    cloud.components[AccessibilityComponent.self] = accessibilityComponent\n    \n    let animation = cloudMovementAnimations[cloudPathsIndex]\n    \n    cloud.playAnimation(animation, transitionDuration: 1.0, startsPaused: false)\n    cloudAnimate(cloud, kind: .sadBlink, shouldRepeat: false)\n    spaceOrigin.addChild(cloud)\n    \n    return cloud\n}\n\n\n\nsessionInfo.reliableMessenger = GroupSessionMessenger(session: newSession, deliveryMode: .reliable)\n\n\nTask {\n    for await (message, sender) in sessionInfo!.reliableMessenger!.messages(of: ScoreMessage.self) {\n        gameModel.clouds[message.cloudID].isHappy = true\n        gameModel\n            .players\n            .filter { $0.name == sender.source.id.asPlayerName }\n            .first!\n            .score += 1\n    }\n}\n\n\n\nsessionInfo.messenger = GroupSessionMessenger(session: newSession, deliveryMode: .unreliable)\n\n\n\n// Send each player's beam data during FaceTime calls where players have selected the Spatial option.\nfunc sendBeamPositionUpdate(_ pose: Pose3D) {\n    if let sessionInfo = sessionInfo, let session = sessionInfo.session, let messenger = sessionInfo.messenger {\n        let everyoneElse = session.activeParticipants.subtracting([session.localParticipant])\n        \n        if isShowingBeam, gameModel.isSpatial {\n            messenger.send(BeamMessage(pose: pose), to: .only(everyoneElse)) { error in\n                if let error = error { print(\"Message failure:\", error) }\n            }\n        }\n    }\n}", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/diorama", "title": "Diorama | Apple Developer Documentation", "content": "Tracking specific points in world space Placing content on detected planes Incorporating real-world surroundings in an immersive experience Setting up access to ARKit data Happy Beam ARKit Capturing screenshots and video from Apple Vision Pro for 2D viewing Designing RealityKit content with Reality Composer Pro Understanding RealityKit\u2019s modular architecture Diorama Swift Splash RealityKit and Reality Composer Pro Positioning and sizing windows Presenting windows and spaces Hello World SwiftUI Improving accessibility support in your visionOS app Adopting best practices for privacy and user preferences Designing for visionOS Design Drawing sharp layer-based content in visionOS Creating fully immersive experiences in your app Adding 3D content to your app Creating your first visionOS app App construction   Use Reality Composer Pro to compose, edit, and preview RealityKit content for your visionOS app. In your Reality Composer Pro project, you can create one or more scenes, each of which contains a hierarchy of virtual objects called entities that your app can efficiently load and display. In addition to helping you compose entity hierarchies, Reality Composer Pro also gives you the ability to add and configure components \u2014 even custom components that you\u2019ve written \u2014 to the entities in your scenes. You can also design the visual appearance of entities using Shader Graph, a node-based visual tool for creating RealityKit materials. Shader Graph gives you a tremendous amount of control over the surface details and shape of entities. You can even create animated materials and dynamic materials that change based on the state of your app or user input. Diorama demonstrates many of RealityKit and Reality Composer Pro\u2019s features. It displays an interactive, virtual topographical trail map, much like the real-world dioramas you find at trailheads and ranger stations in national parks. This virtual map has points of interest you can tap to bring up more detailed information. You can also smoothly transition between two trail maps: Yosemite and Catalina Island. Your Reality Composer Pro project must contain assets, which you use to compose scenes for your app. Diorama\u2019s project has several assets, including 3D models like the diorama table, trail map, some birds and clouds that fly over the map, and a number of sounds and images. Reality Composer Pro provides a library of 3D models you can use. Access the library by clicking the Add (+) button on the right side of the toolbar. Selecting objects from the library imports them into your project.  Diorama uses custom assets instead of the available library assets. To use custom assets in your own Reality Composer Pro scenes, import them into your project in one of three ways: by dragging them to Reality Composer Pro\u2019s project browser, using File > Import from the File menu, or copying the assets into the .rkassets bundle inside your project\u2019s Swift package.  Note Although you can still load USDZ files and other assets directly in visionOS, RealityKit compiles assets in your Reality Composer Pro project into a binary format that loads considerably faster than loading from individual files. A single Reality Composer Pro project can have multiple scenes. A scene is an entity hierarchy stored in the project as a .usda file that you can load and display in a RealityView. You can use Reality Composer\u2019s scenes to build an entire RealityKit scene, or to store reusable entity hierarchies that you can use as building block for composing scenes at runtime \u2014 the approach Diorama uses. You can add as many different scenes to your project as you need by selecting File > New > Scene, or pressing \u2318N. At the top of the Reality Composer Pro window, there\u2019s a separate tab for every scene that\u2019s currently open. To open a scene, double-click the scene\u2019s .usda file in the project browser. To edit a scene, select its tab, and make changes using the hierarchy viewer, the 3D view, and the inspector.  RealityKit can only include entities in a scene, but it can\u2019t use every type of asset that Reality Composer Pro supports as an entity. Reality Composer Pro automatically turns some assets, like 3D models, into an entity when you place them in a scene. It uses other assets indirectly. It uses image files, for example, primarily to define the surface details of model entities. Diorama uses multiple scenes to group assets together and then, at runtime, combines those scenes into a single immersive experience. For example, the diorama table has its own scene that includes the table, the map surface, and the trail lines. There are separate scenes for the birds that flock over the table, and for the clouds that float above it.  To add entities to a scene, drag assets from the project browser to the scene\u2019s hierarchy view or 3D view. If the asset you drag is a type that can be represented as an entity, Reality Composer Pro adds it to your scene. You can select any asset in the scene hierarchy or the 3D view and change its location, rotation, and scale using the inspector on the right side of the window or the manipulator in the 3D view. RealityKit follows a design pattern called Entity Component System (ECS). In an ECS app, you store additional data on an entity using components and can implement entity behavior by writing systems that use the data from those components. You can add and configure components to entities in Reality Composer Pro, including both shipped components like PhysicsBodyComponent, and custom components that you write and place in the Sources folder of your Reality Composer Pro Swift package. You can even create new components in Reality Composer Pro and then edit them in Xcode. For more information about ECS, see Understanding RealityKit\u2019s modular architecture. Diorama uses custom components to identify which transforms are points of interest, to mark the birds so the app can make sure they flock together, and to control the opacity of entities that are specific to just one of the two maps. To add a component to an entity, select that entity in the hierarchy view or 3D view. At the bottom right of the inspector window, click on the Add Component button. A list of available components appears and the first item in that list is New Component. This item creates a new component class, and optionally a new system class, and adds the component to the selected entity. If you look at the list of components, you see the PointOfInterestComponent that Diorama uses to indicate which transforms are points of interest. If the selected entity doesn\u2019t already contain a PointOfInterestComponent, selecting that adds it to the selected entity. Each entity can only have one component of a specific type. You can edit the values of the existing component in the inspector, which changes what shows up when you tap that point of interest in the app.  In Reality Composer Pro, a transform is an empty entity that marks a point in space. A transform contains a location, rotation, and scale, and its child entities inherit those. But, transforms have no visual representation and do nothing by themselves. Use transforms to mark locations in your scene or organize your entity hierarchy. For example, you might make several entities that need to move together into child entities of the same transform, so you can move them together by moving the parent transform. Diorama uses transforms with a PointOfInterestComponent to indicate points of interest on the map. When the app runs, those transforms mark the location of the floating placards with the name of the location. Tapping on a placard expands it to show more detailed information. To turn transforms into an interactive view, the app looks for a specific component on transforms called a PointOfInterestComponent. Because a transform contains no data other than location, orientation, and scale, it uses this component to hold the data the app needs to display on the placards. If you open the DioramaAssembled scene in Reality Composer Pro and click on the transform called Cathedral_Rocks, you see the PointOfInterestComponent in the inspector.  To load a Reality Composer Pro scene, use load(named:in:), passing the name of the scene you want to load and the project\u2019s bundle. Reality Composer Pro Swift packages define a constant that provides ready access to its bundle. The constant is the name of the Reality Composer Pro project with \u201cBundle\u201d appended to the end. In this case, the project is called RealityKitContent, so the constant is called RealityKitContentBundle. Here\u2019s how Diorama loads the map table in the RealityView initializer: The load(named:in:) function is asynchronous when called from an asynchronous context. Because the content closure of the RealityView initializer is asynchronous, it automatically uses the async version to load the scene.  Note that when using it asynchronously, you must call it using the await keyword. Diorama adds a PointOfInterestComponent to a transform to display details about interesting places. Every point of interest\u2019s name appears in a view that floats above its location on the map. When you tap the floating view, it expands to show detailed information, which the app pulls from the PointOfInterestComponent. The app shows these details by creating a SwiftUI view for each point of interest and querying for all entities that have a PointOfInterestComponent using this query declared in ImmersiveView.swift: In the RealityView initializer, Diorama queries to retrieve the points of interest entities and passes them to a function called createLearnMoreView(for:), which creates the view and saves it for display when it\u2019s tapped. Diorama displays the information added to a PointOfInterestComponent in a LearnMoreView, which it stores as an attachment. Attachments are SwiftUI views that are also RealityKit entities and that you can place into a RealityKit scene at a specific location. Diorama uses attachments to position the view that floats above each point of interest. The app first checks to see if the entity has a component called PointOfInterestRuntimeComponent. If it doesn\u2019t, it creates a new one and adds it to the entity. This new component contains a value you only use at runtime that you don\u2019t need to edit in Reality Composer Pro. By putting this value into a separate component and adding it to entities at runtime, Reality Composer Pro never displays it in the inspector. The PointOfInterestRuntimeComponent stores an identifier called an attachment tag, which uniquely identifies an attachment so the app can retrieve and display it at the appropriate time. Next, Diorama creates a SwiftUI view called a LearnMoreView with the information from the PointOfInterestComponent, tags that view, and stores the tag in the PointOfInterestRuntimeComponent. Finally, it stores the view in an AttachmentProvider, which is a custom class that maintains references to the attachment views so they don\u2019t get deallocated when they\u2019re not in a scene. Assigning a view to an attachment provider doesn\u2019t actually display that view in the scene. The initializer for RealityView has an optional view builder called attachments that\u2019s used to specify the attachments. In the update closure of the initializer, which RealityKit calls when the contents of the view change, the app queries for entities with a PointOfInterestRuntimeComponent, uses the tag from that component to retrieve the correct attachment for it, and then adds that attachment and places it above its location on the map. To switch between the two different topographical maps, Diorama shows a slider that morphs the map between the two locations. To accomplish this, and to draw elevation lines on the map, the FlatTerrain entity in the DioramaAssembled scene uses a Shader Graph material. Shader Graph is a node-based material editor that\u2019s built into Reality Composer Pro. Shader Graph gives you the ability to create dynamic materials that you can change at runtime. Prior to Reality Composer Pro, the only way to implement a dynamic material like this was to create a CustomMaterial and write Metal shaders to implement the necessary logic. Diorama\u2019s DynamicTerrainMaterialEnhanced does two things. It draws contour lines on the map based on height data stored in displacement map images, and it also offsets the vertices of the flat disk based on the same data. By interpolating between two different height maps, the app achieves a smooth transition between the two different sets of height data. When you build Shader Graph materials, you can give them input parameters called promoted inputs that you set from Swift code. This allows you to implement logic that previously required writing a Metal shader. The materials you build in the editor can affect both the look of an entity using the custom surface output node, which equates to writing Metal code in a fragment shader, or the position of vertices using the geometry modifier output, which equates to Metal code running in a vertex shader.  Node graphs can contain subgraphs, which are similar to functions. They contain reusable sets of nodes with inputs and outputs. Subgraphs contain the logic to draw the contour lines and the logic to offset the vertices. Double-click a subgraph to edit it. For more information about building materials using Shader Graph, see Explore Materials in Reality Composer Pro. To change the map, DynamicTerrainMaterialEnhanced has a promoted input called Progress. If that parameter is set to 1.0, it displays Catalina Island. If it\u2019s set to 0, it displays Yosemite. Any other number shows a state in transition between the two. When someone manipulates the slider, the app updates that input parameter based on the slider\u2019s value. Important Shader Graph material parameters are case-sensitive. If the capitalization is wrong, your code won\u2019t actually update the material. The app sets the value of the input parameter in a function called handleMaterial() that the slider\u2019s .onChanged closure calls. That function retrieves the ShaderGraphMaterial from the terrain entity and calls setParameter(name:value:) on it.\n\n\n\nlet entity = try await Entity.load(named: \"DioramaAssembled\", \n                                   in: RealityKitContent.RealityKitContentBundle)\n\n\n\n\nstatic let markersQuery = EntityQuery(where: .has(PointOfInterestComponent.self))\n\n\n\n\nsubscriptions.append(content.subscribe(to: ComponentEvents.DidAdd.self, componentType: PointOfInterestComponent.self, { event in\n    createLearnMoreView(for: event.entity)\n}))\n\n\n\n\nstruct PointOfInterestRuntimeComponent: Component {\n    let attachmentTag: ObjectIdentifier\n}\n\n\n\n\nlet tag: ObjectIdentifier = entity.id\n\n\nlet view = LearnMoreView(name: pointOfInterest.name,\n                         description: pointOfInterest.description ?? \"\",\n                         imageNames: pointOfInterest.imageNames,\n                         trail: trailEntity,\n                         viewModel: viewModel)\n    .tag(tag)\nentity.components[PointOfInterestRuntimeComponent.self] = PointOfInterestRuntimeComponent(attachmentTag: tag)\n\n\nattachmentsProvider.attachments[tag] = AnyView(view)\n\n\n\n\nForEach(attachmentsProvider.sortedTagViewPairs, id: \\.tag) { pair in\n    pair.view\n}\n\n\n\n\nviewModel.rootEntity?.scene?.performQuery(Self.runtimeQuery).forEach { entity in\n\n\n    guard let attachmentEntity = attachments.entity(for: component.attachmentTag) else { return }\n    \n    if let pointOfInterestComponent = entity.components[PointOfInterestComponent.self] {\n        attachmentEntity.components.set(RegionSpecificComponent(region: pointOfInterestComponent.region))\n        attachmentEntity.components.set(OpacityComponent(opacity: 0))\n    }\n    \n    viewModel.rootEntity?.addChild(attachmentEntity)\n    attachmentEntity.setPosition([0, 0.2, 0], relativeTo: entity)\n}\n\n\n\n\nprivate func handleMaterial() {\n    guard let terrain = viewModel.rootEntity?.terrain,\n            let terrainMaterial = terrainMaterial else { return }\n    do {\n        var material = terrainMaterial\n        try material.setParameter(name: materialParameterName, value: .float(viewModel.sliderValue))\n        \n        if var component = terrain.modelComponent {\n            component.materials = [material]\n            terrain.components.set(component)\n        }\n        \n        try terrain.update(shaderGraphMaterial: terrainMaterial, { m in\n            try m.setParameter(name: materialParameterName, value: .float(viewModel.sliderValue))\n        })\n    } catch {\n        print(\"problem: \\(error)\")\n    }\n}\n", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/swift-splash", "title": "Swift Splash | Apple Developer Documentation", "content": "App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space   Apple Vision Pro\u2019s ability to combine virtual content seamlessly with the real world allows for many kinds of interactive virtual experiences. Swift Splash leverages RealityKit and Reality Composer Pro to create a virtual water slide by combining modular slide pieces. When the builder finishes their ride, they can release an adventurous goldfish to try it out. Swift Splash uses multiple Reality Composer Scenes to create prepackaged entity hierarchies that represent each of the slide pieces the player connects to construct their ride. It demonstrates how to hide and reveal sections of the entity hierarchy based on the current state of the app. For example, each slide piece contains an animated fish entity that\u2019s hidden until the ride runs and the fish arrives at that particular piece. While Swift Splash is a fun, game-like experience, the core idea of assembling virtual objects out of predefined parts can also be used as the basis for a productivity or creation app. Swift Splash scenes include Shader Graph materials built in Reality Composer Pro to change the appearance of the ride at runtime. Each piece can be configured to display in one of three materials: metal, wood, or plastic. Other Shader Graph materials create special effects, such as the movement of the water and the flashing lights on the start and end pieces. Even particle effects are included in some of these prepackaged entities, such as the fireworks that play when the goldfish crosses the finish line. Slide pieces are the building blocks of Swift Splash. The Reality Composer project contains a separate scene for each one. In addition to the 3D models that make up the slide piece, each scene contains a number of other entities the app uses to animate and place the slide piece.  In the hierarchy viewer on the left side of the screenshot above, there are two transform entities called connect_in and connect_out. These transforms mark the points where the slide piece connects to the next or previous piece. Swift Splash uses these transforms to place new pieces at the end of the existing slide, as well as to snap pieces to other slide pieces when you manually move them near each other. Slide pieces demonstrate the two primary mechanisms Swift Splash uses to find entities at runtime. For some entities, such as connect_in, Swift Splash uses a naming convention and retrieves the entities by name or suffix when it needs to use them. In other cases, such as when names aren\u2019t unique or the retrieving code needs configuration values, Swift Splash uses a custom component to mark and retrieve entities. For example, animated entities that appear when the ride runs contain a component called RideAnimationComponent. The app uses this component to determine if the entity is an animation that plays while the ride is running. The component also stores additional state the app needs to implement the ride animation, such as a property called duration that specifies when to start the animations on the next connected slide piece. RideAnimationComponent also includes a property called isPersistent. Persistent ride animations stay visible at all times but only animate when the ride is running, such as the animated door on the start piece. Nonpersistent ride animations, such as the fish swimming through a slide piece, display only while the ride is running and the fish swims through that particular piece. Many of Swift Splash\u2019s slide pieces use the same materials. For example, the shader graph material that changes pieces from metal to wood to plastic is shared by all but one of the slide pieces. To avoid having duplicate copies of each material, Swift Splash leverages USD material references to share materials between multiple entities in multiple scenes. The Reality Composer Pro project contains a separate scene for each shared material, containing only that one material. Other track pieces create references to that material. If you change the original material, it affects all of the entities that reference it. For example, a scene called M_RainbowLights.usda contains the material M_RainbowLights, and both StartPiece.usda and EndPiece.usda reference that material.  To maximize load speed and make the most efficient use of available compute resources, Swift Splash parallelizes loading scenes from the Reality Composer project using a TaskGroup. The app creates a separate Task for each of the scenes it needs to load. The app then uses an async iterator to wait for and receive the results. For more information on task groups, see Concurrency in The Swift Programming Language. Each of these loaded pieces acts as a template. When the player adds a new piece of that type, the app clones the piece loaded from Reality Composer Pro and adds the clone to the scene. When multiple entities have more than one overlapping, nonopaque material, RealityKit\u2019s default depth-sorting can cause it to draw those entities in the wrong order. As a result, some entities may not be visible from certain angles or in certain positions relative to other transparent entities. The default depth sorting is based on the center of the entity\u2019s bounding box, which may result in the incorrect drawing order when there are multiple overlapping materials with any amount of transparency. You can see an example of this by looking at the start piece in Reality Composer Pro, or by watching the video below. The following video demonstrates the problem. If the three boxes are the bounding boxes for three different transparent entities, and the small spheres are the box centers, the sphere that\u2019s closest to the camera changes as the camera moves around the boxes, which changes the order that RealityKit\u2019s default depth sorting algorithm draws them. Swift Splash assigns a ModelSortGroupComponent to each of the transparent entities to manually specify the relative depth sorting. To fix the transparency issues in the start piece in the video above, Swift Splash instructs RealityKit to draw the opaque parts of the fish first, its transparent goggles second, the water third, the glass globe fourth, and the selection glow shell last. Swift Splash does this by assigning a ModelSortGroupComponent to each of the overlapping entities using the same ModelSortGroup, but with a different order specified. The root entity for all of the individual slide pieces has a ConnectableComponent. This custom component marks the entity as one that can be connected or snapped to other connectable entities. At runtime, the app adds a ConnectableStateComponent to each slide piece it adds. The component stores state information for the track piece that doesn\u2019t need to be edited in Reality Composer Pro. Among the state information that this component stores is a reference to the next and previous piece. To iterate through the entire ride, ignoring any disconnected pieces, the app gets a reference to the start piece and then iterates until nextPiece is nil. This iteration, similar to iterating a linked list, repeats many times throughout the app. One example is the function that calculates the duration of the built ride by iterating through the individual pieces and adding up the duration of their animations. To build and edit the ride, players interact with Swift Splash in two different ways. They interact with SwiftUI windows to perform certain tasks, such as adding a new piece or deleting an existing piece of the ride. They also manipulate slide pieces using standard visionOS gestures, including taps, double taps, drags, and rotates. The player taps on a piece to select or deselect it. When a player double taps a piece, they select that piece without deselecting any other selected pieces. When someone drags a piece, it moves around the immsersive space, snapping together with other pieces if placed near one. A two-finger rotate gesture spins the selected track piece or pieces on the Z-axis. Swift Splash handles all of these interactions using standard SwiftUI gestures targeted to an entity. To support any of these gestures at any time, the app declares them using SimultaneousGesture. The code for all of the gestures are contained in TrackBuildingView, which controls the app\u2019s immersive space. Here\u2019s how the app defines the rotation gesture: Because multiple tap gestures on the same RealityView execute with a different number of taps, multiple gestures may be called at once. If a player double taps an entity, for example, both the single tap and the double tap gesture code get called, and the app has to determine which one to execute. Swift Splash makes this determination by using a Boolean state variable. If a player single taps, it sets that variable\u00a0\u2014 called shouldSingleTap \u2014 to true. Then it waits for a period of time before executing the rest of its code. If shouldSingleTap gets set to false while it\u2019s waiting, the code doesn\u2019t execute. When SwiftSplash detects a double tap gesture, it sets shouldSingleTap to false, preventing the single-tap code from firing when it executes the double-tap code.\n\n\n\nawait withTaskGroup(of: LoadResult.self) { taskGroup in   \n    // Load the regular slide pieces and ride animations.\n    logger.info(\"Loading slide pieces.\")\n    for piece in pieces {\n        taskGroup.addTask {\n            do {\n                guard let pieceEntity = try await self.loadFromRCPro(named: piece.key.rawValue, \n                                                       fromSceneNamed: piece.sceneName) else {\n                    fatalError(\"Attempted to load piece entity \\(piece.name) but failed.\")\n                }\n                return LoadResult(entity: pieceEntity, key: piece.key.rawValue)\n            } catch {\n                fatalError(\"Attempted to load \\(piece.name) but failed: \\(error.localizedDescription)\")\n            }\n        }\n    }\n    // Continue adding asset load jobs.\n    // ...\n}\n\n\n\n\nfor await result in taskGroup {\n    if let pieceKey = pieces.filter({ piece in\n        piece.key.rawValue == result.key\n    }).first {\n        self.add(template: result.entity, for: pieceKey.key)\n        setupConnectible(entity: result.entity)\n        result.entity.generateCollisionShapes(recursive: true)\n        result.entity.setUpAnimationVisibility()\n    }\n    // ...\n}\n\n\n\n\nfileprivate func setEntityDrawOrder(_ entity: Entity, _ sortOrder: Int32, _ sortGroup: ModelSortGroup) {\n    entity.forEachDescendant(withComponent: ModelComponent.self) { modelEntity, model in\n        logger.info(\"Setting sort order of \\(sortOrder) of \\(entity.name), child entity: \\(modelEntity.name)\")\n        let component = ModelSortGroupComponent(group: sortGroup, order: sortOrder)\n        modelEntity.components.set(component)\n    }\n}\n\n\n/// Manually specify sort ordering for the transparent start piece meshes.\nfunc handleStartPieceTransparency(_ startPiece: Entity) {\n    let group = ModelSortGroup()\n    \n    // Opaque fish parts.\n    if let entity = startPiece.findEntity(named: fishIdleAnimModelName) {\n        setEntityDrawOrder(entity, 1, group)\n    }\n    if let entity = startPiece.findEntity(named: fishRideAnimModelName) {\n        setEntityDrawOrder(entity, 2, group)\n    }\n    \n    // Transparent fish parts.\n    if let entity = startPiece.findEntity(named: fishGlassIdleAnimModelName) {\n        setEntityDrawOrder(entity, 3, group)\n    }\n    if let entity = startPiece.findEntity(named: fishGlassRideAnimModelName) {\n        setEntityDrawOrder(entity, 4, group)\n    }\n    \n    // Water.\n    if let entity = startPiece.findEntity(named: sortOrderWaterName) {\n        setEntityDrawOrder(entity, 5, group)\n    }\n    \n    // Glass globe.\n    if let entity = startPiece.findEntity(named: sortOrderGlassGlobeName) {\n        setEntityDrawOrder(entity, 6, group)\n    }\n    \n    // Selection glow.\n    if let entity = startPiece.findEntity(named: startGlowName) {\n        setEntityDrawOrder(entity, 7, group)\n    }\n}\n\n\n\n\n/// Calculates the duration of the built ride by summing up the individual durations.\npublic func calculateRideDuration() {\n    guard let startPiece = startPiece else { fatalError(\"No start piece found.\") }\n    var nextPiece: Entity? = startPiece\n    var duration: TimeInterval = 0\n    while nextPiece != nil {\n        // Some pieces have more than one ride animation. Use the longest one to calculate duration.\n        var longestAnimation: TimeInterval = 0\n        nextPiece?.forEachDescendant(withComponent: RideAnimationComponent.self) { entity, component in\n            longestAnimation = max(component.duration, longestAnimation)\n        }\n        duration += longestAnimation\n        nextPiece = nextPiece?.connectableStateComponent?.nextPiece\n    }\n    // Remove the part of the animation after the goal post.\n    rideDuration = duration  / animationSpeedMultiplier + 1.0\n}\n\n\n\n\n.simultaneousGesture(\n    RotateGesture()\n        .targetedToAnyEntity()\n        .onChanged({ value in\n            guard appState.phase == .buildingTrack || appState.phase == .placingStartPiece || appState.phase == .draggingStartPiece else { return }\n            handleRotationChanged(value)\n        })\n        .onEnded({ value in\n            guard appState.phase == .buildingTrack || appState.phase == .placingStartPiece || appState.phase == .draggingStartPiece else { return }\n            handleRotationChanged(value, isEnded: true)\n        })\n)\n\n\n\n\n.simultaneousGesture(\n    TapGesture()\n        .targetedToAnyEntity()\n        .onEnded({ value in\n            guard appState.phase == .buildingTrack else { return }\n            Task {\n                shouldSingleTap = true\n                try? await Task.sleep(for: .seconds(doubleTapTolerance))\n                if shouldSingleTap {\n", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos#topics", "title": "visionOS | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space  / visionOS API Changes: None visionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.  Developing for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app\u2019s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS. Start with a familiar window-based experience to introduce people to your content. From there, add SwiftUI scene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences. Build your app\u2019s 3D content with RealityKit and Reality Composer Pro, and display it with a RealityView. In an immersive experience, use ARKit to integrate your content with the person\u2019s surroundings.  People can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures. SwiftUI provides built-in support for these standard gestures, so rely on them for most of your app\u2019s input. When you want to go beyond the standard gestures, use ARKit to create custom gestures. Tap to select Pinch to rotate Manipulate objects Create custom gestures Explore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash. visionOS Overview Expand your app into immersive spaces Explore new kinds of interaction Dive into featured sample apps Topics Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos#app-construction", "title": "visionOS | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space  / visionOS API Changes: None visionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.  Developing for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app\u2019s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS. Start with a familiar window-based experience to introduce people to your content. From there, add SwiftUI scene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences. Build your app\u2019s 3D content with RealityKit and Reality Composer Pro, and display it with a RealityView. In an immersive experience, use ARKit to integrate your content with the person\u2019s surroundings.  People can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures. SwiftUI provides built-in support for these standard gestures, so rely on them for most of your app\u2019s input. When you want to go beyond the standard gestures, use ARKit to create custom gestures. Tap to select Pinch to rotate Manipulate objects Create custom gestures Explore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash. visionOS Overview Expand your app into immersive spaces Explore new kinds of interaction Dive into featured sample apps Topics Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/creating-your-first-visionos-app", "title": "Creating your first visionOS app | Apple Developer Documentation", "content": "App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space   If you\u2019re new to visionOS, start with a new Xcode project to learn about the platform features, and to familiarize yourself with visionOS content and techniques. When you build an app for visionOS, SwiftUI is an excellent choice because it gives you full access to visionOS features. Although you can also use UIKit to build portions of your app, you need to use SwiftUI for many features that are unique to the platform. Note Developing for visionOS requires a Mac with Apple silicon. In any SwiftUI app, you place content onscreen using scenes. A scene contains the views and controls to display onscreen. Scenes also define the appearance of those views and controls when they appear onscreen. In visionOS, you can include both 2D and 3D views in the same scene, and you can present those views in a window or as part of the person\u2019s surroundings. Scene with a window Scene with a window and 3D objects Start with a new Xcode project and add features to familiarize yourself with visionOS content and techniques. Run your app in Simulator to verify your content looks like you expect, and run it on device to see your 3D content come to life. Organize your content around one or more scenes, which manage your app\u2019s interface. Each scene contains the views and controls you want to display, and the scene type determines whether your content adopts a 2D or 3D appearance. SwiftUI adds 3D scene types specifically for visionOS, and also adds 3D elements and layout options for all scene types. Create a new project in Xcode by choosing File > New > Project. Navigate to the visionOS section of the template chooser, and choose the App template. When prompted, specify a name for your project along with other options. When creating a new visionOS app, you can configure your app\u2019s initial scene types from the configuration dialog. To display primarily 2D content in your initial scene, choose a Window as your initial scene type. For primarily 3D content, choose a Volume. You can also add an immersive scene to place your content in the person\u2019s surroundings.  Include a Reality Composer Pro project file when you want to create 3D assets or scenes to display from your app. Use this project file to build content from primitive shapes and existing USDZ assets. You can also use it to build and test custom RealityKit animations and behaviors for your content. Build your initial interface using standard SwiftUI views. Views provide the basic content for your interface, and you customize the appearance and behavior of them using SwiftUI modifiers. For example, the .background modifier adds a partially transparent tint color behind your content: To learn more about how to create and configure interfaces using SwiftUI, see SwiftUI Essentials. Many SwiftUI views handle interactions automatically \u2014 all you do is provide code to run when the interactions occur. You can also add SwiftUI gesture recognizers to a view to handle tap, long-press, drag, rotate, and zoom gestures. The system automatically maps the following types of input to your SwiftUI event-handling code:  Indirect input. The person\u2019s eyes indicate the target of an interaction. To start the interaction, the person touches their thumb and forefinger together on one or both hands. Additional finger and hand movements define the gesture type.  Direct input. When a person\u2019s finger occupies the same space as an onscreen item, the system reports an interaction. Additional finger and hand movements define the gesture type.  Keyboard input. People can use a connected mouse, trackpad, or keyboard to interact with items, trigger menu commands, and perform gestures. For more information about handling interactions in SwiftUI views, see Handling User Input in the SwiftUI Essentials tutorial. Build and run your app in Simulator to see how it looks. Simulator for visionOS has a virtual background as the backdrop for your app\u2019s content. Use your keyboard and your mouse or trackpad to navigate around the environment and interact with your app. Tap and drag the window bar below your app\u2019s content to reposition the window in the environment. Move the pointer over the circle next to the window bar to reveal the window\u2019s close button. Move the cursor to one of the window\u2019s corners to turn the window bar into a resizing control. Note Apps don\u2019t control the placement of windows in the space. The system places each window in its initial position, and updates that position based on further interactions with the app. For additional information about how to interact with your app in Simulator, see Interacting with your app in the visionOS simulator.\n\n\n\n@main\nstruct MyApp: App {\n    var body: some Scene {\n        WindowGroup {\n            ContentView()\n               .background(.black.opacity(0.8))\n        }\n\n\n        ImmersiveSpace(id: \"Immersive\") {\n            ImmersiveView()\n        }\n    }\n}\n", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/adding-3d-content-to-your-app", "title": "Adding 3D content to your app | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space  / visionOS / Adding 3D content to your app API Changes: None A device with a stereoscopic display lets people experience 3D content in a way that feels more real. Content appears to have real depth, and people can view it from different angles, making it seem like it\u2019s there in front of them. When building an app for visionOS, think about ways you might add depth to your app\u2019s interface. The system provides several ways to display 3D content, including in your existing windows, in a volume, and in an immersive space. Choose the options that work best for your app and the content you offer. Window Volume Immersive space Windows are an important part of your app\u2019s interface. With visionOS, apps automatically get materials with the visionOS look and feel, fully resizable windows with spacing tuned for eyes and hands input, and access to highlighting adjustments for your custom controls. Incorporate depth effects into your custom views as needed, and use 3D layout options to arrange views in your windows. Apply a shadow(color:radius:x:y:) or visualEffect(_:) modifier to the view. Apply a shadow(color:radius:x:y:) or visualEffect(_:) modifier to the view. Lift or highlight the view when someone looks at it using a hoverEffect(_:isEnabled:) modifier. Lift or highlight the view when someone looks at it using a hoverEffect(_:isEnabled:) modifier. Lay out views using a ZStack. Lay out views using a ZStack. Animate view-related changes with transform3DEffect(_:). Animate view-related changes with transform3DEffect(_:). Rotate the view using a rotation3DEffect(_:axis:anchor:anchorZ:perspective:) modifier. Rotate the view using a rotation3DEffect(_:axis:anchor:anchorZ:perspective:) modifier. In addition to giving 2D views more depth, you can also add static 3D models to your 2D windows. The Model3D view loads a USDZ file or other asset type and displays it at its intrinsic size in your window. Use this in places where you already have the model data in your app, or can download it from the network. For example, a shopping app might use this type of view to display a 3D version of a product. RealityKit is Apple\u2019s technology for building 3D models and scenes that you update dynamically onscreen. In visionOS, use RealityKit and SwiftUI together to seamlessly couple your app\u2019s 2D and 3D content. Load existing USDZ assets or create scenes in Reality Composer Pro that incorporate animation, physics, lighting, sounds, and custom behaviors for your content. To use a Reality Composer Pro project in your app, add the Swift package to your Xcode project and import its module in your Swift file. For more information, see Managing files and folders in your Xcode project.  When you\u2019re ready to display 3D content in your interface, use a RealityView. This SwiftUI view serves as a container for your RealityKit content, and lets you update that content using familiar SwiftUI techniques. The following example shows a view that uses a RealityView to display a 3D sphere. The code in the view\u2019s closure creates a RealityKit entity for the sphere, applies a texture to the surface of the sphere, and adds the sphere to the view\u2019s content. When SwiftUI displays your RealityView, it executes your code once to create the entities and other content. Because creating entities is relatively expensive, the view runs your creation code only once. When you want to update the state of your entities, change the state of your view and use an update closure to apply those changes to your content. The following example uses an update closure to change the size of the sphere when the value in the scale property changes: For information about how to create content using RealityKit, see RealityKit. To handle interactions with the entities of your RealityKit scenes: Attach a gesture recognizer to your RealityView and add the targetedToAnyEntity() modifier to it. Attach a gesture recognizer to your RealityView and add the targetedToAnyEntity() modifier to it. Attach an InputTargetComponent to the entity or one of its parent entities. Attach an InputTargetComponent to the entity or one of its parent entities. Add collision shapes to the RealityKit entities that support interactions. Add collision shapes to the RealityKit entities that support interactions. The targetedToAnyEntity() modifier provides a bridge between the gesture recognizer and your RealityKit content. For example, to recognize when someone drags an entity, specify a DragGesture and add the modifier to it. When the specified gesture occurs on an entity, SwiftUI executes the provided closure. The following example adds a tap gesture recognizer to the sphere view from the previous example. The code also adds InputTargetComponent and CollisionComponent components to the shape to allow the interactions to occur. If you omit these components, the view doesn\u2019t detect the interactions with your entity. A volume is a type of window that grows in three dimensions to match the size of the content it contains. Windows and volumes both accommodate 2D and 3D content, and are alike in many ways. However, windows clip 3D content that extends too far from the window\u2019s surface, so volumes are the better choice for content that is primarily 3D. To create a volume, add a WindowGroup scene to your app and set its style to volumetric. This style tells SwiftUI to create a window for 3D content. Include any 2D or 3D views you want in your volume. You can also add a RealityView to build your content using RealityKit. The following example creates a volume with a static 3D model of some balloons stored in the app\u2019s bundle: Windows and volumes are a convenient way to display bounded 2D and 3D content, but your app doesn\u2019t control the placement of that content in the person\u2019s surroundings. The system sets the initial position of each window and volume at display time. The system also adds a window bar to allow someone to reposition the window or resize it.  For more information about when to use volumes, see Human Interface Guidelines > Windows. When you need more control over the placement of your app\u2019s content, add that content to an ImmersiveSpace. An immersive space offers an unbounded area for your content, and you control the size and placement of content within the space. After receiving permission from the user, you can also use ARKit with an immersive space to integrate content into their surroundings. For example, you can use ARKit scene reconstruction to obtain a mesh of furniture and nearby objects and have your content interact with that mesh. An ImmersiveSpace is a scene type that you create alongside your app\u2019s other scenes. The following example shows an app that contains an immersive space and a window: If you don\u2019t add a style modifier to your ImmersiveSpace declaration, the system creates that space using the mixed style. This style displays your content together with the passthrough content that shows the person\u2019s surroundings. Other styles let you hide passthrough to varying degrees. Use the immersionStyle(selection:in:) modifier to specify which styles your space supports. If you specify more than one style, you can toggle between the styles using the selection parameter of the modifier. Warning Be mindful of how much content you include in immersive scenes that use the mixed style. Content that fills a significant portion of the screen, even if that content is partially transparent, can prevent the person from seeing potential hazards in their surroundings. If you want to immerse the person in your content, configure your space with the full style. For more information, see, Creating fully immersive experiences in your app. Remember to set the position of items you place in an ImmersiveSpace. Position SwiftUI views using modifiers, and position a RealityKit entity using its transform component. SwiftUI places the origin of a space at a person\u2019s feet initially, but can change this origin in response to other events. For example, the system might shift the origin to accommodate a SharePlay activity that displays your content with Spatial Personas. If you need to position SwiftUI views and RealityKit entities relative to one another, perform any needed coordinate conversions using the methods in the content parameter of RealityView. To display your ImmersiveSpace scene, open it using the openImmersiveSpace action, which you obtain from the SwiftUI environment. This action runs asynchronously and uses the provided information to find and initialize your scene. The following example shows a button that opens the space with the solarSystem identifier: When an app presents an ImmersiveSpace, the system hides the content of other apps to prevent visual conflicts. The other apps remain hidden while your space is visible but return when you dismiss it. If your app defines multiple spaces, you must dismiss the currently visible space before displaying a different space. If you don\u2019t dismiss the visible space, the system issues a runtime warning when you try to open the other space. Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC\n\n\n\nstruct SphereView: View {\n    var body: some View {\n        RealityView { content in\n            let model = ModelEntity(\n                         mesh: .generateSphere(radius: 0.1),\n                         materials: [SimpleMaterial(color: .white, isMetallic: true)])\n            content.add(model)\n        }\n    }\n}\n\n\n\nstruct SphereView: View {\n    var scale = false\n\n\n    var body: some View {\n        RealityView { content in\n            let model = ModelEntity(\n                         mesh: .generateSphere(radius: 0.1),\n                         materials: [SimpleMaterial(color: .white, isMetallic: true)])\n            content.add(model)\n        } update: { content in\n            if let model = content.entities.first {\n                model.transform.scale = scale ? [1.2, 1.2, 1.2] : [1.0, 1.0, 1.0]\n            }\n        }\n    }\n}\n\n\n\nstruct SphereView: View {\n    @State private var scale = false\n\n\n    var body: some View {\n        RealityView { content in\n            let model = ModelEntity(\n                mesh: .generateSphere(radius: 0.1),\n                materials: [SimpleMaterial(color: .white, isMetallic: true)])\n\n\n            // Enable interactions on the entity.\n            model.components.set(InputTargetComponent())\n            model.components.set(CollisionComponent(shapes: [.generateSphere(radius: 0.1)]))\n            content.add(model)\n        } update: { content in\n            if let model = content.entities.first {\n                model.transform.scale = scale ? [1.2, 1.2, 1.2] : [1.0, 1.0, 1.0]\n            }\n        }\n        .gesture(TapGesture().targetedToAnyEntity().onEnded { _ in\n            scale.toggle()\n        })\n    }\n}\n\n\n\nstruct MyApp: App {\n    var body: some Scene {\n        WindowGroup {\n            Model3D(\"balloons\")\n        }.windowStyle(style: .volumetric)\n    }\n}\n\n\n\n@main\nstruct MyImmersiveApp: App {\n    var body: some Scene {\n        WindowGroup() {\n            ContentView()\n        }\n\n\n        ImmersiveSpace(id: \"solarSystem\") {\n            SolarSystemView()\n        }\n    }\n}\n\n\n\nButton(\"Show Solar System\") {\n    Task {\n        let result = await openImmersiveSpace(id: \"solarSystem\")\n        if case .error = result {\n            print(\"An error occurred\")\n        }\n    }\n}", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/creating-fully-immersive-experiences", "title": "Creating fully immersive experiences in your app | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space  / visionOS / Creating fully immersive experiences in your app API Changes: None A fully immersive experience replaces everything the person sees with custom content you create. You might use this type of experience to: Offer a temporary transitional experience Offer a temporary transitional experience Create a distraction-free space for your content Create a distraction-free space for your content Implement a virtual reality (VR) game Implement a virtual reality (VR) game Present a virtual world to explore Present a virtual world to explore With a fully immersive experience, you\u2019re responsible for everything that appears onscreen. The system hides passthrough video and displays the content you provide, showing the person\u2019s hands only when they come into view. To achieve the best performance, use RealityKit or Metal to create and animate your content. Typically, you combine a fully immersive experience with other types of experiences and provide transitions between them. When you display a window first and then offer controls to enter your immersive experience, you give people time to prepare for the transition. It also gives them the option to skip the experience if they prefer to use your app\u2019s windows instead. Give people control over when they enter or exit fully immersive experiences, and provide clear transitions to and from those experiences. Clear visual transitions make it easier to adjust to such a large change. Sudden transitions might be disorienting, unpleasant, or make the person think something went wrong. At launch time, display windows or other content that allows the person to see their surroundings. Add controls to that content to initiate the transition to the fully immersive experience, and provide a clear indication of what the controls do. Inside your experience, provide clear controls and instructions on how to exit the experience. Warning When you start a fully immersive experience, visionOS defines a system boundary that extends approximately 1.5 meters from the initial position of the person\u2019s head. If their head moves outside of that zone, the system automatically stops the immersive experience and turns on the external video again. This feature is an assistant to help prevent someone from colliding with objects. For guidelines on how to design fully immersive experiences, see Human Interface Guidelines. To create a fully immersive experience, open an ImmersiveSpace and set its style to full. An immersive space is a type of SwiftUI scene that lets you place content anywhere in the person\u2019s surroundings. Applying the full style to the scene tells the system to hide passthrough video and display only your app\u2019s content. Declare spaces in the body property of your app object, or anywhere you manage SwiftUI scenes. The following example shows an app with a main window and a fully immersive space. At launch time, the app displays the window. To display an ImmersiveSpace, open it using the openImmersiveSpace action, which you obtain from the SwiftUI environment. This action runs asynchronously and uses the provided information to find and initialize your scene. The following example shows a button that opens the space with the solarSystem identifier: An app can display only one space at a time, and it\u2019s an error for you to try to open a space while another space is visible. To dismiss an open space, use the dismissImmersiveSpace action. For more information about displaying spaces, see the ImmersiveSpace type. RealityKit works well when your content consists of primitive shapes or existing content in USD files. Organize the contents of your scene using RealityKit entities, and animate that content using components and systems. Use Reality Composer Pro to assemble your content visually, and to attach dynamic shaders, animations, audio, and other behaviors to your content. Display the contents of your RealityKit scene in a RealityView in your scene. To load a Reality Composer Pro scene at runtime, fetch the URL of your Reality Composer Pro package file, and load the root entity of your scene. The following example shows how to create the entity for a package located in the app\u2019s bundle: For more information about how to display content in a RealityView and manage interactions with your content, see Adding 3D content to your app. Another option for creating fully immersive scenes is to draw everything yourself using Metal. When using Metal to draw your content, use the Compositor Services framework to place that content onscreen. Compositor Services provides the code you need to set up your Metal rendering engine and start drawing. For details on how to render content using Metal and Compositor Services, and manage interactions with your content, see Drawing fully immersive content using Metal. Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC\n\n\n\n@main\nstruct MyImmersiveApp: App {\n    @State private var currentStyle: ImmersionStyle = .full\n\n\n    var body: some Scene {\n        WindowGroup() {\n            ContentView()\n        }\n\n\n        // Display a fully immersive space.\n        ImmersiveSpace(id: \"solarSystem\") {\n            SolarSystemView()\n        }.immersionStyle(selection: $currentStyle, in: .full)\n    }\n}\n\n\n\nButton(\"Show Solar System\") {\n    Task {\n        let result = await openImmersiveSpace(id: \"solarSystem\")\n        if case .error = result {\n            print(\"An error occurred\")\n        }\n    }\n}\n\n\n\nimport MyRealityBundle\n\n\nlet url = MyRealityBundle.bundle.url(forResource:\n         \"MyRealityBundle\", withExtension: \"reality\")\nlet scene = try await Entity(contentsOf: url)", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/drawing-sharp-layer-based-content", "title": "Drawing sharp layer-based content in visionOS | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space  / visionOS / Drawing sharp layer-based content in visionOS API Changes: None If your app uses Core Animation layers directly, update your layer code to draw a high-resolution version of your content when appropriate. SwiftUI and UIKit views use Core Animation layers to manage interface content efficiently. When a view draws its content, the underlying layer captures that content and caches it to improve subsequent render operations. Core Animation on most Apple platforms rasterizes your layer at the same resolution as the screen, but Core Animation on visionOS can rasterize at different resolutions to maximize both content clarity and performance. The system follows the person\u2019s eyes and renders content immediately in front of them at the highest possible resolution. Outside of this focal area, the system renders content at progressively lower resolutions to reduce GPU workloads. Because the content is in the person\u2019s peripheral vision, these lower resolutions don\u2019t impact the content\u2019s clarity. As the person\u2019s eyes move around, the system redraws content at different resolutions to match the change in focus. A figure at 2x resolution A figure at 8x resolution If you deliver content using custom CALayer objects, you can configure your custom layers to support drawing at different resolutions. If you don\u2019t perform this extra configuration step, each layer rasterizes its content at a @2x scale factor, which is good enough for most content and matches what the layer provides on a Retina display. However, if you opt in to drawing at different resolutions, the layer rasterizes its content at up to @8x scale factor in visionOS, which adds significant detail to text and vector-based content. Dynamic content scaling is off by default for all Core Animation layers, and frameworks or apps must turn on this support explicitly. If your interface uses only SwiftUI or UIKit views, you don\u2019t need to do anything to support this feature. SwiftUI and UIKit enable it automatically for views that benefit from the added detail, such as text views and image views with SF Symbols or other vector-based artwork. However, the frameworks don\u2019t enable the feature for all views, including UIView and View. If your visionOS interface includes custom Core Animation layers, you can enable the wantsDynamicContentScaling property of any CALayer objects that contain vector-based content. Setting this property to true tells the system that you support rendering your layer\u2019s content at different resolutions. However, the setting is not a guarantee that the system applies dynamic content scaling to your content. The system can disable the feature if your layer draws using incompatible functions or techniques. The following example shows how to enable this feature for a CATextLayer object. After configuring the layer, set the wantsDynamicContentScaling property to true and add the layer to your layer hierarchy. Dynamic content scaling works best when the layer contains text or vector-based content. Don\u2019t enable the feature if you do any of the following in your layer: You set the layer\u2019s content using the contents property. You set the layer\u2019s content using the contents property. You draw primarily bitmap-based content. You draw primarily bitmap-based content. You redraw your layer\u2019s contents repeatedly over a short time period. You redraw your layer\u2019s contents repeatedly over a short time period. The CAShapeLayer class ignores the value of the wantsDynamicContentScaling property and always enables dynamic content scaling. For other Core Animation layers, you must enable the feature explicitly to take advantage of it. Dynamic content scaling requires you to draw your layer\u2019s contents using one of the prescribed methods. If you define a custom subclass of CALayer, draw your layer\u2019s content in the draw(in:) method. If you use a CALayerDelegate object to draw the layer\u2019s content, use the delgate\u2019s draw(_:in:) method instead. When you enable dynamic content scaling for a layer, the system captures your app\u2019s drawing commands for playback later. As the person\u2019s eyes move, the system draws the layer at higher resolutions when someone looks directly at it, or at lower resolutions otherwise. Because the redraw operations implicitly communicate what the person is looking at, the system performs them outside of your app\u2019s process. Letting the system handle these operations maintains the person\u2019s privacy while still giving your app the benefits of high-resolution drawing. Some Core Graphics routines are incompatible with dynamic content scaling. Even if you enable dynamic content scaling for your layer, the system automatically disables the feature if your layer uses any of the following: Core Graphics shaders. Core Graphics shaders. APIs that set intent, quality, or other bitmap-related properties. For example, don\u2019t call CGContextSetInterpolationQuality. APIs that set intent, quality, or other bitmap-related properties. For example, don\u2019t call CGContextSetInterpolationQuality. A CGBitmapContext to draw content. A CGBitmapContext to draw content. If your app creates timer-based animations, don\u2019t animate layer changes using your drawing method. Calling setNeedsDisplay() on your layer repeatedly in a short time causes the system to draw the layer multiple times in quick succession. Because visionOS needs a little extra time to draw a layer at high resolution, each redraw request forces it to throw away work. A better option is to animate layer-based properties to achieve the same effect, or use a CAShapeLayer to animate paths when needed. The backing store for a layer consumes more memory at higher resolutions than at lower resolutions. Measure your app\u2019s memory usage before and after you enable dynamic content scaling to make sure the increased memory cost is worth the benefit. If your app\u2019s memory usage increases too much, limit which layers adopt dynamic content scaling. You can also reduce the amount of memory each layer uses in the following ways: Make your layer the smallest size possible. Larger layers require significantly more memory, especially at higher resolutions. Make the size of the layer match the size of your content by eliminating padding or extra space. Make your layer the smallest size possible. Larger layers require significantly more memory, especially at higher resolutions. Make the size of the layer match the size of your content by eliminating padding or extra space. Separate complex content into different layers. Instead of drawing everything in a single layer, build your content from multiple layers and arrange them hierarchically to achieve the same result. Enable dynamic content scaling only in the layers that actually need it. Separate complex content into different layers. Instead of drawing everything in a single layer, build your content from multiple layers and arrange them hierarchically to achieve the same result. Enable dynamic content scaling only in the layers that actually need it. Apply special effects using layer properties whenever possible. Applying effects during drawing might require you to increase the layer\u2019s size. For example, apply scale and rotation effects to the layer\u2019s transform property, instead of during drawing. Apply special effects using layer properties whenever possible. Applying effects during drawing might require you to increase the layer\u2019s size. For example, apply scale and rotation effects to the layer\u2019s transform property, instead of during drawing. Don\u2019t draw your layer\u2019s content at different resolutions in advance and cache the images. Maintaining multiple images requires more memory. If you do cache images, draw them only at @2x scale factor. Don\u2019t draw your layer\u2019s content at different resolutions in advance and cache the images. Maintaining multiple images requires more memory. If you do cache images, draw them only at @2x scale factor. Don\u2019t use your drawing code to draw a single image. If your layer\u2019s content consists of an image, assign that image to the layer\u2019s contents property directly. Don\u2019t use your drawing code to draw a single image. If your layer\u2019s content consists of an image, assign that image to the layer\u2019s contents property directly. Complex drawing code can also lead to performance issues. A layer with many strokes can render quickly at lower scale factors, but might be computationally too complex to render at larger scales. If a complex layer doesn\u2019t render correctly at higher resolutions, turn off dynamic content scaling and measure the render times again. Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC\n\n\n\nlet layer = CATextLayer()\n\n\nlayer.string = \"Hello, World!\"\nlayer.foregroundColor = UIColor.black.cgColor\nlayer.frame = parentLayer.bounds\n\n\n// Setting this property to true enables content scaling \n// and calls setNeedsDisplay to redraw the layer's content.\nlayer.wantsDynamicContentScaling = true\n\n\nparentLayer.addSublayer(layer)", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos#design", "title": "visionOS | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode / visionOS API Changes: None visionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.  Developing for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app\u2019s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS. Start with a familiar window-based experience to introduce people to your content. From there, add SwiftUI scene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences. Build your app\u2019s 3D content with RealityKit and Reality Composer Pro, and display it with a RealityView. In an immersive experience, use ARKit to integrate your content with the person\u2019s surroundings.  People can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures. SwiftUI provides built-in support for these standard gestures, so rely on them for most of your app\u2019s input. When you want to go beyond the standard gestures, use ARKit to create custom gestures. Tap to select Pinch to rotate Manipulate objects Create custom gestures Explore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash. visionOS Overview Expand your app into immersive spaces Explore new kinds of interaction Dive into featured sample apps Topics Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/adopting-best-practices-for-privacy", "title": "Adopting best practices for privacy and user preferences | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space  / visionOS / Adopting best practices for privacy and user preferences API Changes: None To protect user privacy, the system handles camera and sensor inputs without passing the information to apps directly. Instead, the system enables your app to seamlessly interact with a user\u2019s surroundings and to automatically receive input from the user. For example, the system handles the eye- and hand-position data needed to detect interactions with your app\u2019s content. Similarly, the system provides a way to automatically alter a view\u2019s appearance when someone looks at it, without your app ever knowing what the user is looking at. In the few cases where you actually need access to hand position or information about the user\u2019s surroundings, the system requires you to obtain authorization from the user first.  Important It\u2019s your responsibility to protect any data your app collects, and to use it in responsible and privacy-preserving ways. Don\u2019t ask for data that you don\u2019t need, be transparent about how you use the data you acquire, and respect the choices of the person whose data it is. For information about how to specify the privacy data your app uses, see Describing data use in privacy manifests. For general information about privacy, see Protecting the User\u2019s Privacy. On Apple Vision Pro, people use their eyes and hands to interact with the items they see in front of them. Where they look determines where the system applies focus, and a tap gesture with either hand generates a touch event on that focused item. The system can also detect when someone\u2019s fingers interact with virtual items in the person\u2019s field of vision. When you adopt the standard UIKit and SwiftUI event-handling mechanisms, you get all of these interactions automatically. For most apps, the system-provided gesture recognizers are sufficient for responding to interactions. Although you can get the position of someone\u2019s hands with ARKit, doing so isn\u2019t necessary for most apps. Collect hand-position data only when the system doesn\u2019t offer what you need. For example, you might use hand-position data to attach 3D content to the person\u2019s hands. Some other things to remember about hand-position data: People can deny your request for access to hand-position data. Be prepared to handle situations where the data isn\u2019t available. People can deny your request for access to hand-position data. Be prepared to handle situations where the data isn\u2019t available. You must present an immersive space to access hand data. When you open an immersive space, the system hides other apps. You must present an immersive space to access hand data. When you open an immersive space, the system hides other apps. For information about how to handle the standard-system events, see the SwiftUI and UIKit documentation. The following ARKit features require you to provide a usage description string in your app\u2019s Info.plist file: World-tracking data World-tracking data Hand-tracking data Hand-tracking data Other privacy-sensitive technologies in visionOS also require you to supply usage description strings. For example, you provide usage descriptions for the Core Location features you adopt. These strings communicate why your app needs the data, and how you plan to use the data to help the person using your app. The first time you request authorization to use the technology, the system prompts the person to grant or deny access to your app. The system includes your usage-description string in the dialog it displays. For information about requesting access to ARKit data, see ARKit. For guidance on how to craft good messages around privacy-friendly features, see Human Interface Guidelines. Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/improving-accessibility-support-in-your-app", "title": "Improving accessibility support in your visionOS app | Apple Developer Documentation", "content": "Tracking specific points in world space Placing content on detected planes Incorporating real-world surroundings in an immersive experience Setting up access to ARKit data Happy Beam ARKit Capturing screenshots and video from Apple Vision Pro for 2D viewing Designing RealityKit content with Reality Composer Pro Understanding RealityKit\u2019s modular architecture Diorama Swift Splash RealityKit and Reality Composer Pro Positioning and sizing windows Presenting windows and spaces Hello World SwiftUI Improving accessibility support in your visionOS app Adopting best practices for privacy and user preferences Designing for visionOS Design Drawing sharp layer-based content in visionOS Creating fully immersive experiences in your app Adding 3D content to your app Creating your first visionOS app App construction   visionOS is an immersive platform that supports people of all abilities. Even though experiences incorporate stunning visual content and hand- and eye-tracking technologies, people can engage with content in other ways. In fact, the platform supports people in many different situations, including those who are blind, have low vision, have limited mobility, or have limb differences. With the help of assistive technologies, people can interact with all of your app\u2019s content. During development, enable VoiceOver and other assistive features and test your app\u2019s accessibility support. Make sure people can navigate your app\u2019s interface intuitively, and that all of the necessary elements are present. Improve the descriptive information for those elements to communicate their intended purpose. And make sure your app adapts to changing conditions, such as changes to the Dynamic Type setting while your app is running. Default font size Increased font size For general information about supporting accessibility, see Accessibility. For design guidance, see Human Interface Guidelines > Accessibility. VoiceOver and other assistive technologies rely on the accessibility information that your app\u2019s views and content provide. SwiftUI and UIKit provide default information for the standard system views, but RealityKit doesn\u2019t provide default information for the entities in your scenes. To configure the accessibility information for a RealityKit entity, add an instance of AccessibilityComponent to the entity. Use this component to specify the same values you specify for the rest of your app\u2019s views. The following example shows how to create this component and add it to an entity: People can use VoiceOver to initiate specific types of actions on your entities. Assign a value to the systemActions property of your component if your entity supports the incrementing or decrementing of its value, or supports activation with a gesture other than a standard tap. You don\u2019t need to set a system action if you let people interact with the entity using a standard single-tap gesture. The following example uses the content of a RealityView to determine when activation events occur on the view\u2019s entities. After subscribing to the view\u2019s activation events, the code sets up an asynchronous task to handle incoming events. When a new event occurs, the task executes the custom code to handle a collision. When VoiceOver is active in visionOS, people use hand gestures to navigate your app\u2019s interface and inspect elements. To prevent your app\u2019s code from interfering with VoiceOver interactions, the system doesn\u2019t deliver hand input to your app during this time. However, a person can perform a special VoiceOver gesture to enable Direct Gesture mode, which leaves VoiceOver enabled but restores hand input to your app. Add VoiceOver announcements to your code to communicate the results of meaningful events. VoiceOver speaks these announcements at all times, but they are particularly useful when Direct Gesture mode is on. The following example posts an announcement when a custom gesture causes an interaction with a game piece: Reduced mobility can affect a person\u2019s ability to interact with your app\u2019s content. When designing your app\u2019s input model, avoid experiences that require specific body movements or positions. For example, if your app supports custom hand gestures, add menu commands for each gesture so someone can enter them using a keyboard or assistive device. Some assistive technologies let people interact with your app using only their eyes. Using these technologies they can select, scroll, long press, or drag items in your interface. Even if you support other types of interactions, give people a way to access all of your app\u2019s behavior using only these interactions. Some assistive technologies allow people to navigate or view your app\u2019s interface using head movements. As the person\u2019s head moves, the assistive technology focuses on the item directly in front of them. Content that follows the movements of the person\u2019s head interferes with the behavior of these assistive technologies. When designing your interface, place content in windows or anchor it to locations other than the virtual camera. If you do need head-anchored content, provide an alternative solution when relevant assistive technologies are in use. For example, you might move head-anchored content to an anchor point that doesn\u2019t follow the person\u2019s head movements. To determine when to change the anchoring approach for your content, check the accessibilityPrefersHeadAnchorAlternative environment variable in SwiftUI, or call the AXPrefersHeadAnchorAlternative() function. This environment variable is true when an assistive technology is in use that conflicts with head-anchored content. Adapt your content to use alternate anchoring mechanisms at that time. Motion effects on any immersive device can be jarring, even for people who aren\u2019t sensitive to motion. Limit the use of motion effects that incorporate rapid movement, bouncing or wave-like movement, zooming animations, multi-axis movement, spinning, or rotations. When the person wearing the device is sensitive to motion effects, eliminate the use of these effects altogether. The Reduce Motion system setting lets you know when to provide alternatives for all of your app\u2019s motion effects. Access this setting using the accessibilityReduceMotion environment variable in SwiftUI or with the isReduceMotionEnabled property in UIKit. When the setting is true, provide suitable alternatives for motion effects or eliminate them altogether. For example, show a static snapshot of the ocean instead of a video. For more information, see Human Interface Guidelines > Motion. For people who are deaf or hard of hearing, provide high-quality captions for your app\u2019s content. Captions are a necessity to some, but are practical for everyone in certain situations. For example, captions are useful to someone watching a video in a noisy environment. Remember to include captions not just for text and dialogue, but also for music and sound effects in your content. For Spatial Audio content, include information in your captions that indicates the direction of various sounds. AVKit and AVFoundation provide built-in support for displaying captioned content. These frameworks configure the font, size, color, and style of the captions automatically according to the person\u2019s accessibility settings. For example, the frameworks adopt the current Dynamic Type setting when displaying text. If you have a custom video engine, check the isClosedCaptioningEnabled accessibility setting to determine when to display captions. To get the correct appearance information for your captioned content, adopt Media Accessibility in your project. This framework provides you with the optimal font, color, and opacity information to apply to captioned text and images.\n\n\n\nvar accessibilityComponent = AccessibilityComponent()\naccessibilityComponent.isAccessibilityElement = true\naccessibilityComponent.traits = [.button]\naccessibilityComponent.label = \"Sports car\"\naccessibilityComponent.value = \"Parked\"\naccessibilityComponent.systemActions = [.activate]\nmyEntity.components[AccessibilityComponent.self] = accessibilityComponent\n\n\n\n\nactivationSubscription = content.subscribe(to: AccessibilityEvents.Activate.self, \n                            on: nil, componentType: nil) { activation in\n    Task {\n        try handleCollision(for: activation.entity, gameModel: gameModel)\n    }\n}\n\n\n\n\nAccessibilityNotification.Announcement(\"Game piece hit\").post()\n", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos#swiftui", "title": "visionOS | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space  / visionOS API Changes: None visionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.  Developing for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app\u2019s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS. Start with a familiar window-based experience to introduce people to your content. From there, add SwiftUI scene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences. Build your app\u2019s 3D content with RealityKit and Reality Composer Pro, and display it with a RealityView. In an immersive experience, use ARKit to integrate your content with the person\u2019s surroundings.  People can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures. SwiftUI provides built-in support for these standard gestures, so rely on them for most of your app\u2019s input. When you want to go beyond the standard gestures, use ARKit to create custom gestures. Tap to select Pinch to rotate Manipulate objects Create custom gestures Explore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash. visionOS Overview Expand your app into immersive spaces Explore new kinds of interaction Dive into featured sample apps Topics Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/world", "title": "Hello World | Apple Developer Documentation", "content": "Tracking specific points in world space Placing content on detected planes Incorporating real-world surroundings in an immersive experience Setting up access to ARKit data Happy Beam ARKit Capturing screenshots and video from Apple Vision Pro for 2D viewing Designing RealityKit content with Reality Composer Pro Understanding RealityKit\u2019s modular architecture Diorama Swift Splash RealityKit and Reality Composer Pro Positioning and sizing windows Presenting windows and spaces Hello World SwiftUI Improving accessibility support in your visionOS app Adopting best practices for privacy and user preferences Designing for visionOS Design Drawing sharp layer-based content in visionOS Creating fully immersive experiences in your app Adding 3D content to your app Creating your first visionOS app App construction   You can use visionOS scene types and styles to share information in fun and compelling ways. Features like volumes and immersive spaces let you put interactive virtual objects into people\u2019s environments, or put people into a virtual environment. Hello World uses these tools to teach people about the Earth \u2014 the planet we call home. The app shows how the Earth\u2019s tilt creates the seasons, how objects move as they orbit the Earth, and how Earth appears from space. The app uses SwiftUI to define its interface, including both 2D and 3D elements. To create, customize, and manage 3D models and effects, it also relies on the RealityKit framework and Reality Composer Pro. Hello World constructs the scene that it displays at launch \u2014 the first scene that appears in the WorldApp structure \u2014 using a WindowGroup: Like other platforms \u2014 for example, macOS and iOS \u2014 visionOS displays a window group as a familiar-looking window. In visionOS, people can resize and move windows around the Shared Space. Even if your app offers a sophisticated 3D experience, a window is a great starting point for an app because it eases people into the experience. It\u2019s also a good place to provide instructions or controls. Tip This particular window group uses the plain window style to maintain control over the glass background effect that visionOS would otherwise automatically add. After you watch a brief introductory animation that shows the text Hello World typing in, the Modules view that defines the primary scene\u2019s content presents options to explore different aspects of the world. This view contains a table of contents at the root of a NavigationStack: A visionOS navigation stack has the same behavior that it has in other platforms. When it first appears, the stack displays its root view. When someone chooses an embedded NavigationLink, the stack draws a new view and displays a back button in the toolbar. When someone taps the back button, the stack restores the previous view.  The trailing closure of the navigationDestination(for:destination:) view modifier in the code above displays a view when someone activates a link based on a module input that comes from the corresponding link\u2019s initializer: The possible module values come from a custom Module enumeration: The globe module opens with a few facts about the Earth in the main window next to a decorative, flat image that supports the content. To help people understand even more, the module includes a button titled View Globe that opens a 3D interactive globe in a new window.  To be able to open multiple scene types, Hello World includes the UIApplicationSceneManifest key in its Information Property List file. The value for this key is a dictionary that includes the UIApplicationSupportsMultipleScenes key with a value of true: With the key in place, the app makes use of a second WindowGroup in its App declaration. This new window group uses the Globe view as its content: This window group creates a volume \u2014 which is a container that has three dimensions and behaves like a transparent box \u2014 because Hello World uses the volumetric window style scene modifier. People can move this box around the Shared Space like they move other window types, and the content remains fixed inside. The defaultSize(width:height:depth:in:) modifier specifies a size for the volume in meters, including a depth dimension. The Globe view inside the volume contains 3D content, but is still just a SwiftUI view. It contains two elements in a ZStack: a subview that draws a model of the Earth, and another that provides a control panel that people can use to configure the model\u2019s appearance. The globe module presents a View Globe button that people can tap to display or dismiss the volume, depending on the current state. Hello World achieves this behavior by creating a Toggle with the button style, and embedding it in a custom GlobeToggle view.  When someone taps the toggle, the isShowingGlobe state changes, and the onChange(of:initial:_:) modifier calls the openWindow or dismissWindow action to open or dismiss the volume, respectively. The view gets these actions from the environment and uses an identifier that matches the volume\u2019s identifier. You use windows in visionOS the same way you do in other platforms. But even 2D windows in visionOS provide a small amount of depth you can use to create 3D effects \u2014 like elements that appear in front of other elements. Hello World takes advantage of this depth to present small models inline with 2D content. The app\u2019s second module, Objects in Orbit, provides information about objects that go around the Earth, like the Moon and artificial satellites. To give a sense of what these objects look like, the module displays 3D models of these items directly inside the window.  Hello World loads these models from the asset bundle using a Model3D structure inside a custom ItemView. The view scales and positions the model to fit the available space, and applies optional orientation adjustments: The app uses this ItemView once for each model, placing each in an overlay that only becomes visible based on the current selection. For example, the following overlay displays the satellite model with a small amount of tilt in the x-axis and z-axis: The VStack that contains the models also contains a Picker that people use to select a model to view: When you add 3D effects to a 2D window, keep this guidance in mind: Don\u2019t overdo it. These kinds of effects add interest, but can unintentionally obscure important controls or information as people view the window from different directions. Ensure that elements don\u2019t exceed the available depth. Excess depth causes elements to clip. Account for any position or orientation changes that might occur after initial placement. Avoid models intersecting with the backing glass. Again, account for potential movement after initial placement. People can visualize how satellites move around the Earth because the app\u2019s orbit module displays the Earth, the Moon, and a communications satellite together as a single system. People can move the system anywhere in their environment or resize it using standard gestures. They can also move themselves around the system to get different perspectives.  Note To learn about designing with gestures in visionOS, read Gestures in Human Interface Guidelines. To create this visualization, the app displays the Orbit view \u2014 which contains a single RealityView that models the entire system \u2014 in an ImmersiveSpace scene with the mixed immersion style: As with any secondary scene in a visionOS app, this scene depends on having the UIApplicationSupportsMultipleScenes key in the Information Property List file. The app also opens and closes the space using a toggle view that resembles the one used for the globe: There are a few key differences from the version that appears in the section Open and dismiss the globe volume: OrbitToggle uses openImmersiveSpace and dismissImmersiveSpace from the environment, rather than the window equivalents. The dismiss action in this case doesn\u2019t require an identifier, because people can only open one space at a time, even across apps. The open and dismiss actions for spaces operate asynchronously, and so they appear inside a Task. The app\u2019s final module gives people a sense of the Earth\u2019s place in the solar system. Like other modules, this one includes information and a decorative image next to a button that leads to another visualization \u2014 in this case so people can experience Earth from space. When a person taps the button, the app takes over the entire display and shows stars in all directions. The Earth appears directly in front, the Moon to the right, and the Sun to the left. The main window also shows a small control panel that people can use to exit the fully immersive experience.  Tip People can always close the currently open immersive space by pressing the device\u2019s Digital Crown, but it\u2019s typically useful when you provide a built-in mechanism to maintain control of the experience within your app. The app uses another immersive space scene for this module, but here with the full immersion style that turns off the passthrough video: This scene depends on the same UIApplicationSupportsMultipleScenes key that other secondary scenes do, and is activated by a SolarSystemToggle that\u2019s similar to the ones that the app uses for the other scenes: This control appears in the main window to provide a way to begin the fully immersive experience, and separately in the control panel as a way to exit the experience. Because the app uses this control as two distinct buttons rather than as a toggle in one location, it\u2019s composed of a Button with behavior that changes depending on the app state rather than as a toggle with a button style. To reuse the main window for the solar system controls, Hello World places both the navigation stack and the controls in a ZStack, and then sets the opacity of each to ensure that only one appears at a time:\n\n\n\nWindowGroup(\"Hello World\", id: \"modules\") {\n    Modules()\n        .environment(model)\n}\n.windowStyle(.plain)\n\n\n\n\nNavigationStack(path: $model.navigationPath) {\n    TableOfContents()\n        .navigationDestination(for: Module.self) { module in\n            ModuleDetail(module: module)\n                .navigationTitle(module.eyebrow)\n        }\n}\n\n\n\n\nNavigationLink(value: module) { /* The link's label. */ }\n\n\n\n\nenum Module: String, Identifiable, CaseIterable, Equatable {\n    case globe, orbit, solar\n    // ...\n}\n\n\n\n\n<key>UIApplicationSceneManifest</key>\n<dict>\n    <key>UIApplicationSupportsMultipleScenes</key>\n    <true/>\n    <key>UISceneConfigurations</key>\n    <dict/>\n</dict>\n\n\n\n\nWindowGroup(id: Module.globe.name) {\n    Globe()\n        .environment(model)\n}\n.windowStyle(.volumetric)\n.defaultSize(width: 0.6, height: 0.6, depth: 0.6, in: .meters)\n\n\n\n\nstruct GlobeToggle: View {\n    @Environment(ViewModel.self) private var model\n    @Environment(\\.openWindow) private var openWindow\n    @Environment(\\.dismissWindow) private var dismissWindow\n\n\n    var body: some View {\n        @Bindable var model = model\n\n\n        Toggle(Module.globe.callToAction, isOn: $model.isShowingGlobe)\n            .onChange(of: model.isShowingGlobe) { _, isShowing in\n                if isShowing {\n                    openWindow(id: Module.globe.name)\n                } else {\n                    dismissWindow(id: Module.globe.name)\n                }\n            }\n            .toggleStyle(.button)\n    }\n}\n\n\n\n\nprivate struct ItemView: View {\n    var item: Item\n    var orientation: SIMD3<Double> = .zero\n\n\n    var body: some View {\n        Model3D(named: item.name, bundle: worldAssetsBundle) { model in\n            model.resizable()\n                .scaledToFit()\n                .rotation3DEffect(\n                    Rotation3D(\n                        eulerAngles: .init(angles: orientation, order: .xyz)\n                    )\n                )\n                .frame(depth: modelDepth)\n                .offset(z: -modelDepth / 2)\n        } placeholder: {\n            ProgressView()\n                .offset(z: -modelDepth * 0.75)\n        }\n    }\n}\n\n\n\n\n.overlay {\n    ItemView(item: .satellite, orientation: [0.15, 0, 0.15])\n        .opacity(selection == .satellite ? 1 : 0)\n}\n\n\n\n\nPicker(\"Satellite\", selection: $selection) {\n    ForEach(Item.allCases) { item in\n        Text(item.name)\n    }\n}\n.pickerStyle(.segmented)\n\n\n\n\nImmersiveSpace(id: Module.orbit.name) {\n    Orbit()\n        .environment(model)\n}\n.immersionStyle(selection: $orbitImmersionStyle, in: .mixed)\n\n\n\n\nstruct OrbitToggle: View {\n    @Environment(ViewModel.self) private var model\n    @Environment(\\.openImmersiveSpace) private var openImmersiveSpace\n    @Environment(\\.dismissImmersiveSpace) private var dismissImmersiveSpace\n\n\n    var body: some View {\n        @Bindable var model = model\n\n\n        Toggle(Module.orbit.callToAction, isOn: $model.isShowingOrbit)\n            .onChange(of: model.isShowingOrbit) { _, isShowing in\n                Task {\n                    if isShowing {\n                        await openImmersiveSpace(id: Module.orbit.name)\n                    } else {\n                        await dismissImmersiveSpace()\n                    }\n                }\n            }\n            .toggleStyle(.button)\n    }\n}\n\n\n\n\nImmersiveSpace(id: Module.solar.name) {\n    SolarSystem()\n        .environment(model)\n}\n.immersionStyle(selection: $solarImmersionStyle, in: .full)\n\n\n\n\nstruct SolarSystemToggle: View {\n    @Environment(ViewModel.self) private var model\n    @Environment(\\.openImmersiveSpace) private var openImmersiveSpace\n    @Environment(\\.dismissImmersiveSpace) private var dismissImmersiveSpace\n\n\n    var body: some View {\n        Button {\n            Task {\n                if model.isShowingSolar {\n                    await dismissImmersiveSpace()\n                } else {\n                    await openImmersiveSpace(id: Module.solar.name)\n                }\n            }\n        } label: {\n            if model.isShowingSolar {\n                Label(\n                    \"Exit the Solar System\",\n                    systemImage: \"arrow.down.right.and.arrow.up.left\")\n            } else {\n                Text(Module.solar.callToAction)\n            }\n        }\n    }\n}\n\n\n\n\nZStack {\n    SolarSystemControls()\n        .opacity(model.isShowingSolar ? 1 : 0)\n\n\n    NavigationStack(path: $model.navigationPath) {\n        // ...\n    }\n    .opacity(model.isShowingSolar ? 0 : 1)\n}\n.animation(.default, value: model.isShowingSolar)\n", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/presenting-windows-and-spaces", "title": "Presenting windows and spaces | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode Tracking specific points in world space Placing content on detected planes Incorporating real-world surroundings in an immersive experience Setting up access to ARKit data Happy Beam ARKit Capturing screenshots and video from Apple Vision Pro for 2D viewing Designing RealityKit content with Reality Composer Pro Understanding RealityKit\u2019s modular architecture Diorama Swift Splash RealityKit and Reality Composer Pro Positioning and sizing windows Presenting windows and spaces Hello World SwiftUI Improving accessibility support in your visionOS app Adopting best practices for privacy and user preferences Designing for visionOS Design Drawing sharp layer-based content in visionOS Creating fully immersive experiences in your app Adding 3D content to your app Creating your first visionOS app App construction  / visionOS / Presenting windows and spaces API Changes: None An app\u2019s scenes, which contain views that people interact with, can take different forms. For example, a scene can fill a window, a tab in a window, or an entire screen. Some scenes can even place views throughout a person\u2019s surroundings. How a scene appears depends on its type, the platform, and the context. When someone launches your app, SwiftUI looks for the first WindowGroup, Window, or DocumentGroup in your app declaration and opens a scene of that type, typically filling a new window or the entire screen, depending on the platform. For example, the following app running in macOS presents a window that contains a MailViewer view: In visionOS, you can alternatively configure your app to open the first ImmersiveSpace that the app declares. In any case, specific platforms and configurations enable you to open more than one scene at a time. Under those conditions, you can use actions that appear in the environment to programmatically open and close the scenes in your app. If you share code among different platforms and need to find out at runtime whether the current system supports displaying multiple scenes, read the supportsMultipleWindows environment value. The following code creates a button that\u2019s hidden unless the app supports multiple windows: The value that you read depends on both the platform and how you configure your app: In macOS, this property returns true for any app that uses the SwiftUI app lifecycle. In macOS, this property returns true for any app that uses the SwiftUI app lifecycle. In iPadOS and visionOS, this property returns true for any app that uses the SwiftUI app lifecycle and has the Information Property List key UIApplicationSupportsMultipleScenes set to true, and false otherwise. In iPadOS and visionOS, this property returns true for any app that uses the SwiftUI app lifecycle and has the Information Property List key UIApplicationSupportsMultipleScenes set to true, and false otherwise. For all other platforms and configurations, the value returns false. For all other platforms and configurations, the value returns false. If your app only ever runs in one of these situations, you can assume the associated behavior and don\u2019t need to check the value. You can always present multiple scenes in macOS. To enable an iPadOS or visionOS app to simultaneously display multiple scenes \u2014 including ImmersiveSpace scenes in visionOS \u2014 add the UIApplicationSupportsMultipleScenes key with a value of true in the UIApplicationSceneManifest dictionary of your app\u2019s Information Property List. Use the Info tab in Xcode for your app\u2019s target to add this key:  Apps on other platforms can display only one scene during their lifetime. Some platforms provide built-in controls that enable people to open instances of the window-style scenes that your app defines. For example, in macOS people can choose File > New Window from the menu bar to open a new window. SwiftUI also provides ways for you to open new windows programmatically. To do this, get the openWindow action from the environment and call it with an identifier, a value, or both to indicate what kind of window to open and optionally what data to open it with. The following view opens a new instance of the previously defined mail viewer window when someone clicks or taps the button: When the action runs on a system that supports multiple scenes, SwiftUI looks for a window in the app declaration that has a matching identifier and creates a new scene of that type. Important If supportsMultipleWindows is false and you try to open a new window, SwiftUI ignores the action and logs a runtime error. In addition to opening more instances of an app\u2019s main window, as in the above example, you can also open other window types that your app\u2019s body declares. For example, you can open an instance of the Window that displays connectivity information: In visionOS, you open an immersive space \u2014 a scene that you can use to present unbounded content in a person\u2019s surroundings \u2014 in much the same way that you open a window, except that you use the openImmersiveSpace action. The action runs asynchronously, so you use the await keyword when you call it, and typically do so from inside a Task: Because your app operates in a Full Space when you open an ImmersiveSpace scene, you can only open one scene of this type at a time. If you try to open a space when one is already open, the system logs a runtime error. Your app can display any number of windows together with an immersive space. However, when you open a space from your app, the system hides all windows that belong to other apps. After you dismiss your space, the other apps\u2019 windows reappear. Similarly, the system hides your app\u2019s windows if another app opens an immersive space. When visionOS launches an app, it opens the first window group, window, or document scene that the app\u2019s body declares, just like on other platforms. This is true even if you first declare a space. However, if you want to open your app into an immersive space directly, specify a space as the default scene for your app by adding the UIApplicationPreferredDefaultSceneSessionRole key to your app\u2019s information property list and setting its value to UISceneSessionRoleImmersiveSpaceApplication. In that case, visionOS opens the first space that it finds in your app declaration. Important Be careful not to overwhelm people when starting your app with an immersive space. For design guidance, see Immersive experiences. People can close windows using system controls, like the close button built into the frame around a macOS window. You can also close windows programmatically. Get the dismissWindow action from the environment, and call it using the identifier of the window that you want to dismiss: In iPadOS and visionOS, the system ignores the dismiss action if you use it to close a window that\u2019s your app\u2019s only open scene. To close a space, call the dismissImmersiveSpace action. Like the corresponding open space action, the close action operates asynchronously and requires the await keyword: You don\u2019t need to specify an identifier for this action, because there can only ever be one space open at a time. Like with windows, you can\u2019t dismiss a space that\u2019s your app\u2019s only open scene. Because you can\u2019t programmatically close the last open window or immersive space in a visionOS app, be sure to open a new scene before closing the old one. Pay particular attention to the sequencing when moving between a window and an immersive space, because the space\u2019s open and dismiss actions run asynchronously. For example, consider a chess game that begins by displaying a start button in a window. When someone taps the button, the app dismisses the window and opens an immersive space that presents a chess board. The following button demonstrates proper sequencing by opening the space and then closing the window: In the above code, it\u2019s important to include the dismissWindow action inside the task, so that it waits until the openImmersiveSpace action completes. If you put the action outside the task \u2014 either before or after \u2014 it might execute before the asynchronous open action completes, when the window is still the only open scene. In that case, the system opens the space but doesn\u2019t close the window. Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC\n\n\n\n@main\nstruct MailReader: App {\n    var body: some Scene {\n        WindowGroup(id: \"mail-viewer\") {\n            MailViewer()\n        }\n\n\n        Window(\"Connection Status\", id: \"connection\") {\n            ConnectionStatus()\n        }\n    }\n}\n\n\n\nstruct NewWindowButton: View {\n    @Environment(\\.supportsMultipleWindows) private var supportsMultipleWindows\n    @Environment(\\.openWindow) private var openWindow\n\n\n    var body: some View {\n        Button(\"Open New Window\") {\n            openWindow(id: \"mail-viewer\")\n        }\n        .opacity(supportsMultipleWindows ? 1 : 0)\n    }\n}\n\n\n\nstruct NewViewerButton: View {\n    @Environment(\\.openWindow) private var openWindow\n\n\n    var body: some View {\n        Button(\"New Mail Viewer\") {\n            openWindow(id: \"mail-viewer\")\n        }\n    }\n}\n\n\n\nButton(\"Connection Status\") {\n    openWindow(id: \"connection\")\n}\n\n\n\nstruct NewSpaceButton: View {\n    @Environment(\\.openImmersiveSpace) private var openImmersiveSpace\n\n\n    var body: some View {\n        Button(\"View Orbits\") {\n            Task {\n                await openImmersiveSpace(id: \"orbits\")\n            }\n        }\n    }\n}\n\n\n\nprivate struct ContentView: View {\n    @Environment(\\.dismissWindow) private var dismissWindow\n\n\n    var body: some View {\n        Button(\"Done\") {\n            dismissWindow(id: \"connection\")\n        }\n    }\n}\n\n\n\nprivate struct ContentView: View {\n    @Environment(\\.dismissImmersiveSpace) private var dismissImmersiveSpace\n\n\n    var body: some View {\n        Button(\"Done\") {\n            Task {\n                await dismissImmersiveSpace()\n            }\n        }\n    }\n}\n\n\n\nButton(\"Start\") {\n    Task {\n        await openImmersiveSpace(id: \"chessboard\")\n        dismissWindow(id: \"start\") // Runs after the space opens.\n    }\n}", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/positioning-and-sizing-windows", "title": "Positioning and sizing windows | Apple Developer Documentation", "content": "App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space   visionOS and macOS enable people to move and resize windows. In some cases, your app can use scene modifiers to influence a window\u2019s initial geometry on these platforms, as well as to specify the strategy that the system employs to place minimum and maximum size limitations on a window. This kind of configuration affects both windows and volumes, which are windows with the volumetric window style. Your ability to configure window size and position is subject to the following constraints: The system might be unable to fulfill your request. For example, if you specify a default size that\u2019s outside the range of the window\u2019s resizability, the system clamps the affected dimension to keep it in range. Although you can change the window\u2019s content, you can\u2019t directly manipulate window position or size after the window appears. This ensures that people have full control over their workspace. During state restoration, the system restores windows to their previous position and size. Note Windows in iPadOS occupy the full screen, or share the screen with another window in Slide Over or Split View. You can\u2019t programmatically affect window geometry on that platform. In macOS, the first time your app opens a window from a particular scene declaration, the system places the window at the center of the screen by default. For scene types that support multiple simultaneous windows, the system offsets each additional window by a small amount to avoid fully obscuring existing windows. You can override the default placement of the first window in macOS by applying the defaultPosition(_:) scene modifier to indicate where to place the window relative to the screen bounds. For example, you can request that the system place a new window in the bottom trailing corner of the screen: The system aligns the point in the window that corresponds to the specified UnitPoint with the point in the screen that corresponds to the same unit point. You can use a built-in unit point, like bottomTrailing in the above example, or define a custom one. Important You can\u2019t use defaultPosition(_:) in visionOS. The system always places new windows directly in front of people, where they happen to be looking at the moment the window opens. This helps to make people aware of new windows. You can indicate a default initial size for a new window that the system creates from a Scene declaration by applying one of the default size scene modifiers, like defaultSize(width:height:). For example, you can request that new windows that a WindowGroup generates occupy 600 points in the x-dimension and 400 points in the y-dimension: The system might clamp the actual size of the window depending on both the window\u2019s content and resizability settings. Both macOS and visionOS provide interface controls that enable people to resize windows, within certain limits. For example, people can use the control that appears when they look at the corner of a visionOS window to resize a window on that platform. You can specify how the system limits window resizability. The default resizability for all scenes is automatic. With that strategy, Settings windows use the contentSize strategy, where both the minimum and maximum window size match the respective minimum and maximum sizes of the content that the window contains. Other scene types use contentMinSize by default, which retains the minimum size restriction, but doesn\u2019t limit the maximium size. You can specify one of these resizability strategies explicitly by adding the windowResizability(_:) scene modifier to a scene. For example, people can resize windows from the following window group to between 100 and 400 points in both dimensions because the frame modifier imposes those bounds on the content view: You can take this even further and enforce a specific size for a window with content that has a fixed size. When you create a volume, which is a window with the volumetric style, you can specify the volume\u2019s size using one of the three-dimensional default size modifiers, like defaultSize(width:height:depth:in:). The following code creates a volume that\u2019s one meter on a side: The volume maintains this size for its entire lifetime. People can\u2019t change the size of a volume at runtime. Although you can specify a volume\u2019s size in points, it\u2019s typically better to use physical units, like the above code which specifies a size in meters. This is because the system renders a volume with fixed scaling rather than dynamic scaling, unlike a regular window, which means the volume appears more like a physical object than a user interface. For information about the different kinds of scaling, see Spatial layout.\n\n\n\n@main\nstruct MyApp: App {\n    var body: some Scene {\n        WindowGroup {\n            ContentView()\n        }\n        .defaultPosition(.bottomTrailing)\n    }\n}\n\n\n\n\n@main\nstruct MyApp: App {\n    var body: some Scene {\n        WindowGroup {\n            ContentView()\n        }\n        .defaultSize(CGSize(width: 600, height: 400))\n    }\n}\n\n\n\n\n@main\nstruct MyApp: App {\n    var body: some Scene {\n        WindowGroup {\n            ContentView()\n                .frame(\n                    minWidth: 100, maxWidth: 400,\n                    minHeight: 100, maxHeight: 400)\n        }\n        .windowResizability(.contentSize)\n    }\n}\n\n\n\n\nWindowGroup(id: \"globe\") {\n    Globe()\n}\n.windowStyle(.volumetric)\n.defaultSize(width: 1, height: 1, depth: 1, in: .meters)\n", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos#realitykit-and-reality-composer-pro", "title": "visionOS | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space  / visionOS API Changes: None visionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.  Developing for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app\u2019s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS. Start with a familiar window-based experience to introduce people to your content. From there, add SwiftUI scene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences. Build your app\u2019s 3D content with RealityKit and Reality Composer Pro, and display it with a RealityView. In an immersive experience, use ARKit to integrate your content with the person\u2019s surroundings.  People can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures. SwiftUI provides built-in support for these standard gestures, so rely on them for most of your app\u2019s input. When you want to go beyond the standard gestures, use ARKit to create custom gestures. Tap to select Pinch to rotate Manipulate objects Create custom gestures Explore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash. visionOS Overview Expand your app into immersive spaces Explore new kinds of interaction Dive into featured sample apps Topics Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/swift-splash", "title": "Swift Splash | Apple Developer Documentation", "content": "Tracking specific points in world space Placing content on detected planes Incorporating real-world surroundings in an immersive experience Setting up access to ARKit data Happy Beam ARKit Capturing screenshots and video from Apple Vision Pro for 2D viewing Designing RealityKit content with Reality Composer Pro Understanding RealityKit\u2019s modular architecture Diorama Swift Splash RealityKit and Reality Composer Pro Positioning and sizing windows Presenting windows and spaces Hello World SwiftUI Improving accessibility support in your visionOS app Adopting best practices for privacy and user preferences Designing for visionOS Design Drawing sharp layer-based content in visionOS Creating fully immersive experiences in your app Adding 3D content to your app Creating your first visionOS app App construction   Apple Vision Pro\u2019s ability to combine virtual content seamlessly with the real world allows for many kinds of interactive virtual experiences. Swift Splash leverages RealityKit and Reality Composer Pro to create a virtual water slide by combining modular slide pieces. When the builder finishes their ride, they can release an adventurous goldfish to try it out. Swift Splash uses multiple Reality Composer Scenes to create prepackaged entity hierarchies that represent each of the slide pieces the player connects to construct their ride. It demonstrates how to hide and reveal sections of the entity hierarchy based on the current state of the app. For example, each slide piece contains an animated fish entity that\u2019s hidden until the ride runs and the fish arrives at that particular piece. While Swift Splash is a fun, game-like experience, the core idea of assembling virtual objects out of predefined parts can also be used as the basis for a productivity or creation app. Swift Splash scenes include Shader Graph materials built in Reality Composer Pro to change the appearance of the ride at runtime. Each piece can be configured to display in one of three materials: metal, wood, or plastic. Other Shader Graph materials create special effects, such as the movement of the water and the flashing lights on the start and end pieces. Even particle effects are included in some of these prepackaged entities, such as the fireworks that play when the goldfish crosses the finish line. Slide pieces are the building blocks of Swift Splash. The Reality Composer project contains a separate scene for each one. In addition to the 3D models that make up the slide piece, each scene contains a number of other entities the app uses to animate and place the slide piece.  In the hierarchy viewer on the left side of the screenshot above, there are two transform entities called connect_in and connect_out. These transforms mark the points where the slide piece connects to the next or previous piece. Swift Splash uses these transforms to place new pieces at the end of the existing slide, as well as to snap pieces to other slide pieces when you manually move them near each other. Slide pieces demonstrate the two primary mechanisms Swift Splash uses to find entities at runtime. For some entities, such as connect_in, Swift Splash uses a naming convention and retrieves the entities by name or suffix when it needs to use them. In other cases, such as when names aren\u2019t unique or the retrieving code needs configuration values, Swift Splash uses a custom component to mark and retrieve entities. For example, animated entities that appear when the ride runs contain a component called RideAnimationComponent. The app uses this component to determine if the entity is an animation that plays while the ride is running. The component also stores additional state the app needs to implement the ride animation, such as a property called duration that specifies when to start the animations on the next connected slide piece. RideAnimationComponent also includes a property called isPersistent. Persistent ride animations stay visible at all times but only animate when the ride is running, such as the animated door on the start piece. Nonpersistent ride animations, such as the fish swimming through a slide piece, display only while the ride is running and the fish swims through that particular piece. Many of Swift Splash\u2019s slide pieces use the same materials. For example, the shader graph material that changes pieces from metal to wood to plastic is shared by all but one of the slide pieces. To avoid having duplicate copies of each material, Swift Splash leverages USD material references to share materials between multiple entities in multiple scenes. The Reality Composer Pro project contains a separate scene for each shared material, containing only that one material. Other track pieces create references to that material. If you change the original material, it affects all of the entities that reference it. For example, a scene called M_RainbowLights.usda contains the material M_RainbowLights, and both StartPiece.usda and EndPiece.usda reference that material.  To maximize load speed and make the most efficient use of available compute resources, Swift Splash parallelizes loading scenes from the Reality Composer project using a TaskGroup. The app creates a separate Task for each of the scenes it needs to load. The app then uses an async iterator to wait for and receive the results. For more information on task groups, see Concurrency in The Swift Programming Language. Each of these loaded pieces acts as a template. When the player adds a new piece of that type, the app clones the piece loaded from Reality Composer Pro and adds the clone to the scene. When multiple entities have more than one overlapping, nonopaque material, RealityKit\u2019s default depth-sorting can cause it to draw those entities in the wrong order. As a result, some entities may not be visible from certain angles or in certain positions relative to other transparent entities. The default depth sorting is based on the center of the entity\u2019s bounding box, which may result in the incorrect drawing order when there are multiple overlapping materials with any amount of transparency. You can see an example of this by looking at the start piece in Reality Composer Pro, or by watching the video below. The following video demonstrates the problem. If the three boxes are the bounding boxes for three different transparent entities, and the small spheres are the box centers, the sphere that\u2019s closest to the camera changes as the camera moves around the boxes, which changes the order that RealityKit\u2019s default depth sorting algorithm draws them. Swift Splash assigns a ModelSortGroupComponent to each of the transparent entities to manually specify the relative depth sorting. To fix the transparency issues in the start piece in the video above, Swift Splash instructs RealityKit to draw the opaque parts of the fish first, its transparent goggles second, the water third, the glass globe fourth, and the selection glow shell last. Swift Splash does this by assigning a ModelSortGroupComponent to each of the overlapping entities using the same ModelSortGroup, but with a different order specified. The root entity for all of the individual slide pieces has a ConnectableComponent. This custom component marks the entity as one that can be connected or snapped to other connectable entities. At runtime, the app adds a ConnectableStateComponent to each slide piece it adds. The component stores state information for the track piece that doesn\u2019t need to be edited in Reality Composer Pro. Among the state information that this component stores is a reference to the next and previous piece. To iterate through the entire ride, ignoring any disconnected pieces, the app gets a reference to the start piece and then iterates until nextPiece is nil. This iteration, similar to iterating a linked list, repeats many times throughout the app. One example is the function that calculates the duration of the built ride by iterating through the individual pieces and adding up the duration of their animations. To build and edit the ride, players interact with Swift Splash in two different ways. They interact with SwiftUI windows to perform certain tasks, such as adding a new piece or deleting an existing piece of the ride. They also manipulate slide pieces using standard visionOS gestures, including taps, double taps, drags, and rotates. The player taps on a piece to select or deselect it. When a player double taps a piece, they select that piece without deselecting any other selected pieces. When someone drags a piece, it moves around the immsersive space, snapping together with other pieces if placed near one. A two-finger rotate gesture spins the selected track piece or pieces on the Z-axis. Swift Splash handles all of these interactions using standard SwiftUI gestures targeted to an entity. To support any of these gestures at any time, the app declares them using SimultaneousGesture. The code for all of the gestures are contained in TrackBuildingView, which controls the app\u2019s immersive space. Here\u2019s how the app defines the rotation gesture: Because multiple tap gestures on the same RealityView execute with a different number of taps, multiple gestures may be called at once. If a player double taps an entity, for example, both the single tap and the double tap gesture code get called, and the app has to determine which one to execute. Swift Splash makes this determination by using a Boolean state variable. If a player single taps, it sets that variable\u00a0\u2014 called shouldSingleTap \u2014 to true. Then it waits for a period of time before executing the rest of its code. If shouldSingleTap gets set to false while it\u2019s waiting, the code doesn\u2019t execute. When SwiftSplash detects a double tap gesture, it sets shouldSingleTap to false, preventing the single-tap code from firing when it executes the double-tap code.\n\n\n\nawait withTaskGroup(of: LoadResult.self) { taskGroup in   \n    // Load the regular slide pieces and ride animations.\n    logger.info(\"Loading slide pieces.\")\n    for piece in pieces {\n        taskGroup.addTask {\n            do {\n                guard let pieceEntity = try await self.loadFromRCPro(named: piece.key.rawValue, \n                                                       fromSceneNamed: piece.sceneName) else {\n                    fatalError(\"Attempted to load piece entity \\(piece.name) but failed.\")\n                }\n                return LoadResult(entity: pieceEntity, key: piece.key.rawValue)\n            } catch {\n                fatalError(\"Attempted to load \\(piece.name) but failed: \\(error.localizedDescription)\")\n            }\n        }\n    }\n    // Continue adding asset load jobs.\n    // ...\n}\n\n\n\n\nfor await result in taskGroup {\n    if let pieceKey = pieces.filter({ piece in\n        piece.key.rawValue == result.key\n    }).first {\n        self.add(template: result.entity, for: pieceKey.key)\n        setupConnectible(entity: result.entity)\n        result.entity.generateCollisionShapes(recursive: true)\n        result.entity.setUpAnimationVisibility()\n    }\n    // ...\n}\n\n\n\n\nfileprivate func setEntityDrawOrder(_ entity: Entity, _ sortOrder: Int32, _ sortGroup: ModelSortGroup) {\n    entity.forEachDescendant(withComponent: ModelComponent.self) { modelEntity, model in\n        logger.info(\"Setting sort order of \\(sortOrder) of \\(entity.name), child entity: \\(modelEntity.name)\")\n        let component = ModelSortGroupComponent(group: sortGroup, order: sortOrder)\n        modelEntity.components.set(component)\n    }\n}\n\n\n/// Manually specify sort ordering for the transparent start piece meshes.\nfunc handleStartPieceTransparency(_ startPiece: Entity) {\n    let group = ModelSortGroup()\n    \n    // Opaque fish parts.\n    if let entity = startPiece.findEntity(named: fishIdleAnimModelName) {\n        setEntityDrawOrder(entity, 1, group)\n    }\n    if let entity = startPiece.findEntity(named: fishRideAnimModelName) {\n        setEntityDrawOrder(entity, 2, group)\n    }\n    \n    // Transparent fish parts.\n    if let entity = startPiece.findEntity(named: fishGlassIdleAnimModelName) {\n        setEntityDrawOrder(entity, 3, group)\n    }\n    if let entity = startPiece.findEntity(named: fishGlassRideAnimModelName) {\n        setEntityDrawOrder(entity, 4, group)\n    }\n    \n    // Water.\n    if let entity = startPiece.findEntity(named: sortOrderWaterName) {\n        setEntityDrawOrder(entity, 5, group)\n    }\n    \n    // Glass globe.\n    if let entity = startPiece.findEntity(named: sortOrderGlassGlobeName) {\n        setEntityDrawOrder(entity, 6, group)\n    }\n    \n    // Selection glow.\n    if let entity = startPiece.findEntity(named: startGlowName) {\n        setEntityDrawOrder(entity, 7, group)\n    }\n}\n\n\n\n\n/// Calculates the duration of the built ride by summing up the individual durations.\npublic func calculateRideDuration() {\n    guard let startPiece = startPiece else { fatalError(\"No start piece found.\") }\n    var nextPiece: Entity? = startPiece\n    var duration: TimeInterval = 0\n    while nextPiece != nil {\n        // Some pieces have more than one ride animation. Use the longest one to calculate duration.\n        var longestAnimation: TimeInterval = 0\n        nextPiece?.forEachDescendant(withComponent: RideAnimationComponent.self) { entity, component in\n            longestAnimation = max(component.duration, longestAnimation)\n        }\n        duration += longestAnimation\n        nextPiece = nextPiece?.connectableStateComponent?.nextPiece\n    }\n    // Remove the part of the animation after the goal post.\n    rideDuration = duration  / animationSpeedMultiplier + 1.0\n}\n\n\n\n\n.simultaneousGesture(\n    RotateGesture()\n        .targetedToAnyEntity()\n        .onChanged({ value in\n            guard appState.phase == .buildingTrack || appState.phase == .placingStartPiece || appState.phase == .draggingStartPiece else { return }\n            handleRotationChanged(value)\n        })\n        .onEnded({ value in\n            guard appState.phase == .buildingTrack || appState.phase == .placingStartPiece || appState.phase == .draggingStartPiece else { return }\n            handleRotationChanged(value, isEnded: true)\n        })\n)\n\n\n\n\n.simultaneousGesture(\n    TapGesture()\n        .targetedToAnyEntity()\n        .onEnded({ value in\n            guard appState.phase == .buildingTrack else { return }\n            Task {\n                shouldSingleTap = true\n                try? await Task.sleep(for: .seconds(doubleTapTolerance))\n                if shouldSingleTap {\n", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/diorama", "title": "Diorama | Apple Developer Documentation", "content": "Tracking specific points in world space Placing content on detected planes Incorporating real-world surroundings in an immersive experience Setting up access to ARKit data Happy Beam ARKit Capturing screenshots and video from Apple Vision Pro for 2D viewing Designing RealityKit content with Reality Composer Pro Understanding RealityKit\u2019s modular architecture Diorama Swift Splash RealityKit and Reality Composer Pro Positioning and sizing windows Presenting windows and spaces Hello World SwiftUI Improving accessibility support in your visionOS app Adopting best practices for privacy and user preferences Designing for visionOS Design Drawing sharp layer-based content in visionOS Creating fully immersive experiences in your app Adding 3D content to your app Creating your first visionOS app App construction   Use Reality Composer Pro to compose, edit, and preview RealityKit content for your visionOS app. In your Reality Composer Pro project, you can create one or more scenes, each of which contains a hierarchy of virtual objects called entities that your app can efficiently load and display. In addition to helping you compose entity hierarchies, Reality Composer Pro also gives you the ability to add and configure components \u2014 even custom components that you\u2019ve written \u2014 to the entities in your scenes. You can also design the visual appearance of entities using Shader Graph, a node-based visual tool for creating RealityKit materials. Shader Graph gives you a tremendous amount of control over the surface details and shape of entities. You can even create animated materials and dynamic materials that change based on the state of your app or user input. Diorama demonstrates many of RealityKit and Reality Composer Pro\u2019s features. It displays an interactive, virtual topographical trail map, much like the real-world dioramas you find at trailheads and ranger stations in national parks. This virtual map has points of interest you can tap to bring up more detailed information. You can also smoothly transition between two trail maps: Yosemite and Catalina Island. Your Reality Composer Pro project must contain assets, which you use to compose scenes for your app. Diorama\u2019s project has several assets, including 3D models like the diorama table, trail map, some birds and clouds that fly over the map, and a number of sounds and images. Reality Composer Pro provides a library of 3D models you can use. Access the library by clicking the Add (+) button on the right side of the toolbar. Selecting objects from the library imports them into your project.  Diorama uses custom assets instead of the available library assets. To use custom assets in your own Reality Composer Pro scenes, import them into your project in one of three ways: by dragging them to Reality Composer Pro\u2019s project browser, using File > Import from the File menu, or copying the assets into the .rkassets bundle inside your project\u2019s Swift package.  Note Although you can still load USDZ files and other assets directly in visionOS, RealityKit compiles assets in your Reality Composer Pro project into a binary format that loads considerably faster than loading from individual files. A single Reality Composer Pro project can have multiple scenes. A scene is an entity hierarchy stored in the project as a .usda file that you can load and display in a RealityView. You can use Reality Composer\u2019s scenes to build an entire RealityKit scene, or to store reusable entity hierarchies that you can use as building block for composing scenes at runtime \u2014 the approach Diorama uses. You can add as many different scenes to your project as you need by selecting File > New > Scene, or pressing \u2318N. At the top of the Reality Composer Pro window, there\u2019s a separate tab for every scene that\u2019s currently open. To open a scene, double-click the scene\u2019s .usda file in the project browser. To edit a scene, select its tab, and make changes using the hierarchy viewer, the 3D view, and the inspector.  RealityKit can only include entities in a scene, but it can\u2019t use every type of asset that Reality Composer Pro supports as an entity. Reality Composer Pro automatically turns some assets, like 3D models, into an entity when you place them in a scene. It uses other assets indirectly. It uses image files, for example, primarily to define the surface details of model entities. Diorama uses multiple scenes to group assets together and then, at runtime, combines those scenes into a single immersive experience. For example, the diorama table has its own scene that includes the table, the map surface, and the trail lines. There are separate scenes for the birds that flock over the table, and for the clouds that float above it.  To add entities to a scene, drag assets from the project browser to the scene\u2019s hierarchy view or 3D view. If the asset you drag is a type that can be represented as an entity, Reality Composer Pro adds it to your scene. You can select any asset in the scene hierarchy or the 3D view and change its location, rotation, and scale using the inspector on the right side of the window or the manipulator in the 3D view. RealityKit follows a design pattern called Entity Component System (ECS). In an ECS app, you store additional data on an entity using components and can implement entity behavior by writing systems that use the data from those components. You can add and configure components to entities in Reality Composer Pro, including both shipped components like PhysicsBodyComponent, and custom components that you write and place in the Sources folder of your Reality Composer Pro Swift package. You can even create new components in Reality Composer Pro and then edit them in Xcode. For more information about ECS, see Understanding RealityKit\u2019s modular architecture. Diorama uses custom components to identify which transforms are points of interest, to mark the birds so the app can make sure they flock together, and to control the opacity of entities that are specific to just one of the two maps. To add a component to an entity, select that entity in the hierarchy view or 3D view. At the bottom right of the inspector window, click on the Add Component button. A list of available components appears and the first item in that list is New Component. This item creates a new component class, and optionally a new system class, and adds the component to the selected entity. If you look at the list of components, you see the PointOfInterestComponent that Diorama uses to indicate which transforms are points of interest. If the selected entity doesn\u2019t already contain a PointOfInterestComponent, selecting that adds it to the selected entity. Each entity can only have one component of a specific type. You can edit the values of the existing component in the inspector, which changes what shows up when you tap that point of interest in the app.  In Reality Composer Pro, a transform is an empty entity that marks a point in space. A transform contains a location, rotation, and scale, and its child entities inherit those. But, transforms have no visual representation and do nothing by themselves. Use transforms to mark locations in your scene or organize your entity hierarchy. For example, you might make several entities that need to move together into child entities of the same transform, so you can move them together by moving the parent transform. Diorama uses transforms with a PointOfInterestComponent to indicate points of interest on the map. When the app runs, those transforms mark the location of the floating placards with the name of the location. Tapping on a placard expands it to show more detailed information. To turn transforms into an interactive view, the app looks for a specific component on transforms called a PointOfInterestComponent. Because a transform contains no data other than location, orientation, and scale, it uses this component to hold the data the app needs to display on the placards. If you open the DioramaAssembled scene in Reality Composer Pro and click on the transform called Cathedral_Rocks, you see the PointOfInterestComponent in the inspector.  To load a Reality Composer Pro scene, use load(named:in:), passing the name of the scene you want to load and the project\u2019s bundle. Reality Composer Pro Swift packages define a constant that provides ready access to its bundle. The constant is the name of the Reality Composer Pro project with \u201cBundle\u201d appended to the end. In this case, the project is called RealityKitContent, so the constant is called RealityKitContentBundle. Here\u2019s how Diorama loads the map table in the RealityView initializer: The load(named:in:) function is asynchronous when called from an asynchronous context. Because the content closure of the RealityView initializer is asynchronous, it automatically uses the async version to load the scene.  Note that when using it asynchronously, you must call it using the await keyword. Diorama adds a PointOfInterestComponent to a transform to display details about interesting places. Every point of interest\u2019s name appears in a view that floats above its location on the map. When you tap the floating view, it expands to show detailed information, which the app pulls from the PointOfInterestComponent. The app shows these details by creating a SwiftUI view for each point of interest and querying for all entities that have a PointOfInterestComponent using this query declared in ImmersiveView.swift: In the RealityView initializer, Diorama queries to retrieve the points of interest entities and passes them to a function called createLearnMoreView(for:), which creates the view and saves it for display when it\u2019s tapped. Diorama displays the information added to a PointOfInterestComponent in a LearnMoreView, which it stores as an attachment. Attachments are SwiftUI views that are also RealityKit entities and that you can place into a RealityKit scene at a specific location. Diorama uses attachments to position the view that floats above each point of interest. The app first checks to see if the entity has a component called PointOfInterestRuntimeComponent. If it doesn\u2019t, it creates a new one and adds it to the entity. This new component contains a value you only use at runtime that you don\u2019t need to edit in Reality Composer Pro. By putting this value into a separate component and adding it to entities at runtime, Reality Composer Pro never displays it in the inspector. The PointOfInterestRuntimeComponent stores an identifier called an attachment tag, which uniquely identifies an attachment so the app can retrieve and display it at the appropriate time. Next, Diorama creates a SwiftUI view called a LearnMoreView with the information from the PointOfInterestComponent, tags that view, and stores the tag in the PointOfInterestRuntimeComponent. Finally, it stores the view in an AttachmentProvider, which is a custom class that maintains references to the attachment views so they don\u2019t get deallocated when they\u2019re not in a scene. Assigning a view to an attachment provider doesn\u2019t actually display that view in the scene. The initializer for RealityView has an optional view builder called attachments that\u2019s used to specify the attachments. In the update closure of the initializer, which RealityKit calls when the contents of the view change, the app queries for entities with a PointOfInterestRuntimeComponent, uses the tag from that component to retrieve the correct attachment for it, and then adds that attachment and places it above its location on the map. To switch between the two different topographical maps, Diorama shows a slider that morphs the map between the two locations. To accomplish this, and to draw elevation lines on the map, the FlatTerrain entity in the DioramaAssembled scene uses a Shader Graph material. Shader Graph is a node-based material editor that\u2019s built into Reality Composer Pro. Shader Graph gives you the ability to create dynamic materials that you can change at runtime. Prior to Reality Composer Pro, the only way to implement a dynamic material like this was to create a CustomMaterial and write Metal shaders to implement the necessary logic. Diorama\u2019s DynamicTerrainMaterialEnhanced does two things. It draws contour lines on the map based on height data stored in displacement map images, and it also offsets the vertices of the flat disk based on the same data. By interpolating between two different height maps, the app achieves a smooth transition between the two different sets of height data. When you build Shader Graph materials, you can give them input parameters called promoted inputs that you set from Swift code. This allows you to implement logic that previously required writing a Metal shader. The materials you build in the editor can affect both the look of an entity using the custom surface output node, which equates to writing Metal code in a fragment shader, or the position of vertices using the geometry modifier output, which equates to Metal code running in a vertex shader.  Node graphs can contain subgraphs, which are similar to functions. They contain reusable sets of nodes with inputs and outputs. Subgraphs contain the logic to draw the contour lines and the logic to offset the vertices. Double-click a subgraph to edit it. For more information about building materials using Shader Graph, see Explore Materials in Reality Composer Pro. To change the map, DynamicTerrainMaterialEnhanced has a promoted input called Progress. If that parameter is set to 1.0, it displays Catalina Island. If it\u2019s set to 0, it displays Yosemite. Any other number shows a state in transition between the two. When someone manipulates the slider, the app updates that input parameter based on the slider\u2019s value. Important Shader Graph material parameters are case-sensitive. If the capitalization is wrong, your code won\u2019t actually update the material. The app sets the value of the input parameter in a function called handleMaterial() that the slider\u2019s .onChanged closure calls. That function retrieves the ShaderGraphMaterial from the terrain entity and calls setParameter(name:value:) on it.\n\n\n\nlet entity = try await Entity.load(named: \"DioramaAssembled\", \n                                   in: RealityKitContent.RealityKitContentBundle)\n\n\n\n\nstatic let markersQuery = EntityQuery(where: .has(PointOfInterestComponent.self))\n\n\n\n\nsubscriptions.append(content.subscribe(to: ComponentEvents.DidAdd.self, componentType: PointOfInterestComponent.self, { event in\n    createLearnMoreView(for: event.entity)\n}))\n\n\n\n\nstruct PointOfInterestRuntimeComponent: Component {\n    let attachmentTag: ObjectIdentifier\n}\n\n\n\n\nlet tag: ObjectIdentifier = entity.id\n\n\nlet view = LearnMoreView(name: pointOfInterest.name,\n                         description: pointOfInterest.description ?? \"\",\n                         imageNames: pointOfInterest.imageNames,\n                         trail: trailEntity,\n                         viewModel: viewModel)\n    .tag(tag)\nentity.components[PointOfInterestRuntimeComponent.self] = PointOfInterestRuntimeComponent(attachmentTag: tag)\n\n\nattachmentsProvider.attachments[tag] = AnyView(view)\n\n\n\n\nForEach(attachmentsProvider.sortedTagViewPairs, id: \\.tag) { pair in\n    pair.view\n}\n\n\n\n\nviewModel.rootEntity?.scene?.performQuery(Self.runtimeQuery).forEach { entity in\n\n\n    guard let attachmentEntity = attachments.entity(for: component.attachmentTag) else { return }\n    \n    if let pointOfInterestComponent = entity.components[PointOfInterestComponent.self] {\n        attachmentEntity.components.set(RegionSpecificComponent(region: pointOfInterestComponent.region))\n        attachmentEntity.components.set(OpacityComponent(opacity: 0))\n    }\n    \n    viewModel.rootEntity?.addChild(attachmentEntity)\n    attachmentEntity.setPosition([0, 0.2, 0], relativeTo: entity)\n}\n\n\n\n\nprivate func handleMaterial() {\n    guard let terrain = viewModel.rootEntity?.terrain,\n            let terrainMaterial = terrainMaterial else { return }\n    do {\n        var material = terrainMaterial\n        try material.setParameter(name: materialParameterName, value: .float(viewModel.sliderValue))\n        \n        if var component = terrain.modelComponent {\n            component.materials = [material]\n            terrain.components.set(component)\n        }\n        \n        try terrain.update(shaderGraphMaterial: terrainMaterial, { m in\n            try m.setParameter(name: materialParameterName, value: .float(viewModel.sliderValue))\n        })\n    } catch {\n        print(\"problem: \\(error)\")\n    }\n}\n", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/understanding-the-realitykit-modular-architecture", "title": "Understanding RealityKit\u2019s modular architecture | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode Tracking specific points in world space Placing content on detected planes Incorporating real-world surroundings in an immersive experience Setting up access to ARKit data Happy Beam ARKit Capturing screenshots and video from Apple Vision Pro for 2D viewing Designing RealityKit content with Reality Composer Pro Understanding RealityKit\u2019s modular architecture Diorama Swift Splash RealityKit and Reality Composer Pro Positioning and sizing windows Presenting windows and spaces Hello World SwiftUI Improving accessibility support in your visionOS app Adopting best practices for privacy and user preferences Designing for visionOS Design Drawing sharp layer-based content in visionOS Creating fully immersive experiences in your app Adding 3D content to your app Creating your first visionOS app App construction  / visionOS / Understanding RealityKit\u2019s modular architecture API Changes: None RealityKit is a 3D framework designed for building apps, games, and other immersive experiences. Although it\u2019s built in an object-oriented language and uses object-oriented design principles, RealityKit\u2019s architecture avoids heavy use of composition \u2014 where objects are built by adding instance variables that hold references to other objects \u2014 in favor of a modular design based on a paradigm called Entity Component System (ECS) that divides application objects into one of three types. Following the ECS paradigm allows you to re-use the functionality contained in a component in many different entities, even if they have very different inheritance chains. Even if two objects have no common ancestors other than Entity, you can add the same components to both of them and give them the same behavior or functionality. Entities are the core actors of RealityKit. Any object that you can put into a scene, whether visible or not, is an entity and must be a descendent of Entity. Entities can be 3D models, shape primitives, lights, or even invisible items like sound emitters or trigger volumes. Add components to entities to let them store additional state relevant to a specific type of functionality. Entities themselves contain relatively few properties: Nearly all entity state is stored on an entity\u2019s components. RealityKit provides a number of entity types you use to represent different kinds of objects. For example, a ModelEntity represents a 3D model, such as one imported from a .usdz or .reality file. These provided entities are essentially just an Entity with certain components already added to them. Adding a ModelComponent to an instance of Entity, for example, results in an entity with identical functionality to a ModelEntity. Components are modular building blocks that you add to an entity; they identify which entities a system will act on, and maintain the per-entity state that systems rely on. Components can contain logic, but limit component logic to code that validates its property values or sets its initial state. Use systems for any logic that affects the behavior of entities or that potentially changes their state on every frame. To add accessibility information to an entity, for example, add a AccessibilityComponent to it and populate its fields with the information the accessibility system needs, such as putting the description that VoiceOver reads into its label property. Keep in mind that an entity can only hold one copy of any particular type of component at a time. So, for example, you can\u2019t add two accessibility components to one entity. If you add an accessibility component to an entity that already has one, the new component replaces the previous one. A System contains code that RealityKit calls on every frame to implement a specific type of entity behavior or to update a particular type of entity state. Systems use components to store their entity-specific state and query for entities to act on by looking for ones with a specific component or combination of components. For example, a game might have a damage system that monitors and updates the health of every entity that can be damaged or destroyed. Systems typically work together with one or more components, so that damage system might use a health component to keep track of how much damage each entity has taken and how much each one is able to take before it\u2019s destroyed. It might also interact with other components. For example, an entity might have an armor component that provides protection to the entity, and the damage system would also need to use the state stored in that component. Every frame, the damage system queries for entities that have the health component and updates values on those entities\u2019 components based on the current state of the app. If an entity has taken too much damage, the system might trigger a specific animation or remove the entity from the scene. Writing entity logic in a system avoids duplication of work. Using traditional OOP design patterns, where this type of logic would reside on the entity class, can often result in the same calculations being performed multiple times, once for every entity potentially affected. No matter how many entities the calculation potentially impacts the system only has to do the calculation once. For more information on creating systems, see Implementing systems for entities in a scene Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/designing-realitykit-content-with-reality-composer-pro", "title": "Designing RealityKit content with Reality Composer Pro | Apple Developer Documentation", "content": "App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space   Use Reality Composer Pro to visually design, edit, and preview RealityKit content. In Reality Composer Pro, you can create one or more scenes, which  act as a container for RealityKit content. Scenes contain hierarchies of entities, which are virtual objects such as 3D models.  In addition to helping you compose scenes, Reality Composer Pro also gives you the ability to add and configure components \u2014 even custom components that you\u2019ve written \u2014 to the entities in your scenes and also lets you create complex materials and effects using a node-based material editor called Shader Graph. When you create a visionOS project in Xcode, it also contains a default Reality Composer Pro project named RealityKitContent within the Packages folder, which is a Swift package. The RealityKitContent package can include images, 3D models, and other assets like audio and video files. The assets you add to your project go in the RealityKitContent.rkassets bundle, while your source code goes into its Sources directory. The package also contains a file called Package.realitycomposerpro, which is the actual Reality Composer Pro project. To launch Reality Composer Pro, double-click the Package.realitycomposerpro file in the Project navigator, or click the Open in Reality Composer Pro button. If your project doesn\u2019t already have a Reality Composer Pro project, you can launch Reality Composer Pro directly by choosing Xcode > Open Developer Tool > Reality Composer Pro. For efficiency, store all of your RealityKit assets in Reality Composer Pro projects. Xcode compiles Reality Composer Pro projects into a more efficient format when you build your app. Note Loading assets from a .reality file is considerably faster and more resource efficient than loading individual asset files. The Reality Composer Pro window has several sections. The top-half displays the active scene. If you have multiple scenes, the window shows a tab bar at the top with one tab for each open scene. A scene in Reality Composer Pro is an entity hierarchy stored in a .usda file. The left side of the top pane contains the hierarchy browser, which shows a tree representation of the entities in the active scene. You can toggle it using the top-left toolbar button to reveal errors and warnings. The middle pane is the 3D View, which shows a 3D representation of the active scene. The top-right is the inspector, which shows configurable values for the item selected in the 3D view, hierarchy view, or Shader Graph, depending on which has focus. Tip A Reality Composer Pro scene can represent an entire RealityKit scene, and you can have multiple scenes in your Reality Composer Pro project, each driving a different RealityView in the same app. A scene can also contain a collection of entities to use as a building block. For example, if you had an airplane model, you might build a scene for it that contains its 3D model, a particle effect to make smoke come out its engine, and audio entities or components that represent the various sounds a plane makes. Your app could then load those combined assets and use them together anywhere it needs. The bottom half of Reality Composer Pro contains the following four tabs: Displays all of the assets in your project. An advanced, node-based material editor. A tool for combining sound assets. Information about the currently open scene, such as the number of entities, vertices, and animations it contains.  Reality Composer Pro projects start with a single empty scene called Scene which is stored in a file called Scene.usda. You can create as many additional scenes as you need by choosing File > New > Scene. New scenes open as tabs along the top of the window, and they also appear in the Project Browser as .usda files. If you close a scene\u2019s tab and need to re-open it, double-click on the scene\u2019s .usda file in the Project Browser. If you no longer need a scene, delete its .usda file from the Project Browser or remove it from your project\u2019s .rkassets bundle in Xcode. To delete a scene: Close the scene tab by selecting File > Close Tab Select the scene\u2019s .usda file in the Project Browser Control-click the scene\u2019s .usda file  the Project Browser. Choose Delete from the contextual menu. Click Move to Trash. This removes the scene\u2019s .usda and the scene tab at the top of the window. In Reality Composer Pro, you design scenes by first importing assets into your project. Then add assets to scenes and move, rotate, and scale them. The Project Browser tab displays all of the asset files in your project. You can add new assets by dragging them to the Project Browser or by choosing File > Import and select the assets to add to your project. To add an asset from the Project Browser to the current scene, drag it to the 3D view in the center of the window, or to the hierarchy view in the top-left of the window. Note Reality Composer Pro projects can contain assets not used in any scene. Such assets are still compiled into your app and can be loaded at runtime and take full advantage of the efficient loading process for .reality files. Reality Composer Pro can represent many assets as entities, but it can\u2019t represent all assets that way; for example: USDZ models do become an entity or entity hierarchy when you add them to a scene. Image files do not become an entity. Reality Composer Pro only uses image assets indirectly, such as being the source texture for materials you build in Shader Graph. If you drag assets that Reality Composer Pro can\u2019t turn into an entity, nothing happens. Add any 3D models, animations, sounds, and image files you need to your project. You can organize your assets into subfolders to make the Project Browser more manageable as your project grows in size. Reality Composer Pro has a library of assets that you can use in your own apps. You can access the library by clicking the Add button (+) in the toolbar. Click the icon of the down-arrow inside a circle next to an asset to download the asset to Reality Composer Pro. When the download finishes, you can double-click or drag the asset into your project.  Important Reality Composer Pro treats your imported assets as read-only. Changes you make to assets in a scene only affect that scene\u2019s copy of the asset. The changes you make are stored in the scene\u2019s .usda file, not in the original asset. That means you can work without fear of inadvertently changing other scenes. If you plan to make significant changes to an imported 3D model, such as by replacing its materials with dynamic Shader Graph materials, import the model as a.usdc file instead of as a .usdz file, and then separately import just the supporting assets you need to avoid Xcode compiling assets that you don\u2019t need into your app. All RealityKit entities in a scene exist at a specific position, orientation, and scale, even if that entity has no visual representation. When you click to select an entity in the 3D view or hierarchy view, Reality Composer Pro displays: A manipulator over the entity in the 3D view. Any configurable values from the entity\u2019s components in the inspector on the right. You can use the manipulator to move, rotate, and scale the selected entity. To move the selected entity around the 3D scene, drag the small colored cone that corresponds to the axis you want to move it along. Alternatively, you can drag the entity itself to move it freely relative to your viewing angle. To rotate the selected entity, click on the manipulator\u2019s rotation control, which looks like a circle, and drag in a circular motion. Reality Composer Pro\u2019s manipulator only shows one rotation control at a time. To rotate an entity on one of the other axes, click the cone corresponding to the axis you want to rotate. For example, if you want to rotate the entity on the X axis, tap the red cone to bring up the red rotation handle for that axis. To scale the selected entity uniformly, click the rotation circle and drag away from the entity origin to scale it up, or toward the entity origin to scale it down. Because it scales uniformly, it doesn\u2019t matter which rotation handle Reality Composer Pro is showing. Note In the manipulator, Red indicates the X axis, Green indicates the Y axis, and Blue indicates the Z axis. Alternatively, you can make the same changes to the selected entity by typing new values into the transform component of the inspector. The transform component stores the position, rotation, and scale for an entity. The manipulator is just a visual way to change the values on this component.  Reality Composer Pro scenes can get quite complex and sometimes contain overlapping entities, which can be difficult to work with. To simplify a scene, you can deactivate entities to remove them from the 3D view. Deactivate entities by Control-clicking them and selecting Deactivate from the contextual menu. The entity still exists in your project and is shown in the hierarchy view, albeit grayed out and without any child entities. It won\u2019t, however, appear in the 3D view. Xcode doesn\u2019t compile deactivated entities into your app\u2019s bundle, so it\u2019s important to re-activate any entities your app needs before saving your project. To reactivate an entity, Control-click the entity in the hierarchy view and select Activate from the contextual menu. RealityKit follows a design pattern called Entity-Component-System (ECS). In ECS, you store data on an entity using components and then implement entity behavior by writing systems that use the data from those components. You can add and configure components to entities in Reality Composer Pro, including both built-in components like ParticleEmitterComponent, and custom components that you write and place in the Sources folder of your Reality Composer Pro Swift package. You can also create new components in Reality Composer Pro and edit them in Xcode. For more information about ECS, see Understanding RealityKit\u2019s modular architecture. To add a component to an entity, select that entity in the hierarchy view or 3D view. At the bottom-right of the inspector window, click Add Component. A list of available components appears with New Component at the top. If you select the first item, Reality Composer Pro creates a new component class in the Sources folder, and optionally a new system class. It also adds the component to the selected entity. If you select any other item in the list, it adds that component to the selected entity if it doesn\u2019t already have that component.  Reality Composer Pro scenes are hierarchies of RealityKit entities. You can change the relationship between entities in the hierarchy browser except for parts of the hierarchy imported from a .usdz file, which Reality Composer Pro treats as a read-only file. To change the relationship between entities, or to create a relationship between two currently unrelated entities, use the hierarchy view and drag an entity onto the entity that you want it to be part of. If you want an entity to become a root entity, drag it to the Root transform at the top of the hierarchy view. When you import a USDZ model into Reality Composer Pro, it creates a RealityKit material for every physically-based rendering (PBR) material the asset contains. Reality Composer Pro displays materials in the hierarchy view just like it displays entities, except it uses a paintbrush icon. Reality Composer Pro doesn\u2019t display materials in the 3D view. Note The library in Reality Composer Pro contains materials for several common real-world surfaces like metal, wood, and denim that you can import into your project. If you select a PBR material in the hierarchy view, you can edit it using the inspector. You can replace images, colors, or values for any of the PBR attributes with another image, color, or value of your choosing. Any changes you make to a material affects any entity that\u2019s bound to that material. You can also create new materials from scratch by clicking the Add button (+) at the bottom of the scene hierarchy and choosing Material.  PBR materials are great at reproducing real-world surfaces. However, they can\u2019t represent nonrealistic materials like cartoon shaders, and they can\u2019t contain logic. This means that you can\u2019t animate a PBR material or have it react to input from your app. Reality Composer Pro offers a second type of material called a custom material. You can build and edit custom materials using the Shader Graph tab. Shader Graph provides a tremendous amount of control over materials and allows you to do things that would otherwise require writing Metal shaders. For more information on writing Metal shaders, see Metal. Note RealityKit doesn\u2019t represent Reality Composer Pro custom materials as an instance of CustomMaterial, as you might expect. Instead, RealityKit represents these materials as ShaderGraphMaterial instances.  The materials you build in the editor can affect both the look of an entity and its shape. If you build a node graph and connect it to the Custom Surface pin on the output node, that node graph controls the surface appearance of the model and roughly equates to writing Metal code in a fragment shader. If you build a node graph and connect it to the Custom Geometry Modifier output pin, those nodes control the shape of the entity, which equates to Metal code running in a vertex shader. Nodes represent values and operations and serve the same purpose as either a variable or constant, or a function in Metal. If you need the sine of a value, for example, connect the value\u2019s output node to the input pin of a Sin node. Add new nodes to the graph by double-clicking the background of the Shader Graph view or click the New Node button on the right side of the screen. Important Some nodes, like Sin, are universal and can be used with either output pin. Other nodes are specific to either the Custom Surface or Geometry Modifier outputs. If a node name starts with Geometry Modifier, you can only connect it to the Geometry Modifier output pin. If the node\u2019s name starts with \u201cSurface\u201d, you can only connect it to the Custom Surface output pin. To unlock the real power of Shader Graph, you need to be able to change values on the material from Swift code. Shader Graph allows you to do this by creating promoted inputs, which are parameters you can set and read from Swift to change your material at runtime. If you have a feature that you want to turn on and off, you might create a Boolean input parameter and have conditional logic based on its value. If you want to smoothly interpolate between two colors, you might create a Float input parameter and use it to control how to interpolate between the two colors. You can Control-click on a constant node and select Promote to turn it into a promoted input. You can also turn a promoted input back into a constant by Control-clicking it and selecting Demote. If you don\u2019t have an existing constant to promote, you can create new promoted inputs using the inspector. The New Input button only shows up in the inspector when you select a material in the hierarchy view but have no nodes selected in the Shader Graph tab.  To change the value of an input parameter from Swift code, use setParameter(name:value:), passing the name of the parameter and the new value. Note that parameter names are case sensitive, so your name string must exactly match what you called the parameter in Shader Graph. For examples of Shader Graph use, see Diorama and Happy Beam. If your project has multiple scenes that share assets, you can use references to avoid creating duplicate assets. A reference acts like an alias in Finder \u2014 it points to the original asset and functions just as if it were another copy of that asset. Create references using the inspector. By default, the references section is hidden for entities and materials that don\u2019t have any references. To add a new reference to an asset or material that doesn\u2019t have one, choose Reality Composer Pro > Settings and uncheck Hide Empty References.  To add a reference, click the Add button (+) at the bottom of the references section in the inspector, choose the .usda file for the scene that contains the asset, then choose the asset you want to link to. After you do that, the selected entity or material becomes a copy of the one you linked to. Important If you make changes to a linked asset, those changes will affect every linked reference. If you have an Apple Vision Pro connected to your Mac, choose Preview > Play or click the preview button in the Reality Composer Pro toolbar to view your scene on device. The Preview button is the left-most button on the right side of the toolbar \u2014 the one with an Apple Vision Pro icon. If you have multiple Apple Vision Pro devices connected, choose which device to use by clicking the pull-down menu next to the Preview button. Loading a Reality Composer Pro scene is nearly identical to loading a USDZ asset from your app bundle, except you have to specify the Reality Composer Pro package bundle instead. You typically do this in the make closure of a RealityView initializer. Reality Composer Pro packages define a global constant that points to its bundle, which is named after the project with \u201cBundle\u201d appended to it. In the default Xcode visionOS template, the Reality Composer Pro project is called RealityKitContent, so the global bundle variable is called realityKitContentBundle: Note The code above saves a reference to the root node. This isn\u2019t required, but with RealityView, unlike ARView on iOS and macOS, you don\u2019t have ready access to the scene content, so it\u2019s often handy to maintain your own reference to the root entity of your scene in your app\u2019s data model. When RealityKit finishes loading the scene, the scene variable contains the root entity of the scene you specified. Add it to content and RealityKit displays it to the user.\n\n\n\nRealityView { content in\n    if let scene = try? await Entity.load(named: \"Biplane\", \n                                          in: realityKitContentBundle) {\n        myDataModel.add(scene) \n        content.add(scene)\n    }\n} update: { content in\n    // ...\n}\n", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/capturing-screenshots-and-video-from-your-apple-vision-pro-for-2d-viewing", "title": "Capturing screenshots and video from Apple Vision Pro for 2D viewing | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space  / visionOS / Capturing screenshots and video from Apple Vision Pro for 2D viewing API Changes: None Use screenshots and short videos of your visionOS app to showcase your user interface, highlight functionality, and demonstrate usage. Help people understand what to expect from an immersive experience by recording content from Apple Vision Pro that includes your app and its surroundings. The system renders content with spatial effects and optimizations for viewing during immersive experiences. Techniques that improve rendering performance during normal operation \u2014 such as foveated rendering, which reduces image quality in the peripheral \u2014 don\u2019t translate well to 2D displays. To produce high-resolution content for people to view on 2D displays, the system needs to render without these effects and drop some optimizations. Use Developer Capture in Reality Composer Pro to notify the system to reconfigure rendering, and capture screenshots or high-resolution video, including sound, for up to 60 seconds from Apple Vision Pro. Note For guidance on the screenshots and previews you include in your app\u2019s product page, see Submit your apps to the App Store for Apple Vision Pro. Before capturing screenshots and video from your device, pair it with a Mac that has Xcode and the visionOS SDK installed. For instructions on pairing your device, see Running your app in Simulator or on a device. Select a well-lit location that\u2019s free from clutter. Avoid including objects that might distract the audience or get in the way of your app\u2018s windows and 3D content. Include enough detail in the scene to provide context and anchoring points. Avoid material that you don\u2019t have permission to capture, including people, screens, branded products, logos, artwork, and other intellectual property. Use the version of your app that you intend to share with your audience. Build and install your app using a release configuration. This configuration enables code optimizations for better runtime performance and disables the generation of debugging information. Debug configurations typically disable code optimizations and might include UI you don\u02bct intend to share. Don\u2019t use them to record video for previews you intend to share. Build schemes manage the build configuration Xcode uses during build actions, for more information see Customizing the build schemes for a project. Plan the tasks you intend to capture ahead of time and keep them short and focused. Launch your app and go to the state where you plan to begin the capture. Reduce unnecessary processing overhead on Apple Vision Pro by quitting other apps and avoiding background tasks. To capture screenshots or video from a device, select your device from the capture dialog in Reality Composer Pro: Launch Reality Composer Pro. Choose Open Developer Tool > Reality Composer Pro from the Xcode menu. Launch Reality Composer Pro. Choose Open Developer Tool > Reality Composer Pro from the Xcode menu. Choose File > Developer Capture to bring up the Developer Capture dialog. Choose File > Developer Capture to bring up the Developer Capture dialog. Select the device to capture from the pop-up menu. Select the device to capture from the pop-up menu.  If you see the message \u201cPreparing, wait for the device to be ready\u201d. You can click the info button that appears to the right of the pop-up menu for more information. To begin capturing screenshots from Apple Vision Pro, click the button with the still camera icon in the capture dialog. The system begins your capture session:  To capture a screenshot immediately, without a countdown, press the spacebar. Click the countdown button to capture a screenshot after a 3 second countdown. Continue to keep relevant content centered and in frame for screenshots. The aspect ratio of screenshots crops content that appears at the sides of an experience. The status area of the capture dialog displays the time remaining before the system ends the capture session. Click the stop button from your Mac to end the capture session yourself. To begin capturing video from the device, click the video camera button in the Developer Capture dialog. This begins a countdown. When the countdown reaches 0, the capture session begins. As the capture process happens, the video changes because the system reconfigures to render content for viewing in 2D. You might notice reduced responsiveness from the device during the session as it devotes more processing to render and capture the video. While recording on the device, perform your planned interactions. Keep relevant content centered and in frame. The aspect ratio of the video you capture crops content that appears at the sides of an experience. Keep your head stable, and use slow, steady movement to transition the focus of the device when necessary. When viewing the video you capture in 2D, small head movements appear amplified and might be jarring to the audience. The capture session ends when the elapsed time exceeds 60 seconds. You can click the record button again from your Mac to end the session sooner. Note To begin a capture, the device must have a stable connection to your Mac and start at low power and thermal levels to stay below thresholds necessary to achieve consistent frame rates. When capturing multiple sessions, you might need to wait between each session. Each recording session creates a QuickTime Movie file (.mov) and saves it to the desktop of your Mac. The file includes video captured at 30 FPS using 10-bit HEVC in HDTV Rec. 709 color space with system audio recorded in 32-bit floating-point linear PCM. Review the video to make sure that it includes all the content you planned and it doesn\u2019t include any unexpected elements. Ensure that the transitions and animations are smooth and frame rates are consistent. Use additional video-editing tools to trim, edit, and apply post-processing, such as stabilization, to the video to create a high-quality preview. Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos#arkit", "title": "visionOS | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space  / visionOS API Changes: None visionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.  Developing for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app\u2019s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS. Start with a familiar window-based experience to introduce people to your content. From there, add SwiftUI scene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences. Build your app\u2019s 3D content with RealityKit and Reality Composer Pro, and display it with a RealityView. In an immersive experience, use ARKit to integrate your content with the person\u2019s surroundings.  People can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures. SwiftUI provides built-in support for these standard gestures, so rely on them for most of your app\u2019s input. When you want to go beyond the standard gestures, use ARKit to create custom gestures. Tap to select Pinch to rotate Manipulate objects Create custom gestures Explore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash. visionOS Overview Expand your app into immersive spaces Explore new kinds of interaction Dive into featured sample apps Topics Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/happybeam", "title": "Happy Beam | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space  / visionOS / Happy Beam API Changes: None In visionOS, you can create fun, dynamic games and apps using several different frameworks to create new kinds of spatial experiences: RealityKit, ARKit, SwiftUI, and Group Activities. This sample introduces Happy Beam, a game where you and your friends can hop on a FaceTime call and play together. You\u2019ll learn the mechanics of the game where grumpy clouds float around in the space, and people play by making a heart shape with their hands to project a beam. People aim the beam at the clouds to cheer them up, and a score counter keeps track of how well each player does cheering up the clouds. Most apps in visionOS launch as a window that opens different scene types depending on the needs of the app. Here you see how Happy Beam presents a fun interface to people by using several SwiftUI views that display a welcome screen, a coaching screen that gives instructions, a scoreboard, and a game-ending screen. Welcome window Instructions Scoreboard Ending window     The following shows you the primary view in the app that displays each phase of gameplay: When 3D content starts to appear, the game opens an immersive space to present content outside of the main window and in a person\u2019s surroundings. The HappyBeam container view declares a dependency on openImmersiveSpace: It later uses that dependency to open the space from the app\u2019s declaration when it\u2019s time to start showing 3D content: The Happy Beam app recognizes the central heart-shaped hands gesture using ARKit\u2019s support for 3D hand tracking in visionOS. Using hand tracking requires a running session and authorization from the wearer. It uses the NSHandsTrackingUsageDescription user info key to explain to players why the app requests permission for hand tracking.  Hand-tracking data isn\u2019t available when your app is only displaying a window or volume. Instead, it\u2019s available when you present an immersive space, as in the previous example. You can detect gestures using ARKit data with a level of accuracy that depends on your use case and intended experience. For example, Happy Beam could require strict positioning of finger joints to closely resemble a heart shape. Instead, however, it prompts people to make a heart shape and uses a heuristic to indicate when the gesture is close enough. The following checks whether a person\u2019s thumbs and index fingers are almost touching: To support accessibility features and general user preferences, include multiple kinds of input in an app that uses hand tracking as one form of input. Happy Beam supports several kinds of input: Interactive hands input from ARKit with the custom heart gesture. Drag gesture input to rotate the stationary beam on its platform. Accessibility components from RealityKit to support custom actions for cheering up the clouds. Game Controller support to make control over the beam more interactive from Switch Control. The 3D content in the app comes in the form of assets that you can export from Reality Composer Pro. You place each asset in the RealityView that represents your immersive space. The following shows how Happy Beam generates clouds when the game starts, as well as materials for the floor-based beam projector. Because the game uses collision detection to keep score \u2014 the beam cheers up grumpy clouds when they collide \u2014 you make collision shapes for each model that might be involved. You use the Group Activities framework in visionOS to support SharePlay during a FaceTime call. Happy Beam uses Group Activities to sync the score, active players list, and the position of each player\u2019s projected beam. Note Developers using the Apple Vision Pro developer kit can test spatial SharePlay experiences on-device by installing the Persona Preview Profile. Use a reliable channel to send information that\u2019s important to be correct, even if it can be slightly delayed as a result. The following shows how Happy Beam updates the game model\u2019s score state in response to a score message: Use an unreliable messenger for sending data with low-latency requirements. Because the delivery mode is unreliable, some messages might not make it. Happy Beam uses the unreliable mode to send live updates to the position of the beam when each participant in the call chooses the Spatial option in FaceTime. The following shows how Happy Beam serializes beam data for each message: Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC\n\n\n\nstruct HappyBeam: View {\n    @Environment(\\.openImmersiveSpace) private var openImmersiveSpace\n    @Environment(GameModel.self) var gameModel\n    \n    @State private var session: GroupSession<HeartProjection>? = nil\n    @State private var timer = Timer.publish(every: 1, on: .main, in: .common).autoconnect()\n    @State private var subscriptions = Set<AnyCancellable>()\n    \n    var body: some View {\n        let gameState = GameScreen.from(state: gameModel)\n        VStack {\n            Spacer()\n            Group {\n                switch gameState {\n                case .start:\n                    Start()\n                case .soloPlay:\n                    SoloPlay()\n                case .lobby:\n                    Lobby()\n                case .soloScore:\n                    SoloScore()\n                case .multiPlay:\n                    MultiPlay()\n                case .multiScore:\n                    MultiScore()\n                }\n            }\n            .glassBackgroundEffect(\n                in: RoundedRectangle(\n                    cornerRadius: 32,\n                    style: .continuous\n                )\n            )\n        }\n    }\n}\n\n\n\n@main\nstruct HappyBeamApp: App {\n    @State private var gameModel = GameModel()\n    @State private var immersionState: ImmersionStyle = .mixed\n    \n    var body: some SwiftUI.Scene {\n        WindowGroup(\"HappyBeam\", id: \"happyBeamApp\") {\n            HappyBeam()\n                .environmentObject(gameModel)\n        }\n        .windowStyle(.plain)\n        \n        ImmersiveSpace(id: \"happyBeam\") {\n            HappyBeamSpace(gestureModel: HeartGestureModelContainer.heartGestureModel)\n                .environmentObject(gameModel)\n        }\n        .immersionStyle(selection: $immersionState, in: .mixed)\n    }\n}\n\n\n\n@Environment(\\.openImmersiveSpace) private var openImmersiveSpace\n\n\n\nif gameModel.countDown == 0 {\n    Task {\n        await openImmersiveSpace(id: \"happyBeam\")\n    }\n}\n\n\n\nTask {\n    do {\n        try await session.run([handTrackingProvider])\n    } catch {\n        print(\"ARKitSession error:\", error)\n    }\n}\n\n\n\n// Get the position of all joints in world coordinates.\nlet originFromLeftHandThumbKnuckleTransform = matrix_multiply(\n    leftHandAnchor.originFromAnchorTransform, leftHandThumbKnuckle.anchorFromJointTransform\n).columns.3.xyz\nlet originFromLeftHandThumbTipTransform = matrix_multiply(\n    leftHandAnchor.originFromAnchorTransform, leftHandThumbTipPosition.anchorFromJointTransform\n).columns.3.xyz\nlet originFromLeftHandIndexFingerTipTransform = matrix_multiply(\n    leftHandAnchor.originFromAnchorTransform, leftHandIndexFingerTip.anchorFromJointTransform\n).columns.3.xyz\nlet originFromRightHandThumbKnuckleTransform = matrix_multiply(\n    rightHandAnchor.originFromAnchorTransform, rightHandThumbKnuckle.anchorFromJointTransform\n).columns.3.xyz\nlet originFromRightHandThumbTipTransform = matrix_multiply(\n    rightHandAnchor.originFromAnchorTransform, rightHandThumbTipPosition.anchorFromJointTransform\n).columns.3.xyz\nlet originFromRightHandIndexFingerTipTransform = matrix_multiply(\n    rightHandAnchor.originFromAnchorTransform, rightHandIndexFingerTip.anchorFromJointTransform\n).columns.3.xyz\n\n\nlet indexFingersDistance = distance(originFromLeftHandIndexFingerTipTransform, originFromRightHandIndexFingerTipTransform)\nlet thumbsDistance = distance(originFromLeftHandThumbTipTransform, originFromRightHandThumbTipTransform)\n\n\n// Heart gesture detection is true when the distance between the index finger tips centers\n// and the distance between the thumb tip centers is each less than four centimeters.\nlet isHeartShapeGesture = indexFingersDistance < 0.04 && thumbsDistance < 0.04\nif !isHeartShapeGesture {\n    return nil\n}\n\n\n// Compute a position in the middle of the heart gesture.\nlet halfway = (originFromRightHandIndexFingerTipTransform - originFromLeftHandThumbTipTransform) / 2\nlet heartMidpoint = originFromRightHandIndexFingerTipTransform - halfway\n\n\n// Compute the vector from left thumb knuckle to right thumb knuckle and normalize (X axis).\nlet xAxis = normalize(originFromRightHandThumbKnuckleTransform - originFromLeftHandThumbKnuckleTransform)\n\n\n// Compute the vector from right thumb tip to right index finger tip and normalize (Y axis).\nlet yAxis = normalize(originFromRightHandIndexFingerTipTransform - originFromRightHandThumbTipTransform)\n\n\nlet zAxis = normalize(cross(xAxis, yAxis))\n\n\n// Create the final transform for the heart gesture from the three axes and midpoint vector.\nlet heartMidpointWorldTransform = simd_matrix(\n    SIMD4(xAxis.x, xAxis.y, xAxis.z, 0),\n    SIMD4(yAxis.x, yAxis.y, yAxis.z, 0),\n    SIMD4(zAxis.x, zAxis.y, zAxis.z, 0),\n    SIMD4(heartMidpoint.x, heartMidpoint.y, heartMidpoint.z, 1)\n)\nreturn heartMidpointWorldTransform\n\n\n\n@MainActor\nfunc placeCloud(start: Point3D, end: Point3D, speed: Double) async throws -> Entity {\n    let cloud = await loadFromRealityComposerPro(\n        named: BundleAssets.cloudEntity,\n        fromSceneNamed: BundleAssets.cloudScene\n    )!\n        .clone(recursive: true)\n    \n    cloud.generateCollisionShapes(recursive: true)\n    cloud.components[PhysicsBodyComponent.self] = PhysicsBodyComponent()\n    \n    var accessibilityComponent = AccessibilityComponent()\n    accessibilityComponent.label = \"Cloud\"\n    accessibilityComponent.value = \"Grumpy\"\n    accessibilityComponent.isAccessibilityElement = true\n    accessibilityComponent.traits = [.button, .playsSound]\n    accessibilityComponent.systemActions = [.activate]\n    cloud.components[AccessibilityComponent.self] = accessibilityComponent\n    \n    let animation = cloudMovementAnimations[cloudPathsIndex]\n    \n    cloud.playAnimation(animation, transitionDuration: 1.0, startsPaused: false)\n    cloudAnimate(cloud, kind: .sadBlink, shouldRepeat: false)\n    spaceOrigin.addChild(cloud)\n    \n    return cloud\n}\n\n\n\nsessionInfo.reliableMessenger = GroupSessionMessenger(session: newSession, deliveryMode: .reliable)\n\n\nTask {\n    for await (message, sender) in sessionInfo!.reliableMessenger!.messages(of: ScoreMessage.self) {\n        gameModel.clouds[message.cloudID].isHappy = true\n        gameModel\n            .players\n            .filter { $0.name == sender.source.id.asPlayerName }\n            .first!\n            .score += 1\n    }\n}\n\n\n\nsessionInfo.messenger = GroupSessionMessenger(session: newSession, deliveryMode: .unreliable)\n\n\n\n// Send each player's beam data during FaceTime calls where players have selected the Spatial option.\nfunc sendBeamPositionUpdate(_ pose: Pose3D) {\n    if let sessionInfo = sessionInfo, let session = sessionInfo.session, let messenger = sessionInfo.messenger {\n        let everyoneElse = session.activeParticipants.subtracting([session.localParticipant])\n        \n        if isShowingBeam, gameModel.isSpatial {\n            messenger.send(BeamMessage(pose: pose), to: .only(everyoneElse)) { error in\n                if let error = error { print(\"Message failure:\", error) }\n            }\n        }\n    }\n}", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/setting-up-access-to-arkit-data", "title": "Setting up access to ARKit data | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space  / visionOS / Setting up access to ARKit data API Changes: None In visionOS, ARKit can enable new kinds of experiences that leverage data such as hand tracking and world sensing. The system gates access to this kind of sensitive information. Because people can decline your app\u2019s request to use ARKit data or revoke access later, you need to provide alternative ways to use your app and to handle cases where your app loses access to data.  People need to know why your app wants to access data from ARKit. Add the following keys to your app\u2019s information property list to provide a user-facing usage description that explains how your app uses the data: Use this key if your app uses hand tracking. Use this key if your app uses image tracking, plane detection, or scene reconstruction. Note World tracking \u2014 unlike world sensing \u2014 doesn\u2019t require authorization. For more information, see Tracking specific points in world space. You can choose when someone sees an authorization request to use ARKit data. If you need precise control over when the request appears, call the requestAuthorization(for:) method on ARKitSession to explicitly authorize access at the time you call it. Otherwise, people see an authorization request when you call the run(_:) method. This is an implicit authorization because the timing of the request depends entirely on when you start the session. To help protect people\u2019s privacy, ARKit data is available only when your app presents a Full Space and other apps are hidden. Present one of these space styles before calling the run(_:) method. The following shows an app structure that\u2019s set up to use a space with ARKit: Call openImmersiveSpace from your app\u2019s user interface to create a space, start running an ARKit session, and kick off an immersive experience. The following shows a simple view with a button that opens the space: Someone might not want to give your app access to data from ARKit, or they might choose to revoke that access later in Settings. Handle these situations gracefully, and remove or transition content that depends on ARKit data. For example, you might fade out content that you need to remove, or recenter content to an appropriate starting position. If your app uses ARKit data to place content in a person\u2019s surroundings, consider letting people place content using the system-provided interface. Providing alternatives is especially important if you\u2019re using ARKit for user input. People using accessibility features, trackpads, keyboards, or other forms of input might need a way to use your app without ARKit. Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC\n\n\n\n@main\nstruct MyApp: App {\n    @State var session = ARKitSession()\n    @State var immersionState: ImmersionStyle = .mixed\n    var body: some Scene {\n        WindowGroup {\n            ContentView()\n        }\n        ImmersiveSpace(id: \"appSpace\") {\n            MixedImmersionView()\n            .task {\n                let planeData = PlaneDetectionProvider(alignments: [.horizontal])\n                \n                if PlaneDetectionProvider.isSupported {\n                    do {\n                        try await session.run([planeData])\n                        for await update in planeData.anchorUpdates {\n                            // Update app state.\n                        }\n                    } catch {\n                        print(\"ARKit session error \\(error)\")\n                    }\n                }\n            }\n        }\n        .immersionStyle(selection: $immersionState, in: .mixed)\n    }\n}\n\n\n\nstruct ContentView: View {\n    @Environment(\\.openImmersiveSpace) private var openImmersiveSpace\n    \n    var body: some View {\n        Button(\"Start ARKit experience\") {\n            Task {\n                await openImmersiveSpace(id: \"appSpace\")\n            }\n        }\n    }\n}", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/incorporating-real-world-surroundings-in-an-immersive-experience", "title": "Incorporating real-world surroundings in an immersive experience | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode Tracking specific points in world space Placing content on detected planes Incorporating real-world surroundings in an immersive experience Setting up access to ARKit data Happy Beam ARKit Capturing screenshots and video from Apple Vision Pro for 2D viewing Designing RealityKit content with Reality Composer Pro Understanding RealityKit\u2019s modular architecture Diorama Swift Splash RealityKit and Reality Composer Pro Positioning and sizing windows Presenting windows and spaces Hello World SwiftUI Improving accessibility support in your visionOS app Adopting best practices for privacy and user preferences Designing for visionOS Design Drawing sharp layer-based content in visionOS Creating fully immersive experiences in your app Adding 3D content to your app Creating your first visionOS app App construction  / visionOS / Incorporating real-world surroundings in an immersive experience API Changes: None Scene reconstruction helps bridge the gap between the rendered 3D content in your app and the person\u2019s surroundings. Use scene reconstruction in ARKit to give your app an idea of the shape of the person\u2019s surroundings and to bring your app experience into their world. Immersive experiences  \u2014 those that use the mixed space style \u2014 are best positioned to incorporate this kind of contextual information: scene reconstruction is only available in spaces and isn\u2019t as relevant for the full space style. In addition to providing a 3D mesh of the shape of different nearby objects, ARKit gives a classification to each mesh face it detects. For example, it might classify a face of a mesh as being part of an appliance, a piece of furniture, or structural information about the room like the position of walls and floors. The following video shows virtual cubes colliding with the scene reconstruction mesh, which makes the cubes appear to land on a table: Scene reconstruction requires the ARKitSession.AuthorizationType.worldSensing authorization type and corresponding usage description that you supply in your app\u2019s Info.plist file. The following starts a session and processes updates as ARKit refines its reconstruction of the person\u2019s surroundings: You can make rendered 3D content more lifelike by having it appear to interact physically with objects in the person\u2019s surroundings, like furniture and floors. Use RealityKit\u2019s collision components and physics support to provide these interactions in your app. The generateStaticMesh(from:) method bridges between scene reconstruction and RealityKit\u2019s physics simulation. Warning Be mindful of how much content you include in immersive scenes that use the mixed style. Content that fills a significant portion of the screen, even if that content is partially transparent, can prevent the person from seeing potential hazards in their surroundings. If you want to immerse the person in your content, configure your space with the full style. For more information, see Creating fully immersive experiences in your app. Use low-priority tasks to generate meshes, because generating them is a computationally expensive operation. The following creates a mesh entity with collision shapes using scene reconstruction: Note Scene reconstruction meshes only support the PhysicsBodyMode.static physics body component mode. Each object in the scene reconstruction mesh updates its originFromAnchorTransform information independently and requires a separate static mesh because ARKit subdivides its representation of the world into multiple, distinct sections. People using an app that leverages scene reconstruction typically don\u2019t need to see a visual rendering of the scene reconstruction mesh. The system already shows passthrough video in an immersive experience. However, temporarily displaying the scene reconstruction mesh can help while you\u2019re developing and debugging your app. In Xcode\u2019s debugging toolbar, click the Enable Visualizations button and select Collision Shapes. Because each element of the scene reconstruction mesh has a collision component, the details of the mesh appear in the debug visualization. For more information, see Diagnosing issues in the appearance of a running app. Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC\n\n\n\nRealityView { content in\n    content.add(model.setupContentEntity())\n}\n.task {\n    do {\n        if model.dataProvidersAreSupported && model.isReadyToRun {\n            try await model.session.run([model.sceneReconstruction, model.handTracking])\n        } else {\n            await dismissImmersiveSpace()\n        }\n    } catch {\n        logger.error(\"Failed to start session: \\(error)\")\n        await dismissImmersiveSpace()\n        openWindow(id: \"error\")\n    }\n}\n.task {\n    await model.processHandUpdates()\n}\n.task {\n    await model.monitorSessionEvents()\n}\n.task(priority: .low) {\n    await model.processReconstructionUpdates()\n}\n.gesture(SpatialTapGesture().targetedToAnyEntity().onEnded { value in\n    let location3D = value.convert(value.location3D, from: .global, to: .scene)\n    model.addCube(tapLocation: location3D)\n})\n\n\n\nfunc processReconstructionUpdates() async {\n    for await update in sceneReconstruction.anchorUpdates {\n        let meshAnchor = update.anchor\n\n\n        guard let shape = try? await ShapeResource.generateStaticMesh(from: meshAnchor) else { continue }\n        switch update.event {\n        case .added:\n            let entity = ModelEntity()\n            entity.transform = Transform(matrix: meshAnchor.originFromAnchorTransform)\n            entity.collision = CollisionComponent(shapes: [shape], isStatic: true)\n            entity.components.set(InputTargetComponent())\n            \n            entity.physicsBody = PhysicsBodyComponent(mode: .static)\n            \n            meshEntities[meshAnchor.id] = entity\n            contentEntity.addChild(entity)\n        case .updated:\n            guard let entity = meshEntities[meshAnchor.id] else { continue }\n            entity.transform = Transform(matrix: meshAnchor.originFromAnchorTransform)\n            entity.collision?.shapes = [shape]\n        case .removed:\n            meshEntities[meshAnchor.id]?.removeFromParent()\n            meshEntities.removeValue(forKey: meshAnchor.id)\n        }\n    }\n}", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/placing-content-on-detected-planes", "title": "Placing content on detected planes | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space  / visionOS / Placing content on detected planes API Changes: None Flat surfaces are an ideal place to position content in an app that uses a Full Space in visionOS. They provide a place for virtual 3D content to live alongside a person\u2019s surroundings. Use plane detection in ARKit to detect these kinds of surfaces and filter the available planes based on criteria your app might need, such as the size of the plane, its proximity to someone, or a required plane orientation. If you don\u2019t need a specific plane in your app and you\u2019re rendering your app\u2019s 3D content in RealityKit, you can use an AnchorEntity instead. This approach lets you attach 3D content to a plane without prompting the person for world-sensing permission and without any particular knowledge of where that plane is relative to the person. The following shows an anchor that you can use to attach entities to a table: Anchor entities don\u2019t let you choose a specific plane in a person\u2019s surroundings, but rather let you ask for a plane with certain characteristics. When you need more specific plane selection or real-time information about the plane\u2019s position and orientation in the world, use ARKitSession and PlaneDetectionProvider. Plane-detection information comes from an ARKitSession that\u2019s configured to use a PlaneDetectionProvider. You can choose to detect horizontal planes, vertical planes, or both. Each plane that ARKit detects comes with a classification, like PlaneAnchor.Classification.table or PlaneAnchor.Classification.floor. You can use these classifications to further refine which kinds of planes your app uses to present content. Plane detection requires ARKitSession.AuthorizationType.worldSensing authorization. The following starts a session that detects both horizontal and vertical planes, but filters out planes classified as windows: If you\u2019re displaying content that needs to appear attached to a particular plane, update your content whenever you receive new information from ARKit. When a plane is no longer available in the person\u2019s surroundings, ARKit sends a removal event. Respond to these events by removing content associated with the plane. The following shows plane updates that place a text entity on each plane in a person\u2019s surroundings; the text entity displays the kind of plane ARKit detected: Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC\n\n\n\nAnchorEntity(.plane(.horizontal, classification: .table, minimumBounds: [0.5, 0.5]))\n\n\n\nlet session = ARKitSession()\nlet planeData = PlaneDetectionProvider(alignments: [.horizontal, .vertical])\n\n\nTask {\n    try await session.run([planeData])\n    \n    for await update in planeData.anchorUpdates {\n        if update.anchor.classification == .window {\n            // Skip planes that are windows.\n            continue\n        }\n        switch update.event {\n        case .added, .updated:\n            await updatePlane(update.anchor)\n        case .removed:\n            await removePlane(update.anchor)\n        }\n        \n    }\n}\n\n\n\n@MainActor var planeAnchors: [UUID: PlaneAnchor] = [:]\n@MainActor var entityMap: [UUID: Entity] = [:]\n\n\n@MainActor\nfunc updatePlane(_ anchor: PlaneAnchor) {\n    if planeAnchors[anchor.id] == nil {\n        // Add a new entity to represent this plane.\n        let entity = ModelEntity(mesh: .generateText(anchor.classification.description))\n        entityMap[anchor.id] = entity\n        rootEntity.addChild(entity)\n    }\n    \n    entityMap[anchor.id]?.transform = Transform(matrix: anchor.originFromAnchorTransform)\n}\n\n\n@MainActor\nfunc removePlane(_ anchor: PlaneAnchor) {\n    entityMap[anchor.id]?.removeFromParent()\n    entityMap.removeValue(forKey: anchor.id)\n    planeAnchors.removeValue(forKey: anchor.id)\n}", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/tracking-points-in-world-space", "title": "Tracking specific points in world space | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space  / visionOS / Tracking specific points in world space API Changes: None Use world anchors along with an ARKit session\u2019s WorldTrackingProvider to track points of interest in the world over time, as a person moves while wearing the device, and across device usage sessions. For example, someone might place a 3D object in a specific position on their desk and expect it to come back the next time they use the device. ARKit keeps track of a unique identifier for each world anchor your app creates and automatically places those anchors back in the space when the person returns to your app in the same location. A world tracking provider also provides the position of the device the person is wearing. Use an ARKitSession configured for world tracking to start receiving updates on the world anchors your app places. The following shows updates to world anchors your app previously registered using the addAnchor(_:) method: Important If a person repositions the current space \u2014 for example, by holding down the Digital Crown \u2014 world anchor updates begin updating their position relative to the new world origin. For example, a world anchor placed on a table still reports information about the table\u2019s position, but those positions are relative to the updated world origin. You can create world anchors for any point of interest in your app\u2019s world coordinate system once you\u2019ve started a world tracking ARKit session. For example, you might track that a person placed an item at a particular offset from a desk in their space: Once you add a world anchor to your app\u2019s tracking provider using the addAnchor(_:) method, the anchorUpdates sequence in the current session and future runs of your app provides updates to the current position of that new world anchor. The only information ARKit persists about the world anchors in your app is their UUID \u2014 a WorldAnchor instance\u2019s id property \u2014 and pose in a particular space. It\u2019s your app\u2019s responsibility to persist additional information, such as the meaning of each anchor. For example, you might save local data about a custom 3D lamp model that a person placed on their desk. As a person moves from town-to-town or room-to-room, your app won\u2019t receive all of the world anchor updates from each place someone used your app. Instead, the anchorUpdates sequence only provides world anchors for nearby objects. Use the Compositor Services framework and the WorldTrackingProvider class\u2019s queryDeviceAnchor(atTimestamp:) method to get low-latency information about the current and future-predicted pose of the person\u2019s device in world space. For more information, see Drawing fully immersive content using Metal. Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC\n\n\n\nlet session = ARKitSession()\nlet worldInfo = WorldTrackingProvider()\n\n\nTask {\n    try await session.run([worldInfo])\n    \n    for await update in worldInfo.anchorUpdates {\n        switch update.event {\n        case .added, .updated:\n            // Update the app's understanding of this world anchor.\n            print(\"Anchor position updated.\")\n        case .removed:\n            // Remove content related to this anchor.\n            print(\"Anchor position now unknown.\")\n    }\n}\n\n\n\nlet anchor = WorldAnchor(originFromAnchorTransform: deskPlane.originFromAnchorTransform + offset)\ntry await worldInfo.addAnchor(anchor)", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/tracking-images-in-3d-space", "title": "Tracking preregistered images in 3D space | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space  / visionOS / Tracking preregistered images in 3D space API Changes: None Use ARKit\u2019s support for tracking 2D images to place 3D content in a space. ARKit provides updates to the image\u2019s location as it moves relative to the person. If you supply one or more reference images in your app\u2019s asset catalog, people can use a real-world copy of that image to place virtual 3D content in your app. For example, if you design a pack of custom playing cards and provide those assets to people in the form of a real-world deck of playing cards, they can place unique content per card in a fully immersive experience. The following example tracks a set of images loaded from an app\u2019s asset catalog: If you know the real-world dimensions of the images you\u2019re tracking, use the physicalSize property to improve tracking accuracy. The estimatedScaleFactor property provides information about how the scale of the tracked image differs from the expected physical size you provide. Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC\n\n\n\nlet session = ARKitSession()\nlet imageInfo = ImageTrackingProvider(\n    referenceImages: ReferenceImage.loadReferenceImages(inGroupNamed: \"playingcard-photos\")\n)\n\n\nif ImageTrackingProvider.isSupported {\n    Task {\n        try await session.run([imageInfo])\n        for await update in imageInfo.anchorUpdates {\n            updateImage(update.anchor)\n        }\n    }\n}\n\n\nfunc updateImage(_ anchor: ImageAnchor) {\n    if imageAnchors[anchor.id] == nil {\n        // Add a new entity to represent this image.\n        let entity = ModelEntity(mesh: .generateSphere(radius: 0.05))\n        entityMap[anchor.id] = entity\n        rootEntity.addChild(entity)\n    }\n    \n    if anchor.isTracked {\n        entityMap[anchor.id]?.transform = Transform(matrix: anchor.originFromAnchorTransform)\n    }\n}", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos#video-playback", "title": "visionOS | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space  / visionOS API Changes: None visionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.  Developing for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app\u2019s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS. Start with a familiar window-based experience to introduce people to your content. From there, add SwiftUI scene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences. Build your app\u2019s 3D content with RealityKit and Reality Composer Pro, and display it with a RealityView. In an immersive experience, use ARKit to integrate your content with the person\u2019s surroundings.  People can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures. SwiftUI provides built-in support for these standard gestures, so rely on them for most of your app\u2019s input. When you want to go beyond the standard gestures, use ARKit to create custom gestures. Tap to select Pinch to rotate Manipulate objects Create custom gestures Explore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash. visionOS Overview Expand your app into immersive spaces Explore new kinds of interaction Dive into featured sample apps Topics Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/destination-video", "title": "Destination Video | Apple Developer Documentation", "content": "Tracking specific points in world space Placing content on detected planes Incorporating real-world surroundings in an immersive experience Setting up access to ARKit data Happy Beam ARKit Capturing screenshots and video from Apple Vision Pro for 2D viewing Designing RealityKit content with Reality Composer Pro Understanding RealityKit\u2019s modular architecture Diorama Swift Splash RealityKit and Reality Composer Pro Positioning and sizing windows Presenting windows and spaces Hello World SwiftUI Improving accessibility support in your visionOS app Adopting best practices for privacy and user preferences Designing for visionOS Design Drawing sharp layer-based content in visionOS Creating fully immersive experiences in your app Adding 3D content to your app Creating your first visionOS app App construction   Destination Video is a multiplatform video-playback app for visionOS, iOS, and tvOS. People get a familiar media-browsing experience navigating the library\u02bcs content and playing videos they find interesting. The app provides a similar experience on supported platforms, but leverages unique features of visionOS to create a novel, immersive playback experience. When you select a video in the library, Destination Video presents a view that displays additional details about the item. The view presents controls to play the video and specify whether to include it in your Up Next list. In visionOS, it also displays a video poster along its leading edge. Tapping the view\u2019s Preview button displays an inline preview of the video. When you present an AVPlayerViewController object\u2019s interface as a child of another view, inline controls display, for example, pause, skip, and seek. Showing standard playback controls in your app provides a familiar UI that automatically adapts its appearance to fit each platform, and is the recommended choice in most cases. Destination Video uses a simple UI for the inline player view: a single button that toggles state of playback. AVPlayerViewController doesn\u2019t provide this controls style, but the app uses it to display the video content without controls by setting the value of its showsPlaybackControls property to false. It then overlays the custom playback controls it requires. See Destination Video\u2019s InlinePlayerView type for details on how you can implement this. Note AVPlayerViewController only supports displaying 2D content when embedded inline. One of the most exciting features of visionOS is its ability to play 3D video along with Spatial Audio, which adds a deeper level of immersion to the viewing experience. Playing 3D content in your app requires that you display AVPlayerViewController full window. When you present the player this way, the system automatically docks it into the ideal viewing position, and presents streamlined playback controls that keep the person\u2019s focus on the content. Note In iOS or tvOS, you typically present video in a full-screen presentation using the fullScreenCover(isPresented:onDismiss:content:) modifier. This API is available in visionOS; however, the recommended way to present the player for full-window playback is to set it as the root view of your app\u2019s window group. Destination Video\u2019s ContentView displays the app\u2019s library by default. It observes changes to the player model\u2019s presentation property, which indicates whether the app requests inline or full-window playback. When the presentation state changes to fullWindow, the view redraws the UI to display the player view in place of the library. When someone selects the Play Video button on the detail view, the app calls the player model\u2019s loadVideo(_: presentation:) method requesting the fullWindow presentation option. After the player model successfully loads the video content for playback, it updates its presentation value to fullWindow, which causes the app to replace the library with PlayerView. To dismiss the full-window player in visionOS, people tap the Back button in the player UI. To handle this action, the app\u2019s PlayerViewControllerDelegate type defines an AVPlayerViewControllerDelegate object that handles the dismissal. When the delegate receives this call, it clears the media from the player model and resets the presentation state back to its default value, which results in the Destination Video app redisplaying the library view. Media playback apps require common configuration of their capabilities and audio session. In addition to performing the steps outlined in Configuring your app for media playback, Destination Video also adopts new AVAudioSession API to customize a person\u2019s Spatial Audio experience. After the app successfully loads a video for playback, it configures the Spatial Audio experience for the current presentation. For the inline player view, it sets the experience to a small, focused sound stage where the audio originates from the location of the view. When displaying a video full window, it sets the experience to a large, fully immersive sound stage. Building video playback apps for visionOS provides new opportunities to enhance the viewing experience beyond the bounds of the player window. To add a greater level of immersion, the sample presents an immersive space that displays a scene around a person as they watch the video. It defines the immersive space in the DestinationVideo app structure. The immersive space presents an instance of DestinationView, which maps a texture to the inside of a sphere that it displays around a person. The app presents it using the .progressive immersion style, which lets someone change their amount of immersion by turning the Digital Crown on the device. The Destination Video app automatically presents the immersive space when a person navigates to a video\u2019s detail view, and dismisses it when they return to the library. To monitor these events, the app observes its navigation path to determine when a navigation event occurs so it can show or dismiss the space. One of the best ways to enhance your app\u2019s playback experience is to make that experience shareable with others. You can use the AVFoundation and the Group Activities frameworks to build SharePlay experiences that bring people together even when they can\u2019t be in the same location. The Destination Video app creates an experience where people can watch videos with others across devices and platforms. It defines a group activity called VideoWatchingActivity that adopts the GroupActivity protocol.  When people have a FaceTime call active and they play a video in the full-window player, it becomes eligible for playback for everyone on the call. The app\u2019s VideoWatchingCoordinator actor manages Destination Video\u2019s SharePlay functionality. It observes the activation of new VideoWatchingActivity sessions and when one starts, it sets the GroupSession instance on the player object\u2019s AVPlaybackCoordinator. With the player configured to use the group session, when the app loads new videos, they become eligible to share with people in the FaceTime call.\n\n\n\nstruct ContentView: View {\n    \n    /// The library's selection path.\n    @State private var navigationPath = [Video]()\n    /// A Boolean value that indicates whether the app is currently presenting an immersive space.\n    @State private var isPresentingSpace = false\n    /// The app's player model.\n    @Environment(PlayerModel.self) private var player\n    \n    var body: some View {\n        switch player.presentation {\n        case .fullWindow:\n            // Present the player full window and begin playback.\n            PlayerView()\n                .onAppear {\n                    player.play()\n                }\n        default:\n            // Show the app's content library by default.\n            LibraryView(path: $navigationPath, isPresentingSpace: $isPresentingSpace)\n        }\n    }\n}\n\n\n\n\nButton {\n    /// Load the media item for full-window presentation.\n    player.loadVideo(video, presentation: .fullWindow)\n} label: {\n    Label(\"Play Video\", systemImage: \"play.fill\")\n}\n\n\n\n\nfunc playerViewController(_ playerViewController: AVPlayerViewController,\n                          willEndFullScreenPresentationWithAnimationCoordinator coordinator: UIViewControllerTransitionCoordinator) {\n    // Reset the player model's state.\n    player.reset()\n}\n\n\n\n\n/// Configures a person's intended Spatial Audio experience to best fit the presentation.\n/// - Parameter presentation: the requested player presentation.\nprivate func configureAudioExperience(for presentation: Presentation) {\n    #if os(xrOS)\n    do {\n        let experience: AVAudioSessionSpatialExperience\n        switch presentation {\n        case .inline:\n            // Set a small, focused sound stage when watching trailers.\n            experience = .headTracked(soundStageSize: .small, anchoringStrategy: .automatic)\n        case .fullWindow:\n            // Set a large sound stage size when viewing full window.\n            experience = .headTracked(soundStageSize: .large, anchoringStrategy: .automatic)\n        }\n        try AVAudioSession.sharedInstance().setIntendedSpatialExperience(experience)\n    } catch {\n        logger.error(\"Unable to set the intended spatial experience. \\(error.localizedDescription)\")\n    }\n    #endif\n}\n\n\n\n\nstruct DestinationVideo: App {\n    \n    var body: some Scene {\n        // The app's primary window.\n        WindowGroup {\n            ContentView()\n        }\n\n\n        // Defines an immersive space to present a destination in which to watch the video.\n        ImmersiveSpace(for: Destination.self) { $destination in\n            if let destination {\n                DestinationView(destination)\n            }\n        }\n        // Set the immersion style to progressive, so the person can use the Digital Crown to dial in their experience.\n        .immersionStyle(selection: .constant(.progressive), in: .progressive)\n    }\n}\n\n\n\n\n.onChange(of: navigationPath) {\n    Task {\n        // The selection path becomes empty when the person returns to the main library window.\n        if navigationPath.isEmpty {\n            if isSpaceOpen {\n                // Dismiss the space and return the person to their real-world space.\n                await dismissSpace()\n                isSpaceOpen = false\n            }\n        } else {\n            // The navigationPath has one video, or is empty.\n            guard let video = navigationPath.first else { fatalError() }\n            // Await the request to open the destination and set the state accordingly.\n            switch await openSpace(value: video.destination) {\n            case .opened: isSpaceOpen = true\n            default: isSpaceOpen = false\n            }\n        }\n    }\n}\n\n\n\n\nprivate var groupSession: GroupSession<VideoWatchingActivity>? {\n    didSet {\n        guard let groupSession else { return }\n        // Set the group session on the AVPlayer object's playback coordinator\n        // so it can synchronize playback with other devices.\n        playbackCoordinator.coordinateWithSession(groupSession)\n    }\n}\n", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos#xcode-and-instruments", "title": "visionOS | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode Tracking specific points in world space Placing content on detected planes Incorporating real-world surroundings in an immersive experience Setting up access to ARKit data Happy Beam ARKit Capturing screenshots and video from Apple Vision Pro for 2D viewing Designing RealityKit content with Reality Composer Pro Understanding RealityKit\u2019s modular architecture Diorama Swift Splash RealityKit and Reality Composer Pro Positioning and sizing windows Presenting windows and spaces Hello World SwiftUI Improving accessibility support in your visionOS app Adopting best practices for privacy and user preferences Designing for visionOS Design Drawing sharp layer-based content in visionOS Creating fully immersive experiences in your app Adding 3D content to your app Creating your first visionOS app App construction  / visionOS API Changes: None visionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.  Developing for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app\u2019s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS. Start with a familiar window-based experience to introduce people to your content. From there, add SwiftUI scene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences. Build your app\u2019s 3D content with RealityKit and Reality Composer Pro, and display it with a RealityView. In an immersive experience, use ARKit to integrate your content with the person\u2019s surroundings.  People can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures. SwiftUI provides built-in support for these standard gestures, so rely on them for most of your app\u2019s input. When you want to go beyond the standard gestures, use ARKit to create custom gestures. Tap to select Pinch to rotate Manipulate objects Create custom gestures Explore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash. visionOS Overview Expand your app into immersive spaces Explore new kinds of interaction Dive into featured sample apps Topics Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/creating-a-performance-plan-for-visionos-app", "title": "Creating a performance plan for your visionOS app | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space  / visionOS / Creating a performance plan for your visionOS app API Changes: None Performance tuning is an important part of the development process, regardless of platform. Performance tuning means making your app run as efficiently as possible, so it does more work in less time and with fewer system resources. Efficiency is especially important on devices that can support multiple apps in an immersive experience. Apps that consume too many resources, can push the device beyond thermal limits. When this occurs, the system takes steps to cool down to a more acceptable level. This can have a noticeable visual impact and be disorienting for the wearer. As you start development, set aggressive goals and evaluate progress throughout the development cycle. Automate the collection of performance metrics as much as possible and look at data over time to see if performance is improving or declining. When you detect a significant decrease in performance, take immediate steps to correct it. When you start fine-tuning early in development, you have more time to make needed changes to algorithms and approaches. For more information on performance tuning, see Improving your app\u2019s performance. Performance isn\u2019t a single metric that you measure and improve. Typically, you choose several metrics and set goals for each of them. For example, consider: Make sure your app launches quickly; this is your first chance to make a good impression. Your interface needs to respond quickly to interactions, even while doing other work. Minimize the time it takes to start tasks. For example, make sure audio and video start without noticeable delays. For an immersive experience with realtime rendering, it\u2019s important to maintain consistently high frame rates. Help maintain these rates by avoiding unnecessary changes that result in more frequent updates to the shared render server. Measure things like update rates, stalls, and hangs in both the render server and your app. Only render the content you need, and optimize the textures and other resources you use during drawing. When the device begins to reach thermal limits, the system reduces CPU or GPU usage and performance degrades over time. Avoid this thermal ceiling by prioritizing and spreading out work, limiting the number of simultaneous threads your app maintains, and turning off hardware-related features like Core Location when you don\u2019t need them. Make the app do as much as possible using the smallest amount of hardware resources. Minimize task-based overhead. Use as little free memory as possible. Don\u2019t allocate or deallocate memory during critical operations, which might make your app appear slow. After you choose the metrics you want, set realistic goals and prioritize them, so you know which ones matter the most. Performance tuning often involves making tradeoffs between competing goals. For example, if you reduce CPU usage by caching computed data or pre-load assets to improve responsiveness, you increase your app\u2019s memory usage. Make these kinds of tradeoffs carefully, and always measure the results of any changes to learn whether they were successful. In some cases, you might find the sacrifice isn\u2019t worthwhile. Consider how people will use your app. If your app runs in the Shared Space, consider more conservative targets and goals for system resources. If you expect people to use your app for longer periods of time, factor this extended use into your targets and goals when choosing metrics. After you choose the metrics to collect, decide which portions of your app to test. Choose features that are repeatable, measurable, and reliable to test. Repeatable automated tests allow you to compare the results and know the comparisons represent the exact same task. Focus on places where your app executes code, but don\u2019t ignore places where your app hands off data to the system and waits. If your app spends a significant amount of time waiting for information, consider eliminating the requests altogether or batching them to achieve better performance. Focus your tuning efforts on the parts of your app that people use the most, or that have the most impact on overall system performance, including: User-facing workflows User-facing workflows Key algorithms Key algorithms Task that allocate or deallocate memory Task that allocate or deallocate memory Background and network-based tasks Background and network-based tasks Custom Metal shaders Custom Metal shaders Choose actions that people perform frequently or that correspond to important features. For example, if your app lets someone add a new contact, test the workflow for creating the contact, editing the contact, and saving the results. Test your app with a particular feature enabled and disabled to determine whether the feature is solely responsible for any performance impacts. Choose lightweight workflows such as how your app performs at idle time, and also heavyweight workflows, for example, ones that involve user interactions and your app\u2019s responses. For launch times, gather metrics for both hot and cold launches \u2014 that is, when the app is already resident in memory and when it is not. Consider how environmental factors impact your app. The characteristics of your physical environment can affect system load and thermals of the device. Consider the effect that ambient room temperature, the presence of other people, and the number and type of real-world objects can have on the your app\u2018s algorithms. Try to test in different settings to get an idea of whether you need to optimize for these scenarios or not. Use Xcode\u2019s thermal inducers to mimic the device hitting its thermal limits and consider how your app responds to fair, serious, and critical thermal notifications. You might need to have different performance goals when under thermal pressure, and prioritize optimizing for power or find ways to dynamically lower your app\u2018s complexity in response to thermal pressure to give a smoother experience, even if latency is a bit higher. There are many tools and APIs you can use to collect performance-related data for your visionOS app. Use a variety of tools to make sure you have the data you need: Monitor the CPU, memory, disk and network gauges in the Debug navigator to track system resources utilization. Profile your app to gather performance data on most metrics. Instruments lets you profile your app\u2019s code execution, find memory leaks, track memory allocations, analyze file-system or graphics performance, SwiftUI performance, and much more. Use the RealityKit Trace template to monitoring and investigate render server stalls and bottlenecks on visionOS. Use XCTest APIs to collect performance data. Use MetricKit to gather on-device app diagnostics and generate reports. Review diagnostic logs for hangs, disk and energy usage, and crashes in the Xcode Organizer. Review statistics on the contents of your RealityKit scenes. Use this information to optimize your 3D models and textures. Add signposts to your code to generate timing information you can view in Instruments. For more information, see Recording performance data. Include log messages to report significant events and relevant data for those events. For more information, see Generating log messages from your code. Get feedback from testers about their experiences with beta versions of your app. Fill out the Test Information page for your beta version, and request that testers provide feedback about the performance of your app. In general, profile and analyze performance on a physical device rather than in Simulator. Even if something works well in Simulator, it might not perform as well on devices for all use cases. Simulator doesn\u2019t support some hardware features and APIs. There are differences in the rendering pipeline for Simulator running on macOS, so rendering performance characteristics will be different. Other pipelines such as input delivery and audio or video playback are also different. There are, however, some insights you can gain profiling in Simulator, such as CPU stalls, that help you spot areas to investigate and address. Xcode comes with tools to help you automate the collection of performance data: Use the XCTest framework to build test cases to collect performance metrics. XCTest lets you gather several different metrics, including the time it takes to perform operations, the amount of CPU activity that occurs during the test, details about memory or storage use, and more. Use the XCTest framework to build test cases to collect performance metrics. XCTest lets you gather several different metrics, including the time it takes to perform operations, the amount of CPU activity that occurs during the test, details about memory or storage use, and more. Use Instruments to collect metrics for specific interactions with your app. Record those interactions and play them back later to collect a new set of metrics. Use Instruments to collect metrics for specific interactions with your app. Record those interactions and play them back later to collect a new set of metrics. Write custom scripts to gather performance-related data using system command-line tools. Integrate these scripts into your project\u2019s build process to automate their execution. Write custom scripts to gather performance-related data using system command-line tools. Integrate these scripts into your project\u2019s build process to automate their execution. Configure Xcode to run test cases each time you build your app, or create a separate target to run test cases or custom scripts on demand. Integrate your performance tests into your Xcode Cloud workflows, or your own custom continuous integration solution. Note Collect performance data using a production version of your app to obtain more accurate results. Debug builds contain additional code to support debugging operations and logging. You can collect data from debug builds too, but keep those metrics separate from production-build metrics. For information about how to write test cases for your app, see Testing your apps in Xcode. For information about how to automate testing with Xcode Cloud, see Xcode Cloud. Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/analyzing-the-performance-of-your-visionos-app", "title": "Analyzing the performance of your visionOS app | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode Tracking specific points in world space Placing content on detected planes Incorporating real-world surroundings in an immersive experience Setting up access to ARKit data Happy Beam ARKit Capturing screenshots and video from Apple Vision Pro for 2D viewing Designing RealityKit content with Reality Composer Pro Understanding RealityKit\u2019s modular architecture Diorama Swift Splash RealityKit and Reality Composer Pro Positioning and sizing windows Presenting windows and spaces Hello World SwiftUI Improving accessibility support in your visionOS app Adopting best practices for privacy and user preferences Designing for visionOS Design Drawing sharp layer-based content in visionOS Creating fully immersive experiences in your app Adding 3D content to your app Creating your first visionOS app App construction  / visionOS / Analyzing the performance of your visionOS app API Changes: None To maintain the sense of immersion on Apple Vision Pro, the system attempts to provide the device displays with up-to-date imagery at a constant rate and respond to interactions with minimum latency. Any visual choppiness or delay in responsiveness interferes with the spatial experience. Higher power consumption over extended periods of time, or extreme power consumption over shorter periods of time, can trigger thermal mitigations that also impact the quality of the experience. It\u2019s important to minimize your app\u2019s use of system resources to ensure your app performs well on the platform. Many of the same best practices and optimization procedures you use developing for other Apple platforms apply when developing for visionOS as well. For more information about optimizing your app on other platforms, see Improving your app\u2019s performance. To get useful information specific to rendering bottlenecks, high system power use, and other issues that effect the responsiveness of your visionOS app, profile your app with the RealityKit Trace template in Instruments. This template helps you identify: Complex content or content with frequent updates that cause the render server to miss deadlines and drop frames. Complex content or content with frequent updates that cause the render server to miss deadlines and drop frames. Content and tasks that result in high system power use. Content and tasks that result in high system power use. Long running tasks on the the main thread that interfere with efficient processing of input events. Long running tasks on the the main thread that interfere with efficient processing of input events. Tasks running on other threads that don\u2019t complete in time to sync back to the main thread for view hierarchy updates. Tasks running on other threads that don\u2019t complete in time to sync back to the main thread for view hierarchy updates. Note You can profile using a real device or a simulator, but to get the most accurate and actionable information, use a real device. Software and hardware differences between a simulator on your Mac and a real device prevent you from relying on timing information. Simulated devices are useful for quick iteration and improving performance aspects that aren\u2019t based on time. To create a new trace document: Select your app\u2019s scheme and a visionOS run destination from the Xcode project window. Select your app\u2019s scheme and a visionOS run destination from the Xcode project window. Choose Product > Profile. Choose Product > Profile. Choose RealityKit Trace template Choose RealityKit Trace template Select the Choose button. Select the Choose button.  Alternatively, launch Instruments and choose a target app from the template selection dialog. The RealityKit Trace template includes the following instruments: Captures frame render times and lifespans for frames the visionOS render server generates. This instrument indicates when frames miss rendering deadlines and provides average CPU and GPU render rates. Captures comprehensive timing information from the entire render pipeline including rendering, commits, animations, physics, and spatial systems. This instrument identifies potential bottlenecks in your app\u2019s process or in the render server as a result of your app\u2019s content and indicates areas of moderate and high system power usage that require optimization. Captures and displays Runloop execution details. Profiles running threads on all cores at regular intervals for all processes. Captures and displays periods of time when the main thread is unresponsive. Records Metal app events. Consider adding other instruments to your trace for specific investigations. For example, you can use the Thermal State instrument to record device thermal states to check if thermal pressures are throttling performance. Click the record button at the top left of the window to start capturing profile data. Perform the actions in your app that you want to investigate. When you complete the actions, click the record button again to stop recording. To investigate performance issues or analyze system power impact, profile your app in isolation to understand your app\u2019s impact on system performance and ensure you get the most actionable information. For apps that run alongside other apps, profile your app again with those other apps running to understand how people experience your app in conjunction with other apps. To maintain a smooth visual experience, the system tries to render new frames for the Apple Vision Pro at 90 frames per second (FPS). The system renders at other frame rates depending on the content it displays and the current surroundings. Each frame has a deadline for rendering based on the target frame rate. Not meeting these deadlines results in dropped frames. This creates a poor spatial experience overall. People tend to notice it in the visual performance of Persona and SharePlay experiences, video playback, and scrolling. The RealityKit Frames instrument displays the time spent rendering each frame in the Frames section of its timeline:  When you zoom out, you can identify areas with a high number of frame drops or with frames running close to the rendering deadline. The timeline uses green to identify frames that complete rendering before the deadline, orange for frames that complete rendering close to the deadline, and red for frames that don\u2019t complete rendering that the renderer drops. Dropped frames contribute to a poor spatial experience, but frames that complete close to their rendering deadline indicate performance problems too. Hold the Option key and drag to zoom into a frame, or group of frames, to see their lifespan broken down in stages:  This provides you with insight into which portion of the rendering pipeline to investigate further. This timeline also includes sections that visualize the Average CPU Frame Time and Average GPU Frame Time to indicate the type of processing that computes the frames. A region of the timeline without a frame block indicates a period of time without changes to a person\u2019s surroundings or app updates. The render server avoids computing new frames to send to the compositor during these periods which helps optimize power use. When thermal levels rise to levels that trigger thermal mitigations in the system, performance degrades and negatively impacts the responsiveness of your app. Optimize for power to avoid this negative impact. The timeline for the RealityKit Metrics instrument includes a System Power Impact section to identify areas of high power usage in your app:  If the timeline displays green, the tool considers your app\u2019s impact on system power low enough to sustain. Regions that display orange or red indicate the system power usage could cause thermal levels to rise and trigger thermal mitigations. This decreases the availability of system resources, which can cause visual interruptions and responsiveness issues. Note If the render server can\u2019t maintain the target frame rate of 90 FPS due to thermal pressure, it might reduce its frame rate in half. When this occurs, all frames in the frames track show up as missing their rendering deadlines. Other factors can cause reduced frame rate, including the complexity and frequency of the content the system is processing. Use the Thermal State instrument to determine if thermal conditions are causing the rate limiting or if it\u2019s due to other factors. The Bottlenecks section of the timeline for the RealityKit Metrics instrument contains markers that indicate high overhead in your app or the render server that contribute to dropped frames and high system power use. When you encounter either of these issues, check if the timeline identifies bottlenecks you can address. Double-click on any of the markers to display more information in the detail area at the bottom of the instruments window. If the detail area is hidden, choose View > Detail Area > Show Detail Area to reveal it. The render server encounters bottlenecks in either the CPU or GPU. The instrument categorizes bottlenecks by their severity and type. To filter the bottlenecks listed in the detail area to a particular time period, drag inside the timeline to select the region. To see an outline view of the bottlenecks organized by severity and type, select Summary: RealityKit Bottlenecks from the menu at the top left of the detail area. Click the arrow button to the right of the severity or type in the outline view to show the list of bottlenecks in that category. When you select a specific bottleneck, the extended detail provides recommendations for you to address the bottleneck \u2013 choose View > Show Extended Detail to reveal the extended detail if it\u2019s hidden.  The trace provides additional information you can use to identify changes to make in your app to address these bottlenecks. Click the expansion arrow for the RealityKit Metrics instrument timeline to reveal graphs specific to each major category of work. Use the metrics associated with these graphs to determine which RealityKit feature has the biggest impact on high CPU frame times in the app process or in the render server. When interpreting these graphs, lower indicates better performance and power. The metrics represent values from all apps running, so profile with just your app running when trying to optimize for these metrics.  Metrics related to the cost of 3D RealityKit rendering in the render server. This includes the number of draw calls, triangles, and vertices from all apps. Metrics related to UI content rendering costs in the render server. This includes the total number of render passes, offscreen render passes, and translucent UI meshes from all apps. Metrics related to the costs of entity commits in the app and the render server. This includes the number of RealityKit entities shared with the render server from all apps, as well as the number of updates received from all apps over certain intervals. Metrics related to the cost of RealityKit animations in the app and the render server. This includes the number of skeletal animations, across all apps. Metrics related to the cost of RealityKit physics simulations, collisions, and hit testing in the app process and render server. This includes the number of rigid body counts and colliders in use, as well as the type of physics shapes that the UI and other 3D content use, across all apps. Metrics related to the costs of spatial algorithms in the render server. This includes the number of custom anchors, across all apps. Tip The graphs for some sections combine several individual metrics. The heading indicates this by displaying a graph count. Click on the bottom of the timeline\u2019s heading and drag down to display individual graphs for each metric. For example, the 3D Render Timeline might display 13 Graphs in the heading; expanding that timeline exposes individual graphs for 3D Mesh Draw Calls, 3D Mesh Triangles, 3D Mesh Vertices, and the 10 additional metrics. The timeline for your app\u2019s process helps summarize information from the instruments about your process and the work the render server completes for your process.  Choose an option from the pop-up in the timeline header to show different graphs in the timeline: Time each thread spends waiting or busy. Time the main thread is unresponsive. CPU usage and lifecycle status. Overhead attributed to RealityKit systems. When you select the timeline for your app\u2019s process, you can choose instrument summaries and profile data to display in the detail area from the popup-button at its top-left:  To filter the information in the detail area by time, select periods of time in the timeline  above. Select Hangs in your app\u2019s process timeline to identify times in the trace that might have interaction delays. Use the RealityKit Metrics and Time Profiler summaries to better understand the work your app is doing. Choose the following options from the detail area pop-up menu: Shows information from the Time Profiler instrument to determine what your app is doing during a hang. RealityKit System CPU times: Shows minimum, maximum, and average times the CPU spends on various RealityKit system operations. Optimize any 3D render updates, hit testing, and collision work you find. For more information about addressing hangs in your app, see Improving app responsiveness. Use the Audio Playback section of your process\u2019s timeline to identify areas of high audio overhead. The system defaults to using spatial audio for your app when running on visionOS. It processes information in real time about your position, surroundings, and the current location of audio sources to generate an immersive audio experience. If you include too many concurrent audio sources that require the system to adapt audio sources to their location within a large space, the increased demand on system resources can lead to delays in the audio output. To reduce the spatial audio work, limit: The number of concurrently playing audio sources The number of concurrently playing audio sources The number of moving audio sources The number of moving audio sources The size of the soundstage The size of the soundstage Consider creating a pool of audio players to limit the maximum number of players your app uses. Place players on stationary entities, instead of moving entities, when appropriate. Initializing several audio players at the same time causes a high overhead that affects other aspects of the system, such as rendering performance. Consider the other tasks the system completes during these allocations and space them out over time. For more information, see Create a great spatial playback experience. Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos#simulator", "title": "visionOS | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space  / visionOS API Changes: None visionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.  Developing for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app\u2019s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS. Start with a familiar window-based experience to introduce people to your content. From there, add SwiftUI scene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences. Build your app\u2019s 3D content with RealityKit and Reality Composer Pro, and display it with a RealityView. In an immersive experience, use ARKit to integrate your content with the person\u2019s surroundings.  People can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures. SwiftUI provides built-in support for these standard gestures, so rely on them for most of your app\u2019s input. When you want to go beyond the standard gestures, use ARKit to create custom gestures. Tap to select Pinch to rotate Manipulate objects Create custom gestures Explore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash. visionOS Overview Expand your app into immersive spaces Explore new kinds of interaction Dive into featured sample apps Topics Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/interacting-with-your-app-in-the-visionos-simulator", "title": "Interacting with your app in the visionOS simulator | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space  / visionOS / Interacting with your app in the visionOS simulator API Changes: None Use Simulator to run apps in visionOS without installing them on a physical device. When you run your app in Simulator, you can see a monoscopic view of your app\u2019s windows and 3D content inside an immersive space. Use your Mac to alter the viewpoint of the app within the space and navigate the app\u2019s interface.  To use your Mac\u2019s pointer and keyboard to create gestures, choose \u201cSelect to interact with the scene\u201d from the buttons at the bottom-right of a visionOS simulator window. The current gaze position tracks your pointer movements when you hover over content within the space. Use the following actions to trigger gestures: Gesture To simulate Tap Click. Double-tap Double-click. Touch and hold Click and hold. Drag (left, right, up, and down) Drag left, right, up, and down. Drag (forward and back) Shift-drag up and down. Two-handed gestures Press and hold the Option key to display touch points. Move the pointer while pressing the Option key to change the distance between the touch points. Move the pointer and hold the Shift and Option keys to reposition the touch points. Activate device buttons using menu items or by clicking the controls in the simulator window toolbar. Use your Mac\u2019s pointer and the keyboard to reposition your viewpoint in a visionOS simulator window: Movement To simulate Forward Press the W key (or Up Arrow key), or perform a pinch gesture moving two fingers away from each other on a trackpad. Backward Press the S key (or Down Arrow key), or perform a pinch gesture moving two fingers toward each other on a trackpad. Left Press the A key (or Left Arrow key), or scroll left using a trackpad or Magic Mouse. Right Press the D key (or Right Arrow key), or scroll right using a trackpad or Magic Mouse. Up Press the E key, or scroll up using a trackpad or Magic Mouse. Down Press the Q key, or scroll down using a trackpad or Magic Mouse. You can also control the viewpoint with a standard drag. To do so, choose \u201cDrag to pan the camera\u201d from the buttons at the bottom-right of the simulator window to move the viewpoint left, right, up or down and choose \u201cDrag to dolly the camera\u201d to move it forward or backward. To change your viewing angle, Control-drag inside a visionOS simulator window. You can choose \u201cDrag to tilt the camera\u201d from the buttons at the bottom-right of the simulator window to use a drag without the Control key. To reset the viewpoint and viewing angle for a visionOS simulator, choose the Reset Camera button from the toolbar at the top-right of its window. Note To capture the input from the pointer and keyboard, bypassing navigation control, to direct the input to the simulated device, use the Pointer Capture and Keyboard Capture buttons in the toolbar at the top-right of a visionOS simulator window. Press the Esc (Escape) key to disable capture and restore navigation controls. When moving with a trackpad or Magic Mouse, Simulator respects the natural scrolling setting on macOS. You can also use a game controller to control your movement. Use the left stick to move left, right, forward or back. Use R2 and L2 to move up and down. Use the right stick on a game controller to pan around the space. Simulator provides multiple built-in scenes you can use to simulate passthrough in different surroundings. These include unique room layouts and furniture for different testing scenarios, each available in different lighting conditions. Use the simulated scene to test: Readability of your app in varying backgrounds and varying lighting conditions. Readability of your app in varying backgrounds and varying lighting conditions. Different scenarios, including limited and cluttered surroundings, to see how your app adapts to them. Different scenarios, including limited and cluttered surroundings, to see how your app adapts to them. Content layout, positioning, and scale. Content layout, positioning, and scale. Spatial audio and acoustics. Spatial audio and acoustics. To change the simulated scene, click the Simulated Scenes button in the toolbar at the top-right of a visionOS simulator window and choose a different scene. Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos#ios-migration-and-compatibility", "title": "visionOS | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space  / visionOS API Changes: None visionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.  Developing for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app\u2019s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS. Start with a familiar window-based experience to introduce people to your content. From there, add SwiftUI scene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences. Build your app\u2019s 3D content with RealityKit and Reality Composer Pro, and display it with a RealityView. In an immersive experience, use ARKit to integrate your content with the person\u2019s surroundings.  People can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures. SwiftUI provides built-in support for these standard gestures, so rely on them for most of your app\u2019s input. When you want to go beyond the standard gestures, use ARKit to create custom gestures. Tap to select Pinch to rotate Manipulate objects Create custom gestures Explore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash. visionOS Overview Expand your app into immersive spaces Explore new kinds of interaction Dive into featured sample apps Topics Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/bringing-your-app-to-visionos", "title": "Bringing your existing apps to visionOS | Apple Developer Documentation", "content": "App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space   If you have an existing app that runs in iPadOS or iOS, you can build that app against the visionOS SDK to run it on the platform. Apps built specifically for visionOS adopt the standard system appearance, and they look more natural on the platform. Updating your app is also an opportunity to add elements that work well on the platform, such as 3D content and immersive experiences. In most cases, all you need to do to support visionOS is update your Xcode project\u2019s settings and recompile your code. Depending on your app, you might need to make additional changes to account for features that are only found in the iOS SDK. While most of the same technologies are available on both platforms, some technologies don\u2019t make sense or require hardware that isn\u2019t present on visionOS devices. For example, people don\u2019t typically use a headset to make contactless payments, so apps that that use the ProximityReader framework must disable those features when running in visionOS. Note If you use ARKit in your iOS app to create an augmented reality experience, you need to make additional changes to support ARKit in visionOS. For information on how to update this type of app, see Bringing your ARKit app to visionOS. The first step to updating your app is to add visionOS as a supported destination. In your project\u2019s settings, select your app target and navigate to the General tab. In Supported Destinations, click the Add (+) button to add a new destination and select the Apple Vision option. Adding this option lets you build your app specifically for the visionOS SDK.  When you add Apple Vision as a destination, Xcode makes some one-time changes to your project\u2019s build settings. After you add the destination, you can modify your project\u2019s build settings and build phases to customize the build behavior specifically for visionOS. For example, you might remove dependencies for the visionOS version of your app, or change the set of source files you want to compile. For more information about how to update a target\u2019s configuration, see Customizing the build phases of a target. Fix any deprecation warnings in the iOS version of your code before you build for visionOS. Apple marks APIs as deprecated when they are no longer relevant or a suitable replacement exists. When you compile code that calls deprecated APIs, the compiler generates warnings and often suggests replacements for you to use instead. visionOS removed many deprecated symbols entirely, turning these deprecation warnings into missing-symbol errors on the platform. Make changes in the iOS version of your app to see the original deprecation warning and replacement details. In addition to individual symbols, the following frameworks are deprecated in their entirety in both iOS and visionOS. If your app still uses these frameworks, stop using them immediately. The reference documentation for each framework includes information about how to update your code. Accounts Address Book Address Book UI Assets Library GLKit iAd Newsstand Kit NotificationCenter OpenGL ES The iOS SDK includes many frameworks that don\u2019t apply to visionOS, either because they use hardware that isn\u2019t available or their features don\u2019t apply to the platform. Move code that uses these frameworks to separate source files whenever possible, and include those files only in the iOS version of your app. When you can\u2019t isolate the code to separate source files, use conditional statements such as the ones below to offer a different code path for visionOS and iOS. The following example shows how to configure conditional statements to execute separate code paths in visionOS and iOS: The following frameworks are available in the iOS SDK but not in the visionOS SDK.    ActivityKit AdSupport AppClip AutomatedDeviceEnrollment BusinessChat CarKey CarPlay Cinematic ClockKit CoreLocationUI CoreMediaIO CoreNFC CoreTelephony DeviceActivity DockKit ExposureNotification FamilyControls FinanceKit FinanceKitUI ManagedSettings ManagedSettingsUI Messages MLCompute NearbyInteraction OpenAL ProximityReader RoomPlan SafetyKit ScreenTime SensorKit ServiceManagement Social Twitter WidgetKit WorkoutKit  Some frameworks have behavioral changes that impact your app in visionOS, and some frameworks disable features when the required hardware is unavailable. To help you avoid using APIs for missing features, many frameworks offer APIs to check the availability of those features. Continue to use those APIs and take appropriate actions when the features aren\u2019t available. In other cases, be prepared for the framework code to do nothing or to generate errors when you use it. ARKit. This framework requires you to use different APIs for iOS and visionOS. For more information, see Bringing your ARKit app to visionOS. AutomaticAssessmentConfiguration. The framework returns an error if you try to start a test in visionOS. AVFoundation. Capture interfaces aren\u2019t available in visionOS. Use availability checks to determine which services are present. CallKit. You may continue to offer Voice-over-IP (VoIP) services, but phone number verification, call-blocking, and other cellular-related services are unavailable. ClockKit. The APIs of this framework do nothing in visionOS. CoreHaptics. visionOS plays audio feedback instead of haptic feedback. CoreLocation. You can request someone\u2019s location using the standard location service, but most other services are unavailable. Use availability checks to determine which services are present. The Always authorization level is unavailable and automatically becomes When in Use authorization. CoreMotion. Barometer data is unavailable, but most other sensors are available. Use availability checks to determine which sensors you can use. HealthKit and HealthKitUI. Health data is unavailable. Use availability checks to determine when information is available. MapKit. User-tracking features that involve heading information aren\u2019t available. MediaPlayer. Some APIs are unavailable in visionOS. MetricKit. You can gather on-device diagnostic logs and generate reports, but you can\u2019t gather metrics. MusicKit. Some APIs are unavailable in visionOS. NearbyInteraction. The framework does nothing in visionOS. Use availability checks to determine when services are present. PushToTalk. Push to Talk services are unavailable. Check for errors when creating a PTChannelManager. SafariServices. A link that presents a SFSafariViewController now opens a new scene in the Safari app. UIKit. The system reports a maximum of two simultaneous touch inputs \u2014\u00a0one for each of the person\u2019s hands. All system gesture recognizers handle these inputs correctly, including for zoom and rotation gestures that require multiple fingers. If you have custom gesture recognizers that require more than two fingers, update them to support only one or two touches in visionOS. VisionKit. The DataScannerViewController APIs are unavailable, but other features are still available. WatchConnectivity. The framework supports connections only between an iPhone and Apple Watch. Use availability checks to determine when services are available. For additional information about how to isolate code to the iOS version of your app, see Running code on a specific platform or OS version. After your existing code runs correctly in visionOS, look for ways to improve the experience you offer on the platform. In visionOS, you can display content using more than just windows. Think about ways to incorporate the following elements into your interface: Depth. Many SwiftUI and UIKit views use visual effects to add depth. Look for similar ways to incorporate depth into your own custom views. 3D content. Think about where you might incorporate 3D models and shapes into your content. Use RealityKit to implement your content, and a RealityView to present that content from your app. See Adding 3D content to your app. Immersive experiences. Present a space to immerse someone in your app\u2019s content. Spaces let you place content anywhere in a person\u2019s surroundings. You can also create fully immersive experiences that display only your app\u2019s content. See Creating fully immersive experiences in your app. Interactions with someone\u2019s surroundings. Use ARKit to facilitate interactions between your content and the surroundings. For example, detect planar surfaces to use as anchor points for your content. See ARKit. If you built your interface using UIKit, you can still load iOS storyboards into your app, but you can\u2019t customize your interface for visionOS or include 3D content. To include visionOS content in your app, programmatically add your SwiftUI views using UIHostingController or UIViewRepresentable. Alternatively, migrate the relevant parts of your interface to SwiftUI. Moving your interface to SwiftUI gives you less code to maintain and makes it easier to validate that your interface does what you want. For information about mixing SwiftUI and UIKit content, see UIKit integration in SwiftUI. For guidance on how best to incorporate depth and 3D elements in your interface, see Human Interface Guidelines. Add vector-based or high-resolution images to your project specifically to support visionOS. In visionOS, people can view your app\u2019s content at different angles and different distances, so image pixels rarely line up with screen pixels. Vector-based images work best because they maintain their detail and crispness at any size. For bitmap-based images, use high-resolution images (@2x or better) to ensure your images retain detail at different sizes. For more information about designing images for your app, see Images in Human Interface Guidelines. In some cases, it might not make sense to port your app for visionOS. For example, don\u2019t port the following types of apps: Apps that act as containers for app extensions. This includes apps where the primary purpose is to deliver custom keyboard extensions, device drivers, sticker packs, SMS and MMS message filtering extensions, call directory extensions, or widgets. Movement-based apps. This includes apps that follow a person\u2019s location changes, such as apps that offer turn-by-turn directions or navigation. It also includes apps that track body movements. Selfie or photography apps. This includes apps where the primary purpose is to capture images or video from the device\u2019s cameras. If your app uses an unsupported feature but can function without it, you can still bring your app to visionOS. Remove features that aren\u2019t available and focus on bringing the rest of your content to the platform. For example, if you have an app that lets people write down notes and take pictures to include with those notes, disable the picture-taking ability in visionOS but let people add text and incorporate images they already have.\n\n\n\n#if os(visionOS)\n   // visionOS code\n#elseif os(iOS)\n   // iOS code\n#endif\n", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/bringing-your-arkit-app-to-visionos", "title": "Bringing your ARKit app to visionOS | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode Tracking specific points in world space Placing content on detected planes Incorporating real-world surroundings in an immersive experience Setting up access to ARKit data Happy Beam ARKit Capturing screenshots and video from Apple Vision Pro for 2D viewing Designing RealityKit content with Reality Composer Pro Understanding RealityKit\u2019s modular architecture Diorama Swift Splash RealityKit and Reality Composer Pro Positioning and sizing windows Presenting windows and spaces Hello World SwiftUI Improving accessibility support in your visionOS app Adopting best practices for privacy and user preferences Designing for visionOS Design Drawing sharp layer-based content in visionOS Creating fully immersive experiences in your app Adding 3D content to your app Creating your first visionOS app App construction  / visionOS / Bringing your ARKit app to visionOS API Changes: None If you use ARKit to create an augmented reality experience on iPhone or iPad, you need to rethink your use of that technology when bringing your app to visionOS. ARKit plays a crucial role in delivering your content to the display in iPadOS and iOS. In visionOS, you use ARKit only to acquire data about the person\u2019s surroundings, and you do so using a different set of APIs. In visionOS, you don\u2019t need a special view to display an augmented reality interface. Build windows with your app\u2019s content using SwiftUI or UIKit. When you display those windows, visionOS places them in the person\u2019s surroundings for you. If you want to control the placement of any 2D or 3D content in the person\u2019s surroundings, build your content using SwiftUI and RealityKit. When migrating your app to visionOS, reuse as much of your app\u2019s existing content as you can. visionOS supports most of the same technologies as iOS, so you can reuse project assets, 3D models, and most custom views. Don\u2019t reuse your app\u2019s ARKit code or any code that relies on technologies visionOS doesn\u2019t support. For general guidance on how to port apps to visionOS, see Bringing your existing apps to visionOS. To create a single app that runs in both iOS and visionOS, use technologies that are available on both platforms. While ARKit in iOS lets you create your interface using several different technologies, the preferred technologies in visionOS are SwiftUI and RealityKit. If you\u2019re not currently using RealityKit for 3D content, consider switching to it before you start adding visionOS support. If you retain code that uses older technologies in your iOS app, you might need to re-create much of that code using RealityKit when migrating to visionOS. If you use Metal to draw your app\u2019s content, you can bring your code to visionOS to create content for 2D views or to create fully immersive experiences. You can\u2019t use Metal to create 3D content that integrates with the person\u2019s surroundings. This restriction prevents apps from sampling pixels of the person\u2019s surroundings, which might contain sensitive information. For information on how to create a fully immersive experience with Metal, see Drawing fully immersive content using Metal. The recommended format for 3D assets in iOS and visionOS is USDZ. This format offers a compact single file for everything, including your models, textures, behaviors, physics, anchoring, and more. If you have assets that don\u2019t use this format, use the Reality Converter tool that comes with Xcode to convert them for your project. When building 3D scenes for visionOS, use Reality Composer Pro to create your scenes that incorporate your USDZ assets. With Reality Composer Pro, you can import your USD files and edit them in place, nondestructively. If your iOS app applies custom materials to your assets, convert those materials to shader graphs in the app. Although you can bring models and materials to your project using USDZ files, you can\u2019t bring custom shaders you wrote using Metal. Replace any custom shader code with MaterialX shaders. Many digital content creation tools support the MaterialX standard, and let you create dynamic shaders and save them with your USDZ files. Reality Composer Pro and RealityKit support MaterialX shaders, and incorporate them with your other USDZ asset content. For more information about MaterialX, see https://materialx.org. In visionOS, you manage your app\u2019s content, and the system handles the integration of that content with the person\u2019s surroundings. This approach differs from iOS, where you use a special ARKit view to blend your content and the live camera content. Bringing your interface to visionOS therefore means you need to remove this special ARKit view and focus only on your content. If you can display your app\u2019s content using SwiftUI or UIKit views, build a window with those views and present it from your visionOS app. If you use other technologies to incorporate 2D or 3D content into the person\u2019s surroundings, make the following substitutions in the visionOS version of your app. If you create your AR experience using: Update to: RealityKit and ARView RealityKit and RealityView SceneKit and ARSCNView RealityKit and RealityView SpriteKit and ARSKView RealityKit or SwiftUI A RealityView is a SwiftUI view that manages the content and animations you create using RealityKit and Reality Composer Pro. You can add a RealityView to any of your app\u2019s windows to display 2D or 3D content. You can also add the view to an ImmersiveSpace scene, which you use to integrate your RealityKit content into the person\u2019s surroundings. Note You can load iOS storyboards into a visionOS app, but you can\u2019t customize your interface for visionOS or include 3D content. If you want to share interface files between iOS and visionOS, adopt SwiftUI views or create your interface programmatically. For more information about how to use RealityView and respond to interactions with your content, see Adding 3D content to your app. ARKit provides different APIs for iOS and visionOS, and the way you use ARKit services on the platforms is also different. In iOS, you must use ARKit to put your content onscreen, and you can also use it to manage interactions between your content and a person\u2019s surroundings. In visionOS, the system puts your content onscreen, so you only use ARKit to manage interactions with the surroundings. Because of this more limited usage, some apps don\u2019t need ARKit at all in visionOS. The only time you use ARKit in visionOS is when you need one of the following services: Plane detection Plane detection Image tracking Image tracking Scene reconstruction Scene reconstruction Hand tracking Hand tracking World tracking and device-pose prediction World tracking and device-pose prediction Use plane detection, image tracking, and scene reconstruction to facilitate interactions between your app\u2019s virtual content and real-world items. For example, use plane detection to detect a tabletop on which to place your content. Use world tracking to record anchors that you want to persist between launches of your app. Use hand tracking if your app requires custom hands-based input. To start ARKit services in your app, create an ARKitSession object and run it with the data providers for each service. Unlike ARKit in iOS, services in visionOS are independent of one another, and you can start and stop each one at any time. The following example shows how to detect horizontal and vertical planes. Data providers deliver new information using an asynchronous sequence. If you use the world-tracking data provider in visionOS, ARKit automatically persists the anchors you add to your app\u2019s content. You don\u2019t need to persist these anchors yourself. For more information about how to use ARKit, see ARKit. If your app uses ARKit features that aren\u2019t present in visionOS, isolate that code to the iOS version of your app. The following features are available in iOS, but don\u2019t have an equivalent in visionOS: Face tracking Face tracking Body tracking Body tracking Geotracking and placing anchors using a latitude and longitude Geotracking and placing anchors using a latitude and longitude Object detection Object detection App Clip Code detection App Clip Code detection Video frame post-processing Video frame post-processing Although whole body tracking isn\u2019t available in visionOS, you can track the hands of the person wearing the device. Hand gestures are an important way of interacting with content in visionOS. SwiftUI handles common types of interactions like taps and drags, but you can use custom hand tracking for more complex gestures your app supports. If you use ARKit raycasting in iOS to detect interactions with objects in the person\u2019s surroundings, you might not need that code in visionOS. SwiftUI and RealityKit handle both direct and indirect interactions with your app\u2019s content in 3D space, eliminating the need for raycasting in many situations. In other situations, you can use the features of ARKit and RealityKit to manage interactions with your content. For example, you might use ARKit hand tracking to determine where someone is pointing in the scene, and use scene reconstruction to build a mesh you can integrate into your RealityKit content. Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC\n\n\n\nlet session = ARKitSession()\nlet planeData = PlaneDetectionProvider(alignments: [.horizontal, .vertical])\n\n\nTask {\n    try await session.run([planeData])\n    \n    for await update in planeData.anchorUpdates {\n        switch update.event {\n        case .added, .updated:\n            // Update plane representation.\n            print(\"Updated planes.\")\n        case .removed:\n            // Indicate plane removal.\n            print(\"Removed plane.\")\n        }\n    }\n}", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/checking-whether-your-app-is-compatible-with-visionos", "title": "Checking whether your existing app is compatible with visionOS | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode / visionOS / Checking whether your existing app is compatible with visionOS API Changes: None visionOS runs compatible iPad and iPhone apps to provide continuous access to existing content right away. visionOS supports most of the same technologies as iOS, so many apps built to run on iPad or iPhone can run unmodified on visionOS devices. When a compatible app runs in visionOS, it retains the same appearance it had in iPadOS or iOS, and its content appears in a window in the person\u2019s surroundings. If you have an app in the iOS App Store, try downloading it and running it on Apple Vision Pro. If you run into issues, use Xcode to identify and fix them. If you built your app using the iOS SDK, Xcode 15 and later automatically adds a Designed for iPad runtime destination to your project. Use this destination to run your app and test its compatibility in visionOS. You can test most of your app\u2019s core functionality in Simulator, but some features are available only on a device. visionOS contains most of the same technologies as iPadOS and iOS, but there are some differences. In some cases, a feature you use in your app might not be available because of hardware differences or because of differences in how people use a visionOS device. As part of your testing, consider the impact of any missing features on your app\u2019s overall experience. Whenever possible, work around missing features by disabling them or providing alternate ways to access the same content. The following features aren\u2019t available in compatible iPad and iPhone apps in visionOS. Use framework APIs to determine when the features are available. Core Motion services Core Motion services Barometer and magnetometer data Barometer and magnetometer data All location services except the standard service All location services except the standard service HealthKit data HealthKit data Video or still-photo capture Video or still-photo capture Camera features like auto-focus or flash Camera features like auto-focus or flash Rear-facing (selfie) cameras Rear-facing (selfie) cameras In some cases, a framework or feature behaves differently when your app runs in visionOS. Be prepared to handle these differences when your app runs in visionOS. AirPlay. visionOS hides AirPlay sharing buttons in system interfaces, and you can\u2019t use AirPlay features from compatible apps. AirPlay. visionOS hides AirPlay sharing buttons in system interfaces, and you can\u2019t use AirPlay features from compatible apps. App extensions. visionOS doesn\u2019t load App Clips, device drivers, device activity monitors, keyboard extensions, Messages app extensions, photo-editing app extensions, SMS and call-reporting extensions, or widgets. App extensions. visionOS doesn\u2019t load App Clips, device drivers, device activity monitors, keyboard extensions, Messages app extensions, photo-editing app extensions, SMS and call-reporting extensions, or widgets. Apple Watch features. visionOS ignores watchOS apps and WatchKit extensions in your iOS or iPadOS app. The Watch Connectivity framework is unavailable. Face sharing in ClockKit does nothing in visionOS. Apple Watch features. visionOS ignores watchOS apps and WatchKit extensions in your iOS or iPadOS app. The Watch Connectivity framework is unavailable. Face sharing in ClockKit does nothing in visionOS. Audio and video. visionOS doesn\u2019t support Picture in Picture or AV routing features. Check the availability of video features before using them. Be prepared for audio playback to stop automatically when your app moves to the background. Audio and video. visionOS doesn\u2019t support Picture in Picture or AV routing features. Check the availability of video features before using them. Be prepared for audio playback to stop automatically when your app moves to the background. Classroom features. Starting a test with Automatic Assessment Configuration reports an error. Classroom features. Starting a test with Automatic Assessment Configuration reports an error. Cellular telephony. Cellular services are unavailable. You can still implement Voice-over-IP (VoIP) services using CallKit and Core Telephony. Cellular telephony. Cellular services are unavailable. You can still implement Voice-over-IP (VoIP) services using CallKit and Core Telephony. Device management. Calls to the ManagedSettings and ManagedSettingsUI frameworks do nothing. Device management. Calls to the ManagedSettings and ManagedSettingsUI frameworks do nothing. Game controllers. visionOS delivers game controller events only when someone is looking at the app. To require a game controller as an input device for your app, add the GCRequiresControllerUserInteraction key with the visionOS value to your app\u2019s Info.plist. Game controllers. visionOS delivers game controller events only when someone is looking at the app. To require a game controller as an input device for your app, add the GCRequiresControllerUserInteraction key with the visionOS value to your app\u2019s Info.plist. Handoff. visionOS doesn\u2019t attempt to hand off user activities to other devices. Handoff. visionOS doesn\u2019t attempt to hand off user activities to other devices. Haptics. visionOS plays sounds instead of haptics. Haptics. visionOS plays sounds instead of haptics. HomeKit. You can\u2019t add accessories using a QR code from a visionOS device. HomeKit. You can\u2019t add accessories using a QR code from a visionOS device. Metrics. You can use MetricKit to gather on-device diagnostic logs and generate reports, but you can\u2019t gather metrics. Metrics. You can use MetricKit to gather on-device diagnostic logs and generate reports, but you can\u2019t gather metrics. Multi-Touch. The system reports a maximum of two simultaneous touch inputs \u2014\u00a0one for each of the person\u2019s hands. All system gesture recognizers handle these inputs correctly, including for zoom and rotation gestures that require multiple fingers. If you have custom gesture recognizers that require more than two points of interaction, update them to support only one or two touches in visionOS. Multi-Touch. The system reports a maximum of two simultaneous touch inputs \u2014\u00a0one for each of the person\u2019s hands. All system gesture recognizers handle these inputs correctly, including for zoom and rotation gestures that require multiple fingers. If you have custom gesture recognizers that require more than two points of interaction, update them to support only one or two touches in visionOS. Parental controls. Calls to the FamilyControls framework do nothing. Parental controls. Calls to the FamilyControls framework do nothing. PencilKit. visionOS doesn\u2019t report touches of type UITouch.TouchType.pencil, but it does report other types of touches. PencilKit. visionOS doesn\u2019t report touches of type UITouch.TouchType.pencil, but it does report other types of touches. Push to Talk. Calls to the Push to Talk framework do nothing. Push to Talk. Calls to the Push to Talk framework do nothing. Safari Services. Links that present an SFSafariViewController open a Safari scene instead. Safari Services. Links that present an SFSafariViewController open a Safari scene instead. ScreenTime. Calls to the Screen Time framework do nothing. ScreenTime. Calls to the Screen Time framework do nothing. Sensor-related features. Calls to the SensorKit framework do nothing. Sensor-related features. Calls to the SensorKit framework do nothing. Social media. Calls to the Social framework do nothing. Social media. Calls to the Social framework do nothing. System interfaces. Authorization prompts, Sign in with Apple prompts, and other system-provided interfaces run asynchronously outside of your app\u2019s process. Because these interfaces don\u2019t run modally in your app, your app might not receive immediate responses. System interfaces. Authorization prompts, Sign in with Apple prompts, and other system-provided interfaces run asynchronously outside of your app\u2019s process. Because these interfaces don\u2019t run modally in your app, your app might not receive immediate responses. Vehicle features. The system doesn\u2019t call your app\u2019s CarPlay code. Calls you make using CarKey do nothing. Vehicle features. The system doesn\u2019t call your app\u2019s CarPlay code. Calls you make using CarKey do nothing. Vision. Data scanners do nothing in VisionKit. Vision. Data scanners do nothing in VisionKit. The version of ARKit in iOS is incompatible with the one in visionOS and visionOS can\u2019t display windows that contain ARKit views. For information about how to bring an ARKit app to visionOS, see Bringing your ARKit app to visionOS. For details about how to handle missing features in your code, see Making your existing app compatible with visionOS. The following App Store features for iOS continue to work when your app runs in visionOS: In-app purchases and subscriptions In-app purchases and subscriptions App capabilities and entitlements App capabilities and entitlements On-demand resources On-demand resources App thinning App thinning When you use app thinning to optimize your app for different devices and operating systems, the App Store selects the resources and content that offer the best fit for visionOS devices. It then removes any other resources to create a streamlined installation of your app. When you export your app from Xcode 15 or later, you can test the thinning support using the visionOS virtual thinning target. When you\u2019re ready to distribute your app, create an archive and export it using the Ad-Hoc or Development distribution method. During the export process, Xcode creates an appropriately signed app for you to distribute to your testers. For more information, see Distributing your app to registered devices. The App Store makes compatible iPad and iPhone apps available in visionOS automatically after you sign the updated Apple Developer Program License Agreement. If you don\u2019t want your app to run on Apple Vision Pro, change your app\u2019s availability in App Store Connect. Select your app in App Store Connect. Select your app in App Store Connect. Navigate to the Pricing and Availability information. Navigate to the Pricing and Availability information. Disable the \u201cMake this app available on Apple Vision Pro\u201d option. Disable the \u201cMake this app available on Apple Vision Pro\u201d option. When you remove your app\u2019s availability for Apple Vision Pro, the App Store stops making your iOS app available for visionOS. People who already downloaded your iOS app can still run it in visionOS, but they can\u2019t download it again. This setting doesn\u2019t affect the version of your app built using the visionOS SDK. Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos/making-your-app-compatible-with-visionos", "title": "Making your existing app compatible with visionOS | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode Tracking specific points in world space Placing content on detected planes Incorporating real-world surroundings in an immersive experience Setting up access to ARKit data Happy Beam ARKit Capturing screenshots and video from Apple Vision Pro for 2D viewing Designing RealityKit content with Reality Composer Pro Understanding RealityKit\u2019s modular architecture Diorama Swift Splash RealityKit and Reality Composer Pro Positioning and sizing windows Presenting windows and spaces Hello World SwiftUI Improving accessibility support in your visionOS app Adopting best practices for privacy and user preferences Designing for visionOS Design Drawing sharp layer-based content in visionOS Creating fully immersive experiences in your app Adding 3D content to your app Creating your first visionOS app App construction  / visionOS / Making your existing app compatible with visionOS API Changes: None A compatible iPadOS or iOS app links against the iOS SDK and runs in visionOS. Although visionOS provides a complete set of iOS frameworks for linking, some features of those frameworks might be unavailable due to hardware or usage differences. To ensure your app runs correctly in visionOS, handle any missing features gracefully and provide workarounds wherever possible. Some frameworks offer APIs to let you determine when framework features are available or whether your app is authorized to use them. Always check these APIs before you try to use the corresponding features, and don\u2019t assume a feature is available because the necessary hardware is present. The device\u2019s configuration also plays a role in determining the results of some availability and authorization checks, and features might not be present when your app runs in Simulator. If an availability or authorization check fails, don\u2019t try to use the associated feature in your app. The following frameworks support availability or authorization checks: ActivityKit. Check the areActivitiesEnabled property of ActivityAuthorizationInfo to determine if Live Activities are authorized. ActivityKit. Check the areActivitiesEnabled property of ActivityAuthorizationInfo to determine if Live Activities are authorized. ARKit. Check the isSupported property of your configuration object to determine availability of augmented reality features. In visionOS, ARKit views such as  ARView are never available, so isolate interface code containing those views to the iOS version of your app. ARKit. Check the isSupported property of your configuration object to determine availability of augmented reality features. In visionOS, ARKit views such as  ARView are never available, so isolate interface code containing those views to the iOS version of your app. AVFoundation. Identify what cameras are available using the AVCaptureDevice.DiscoverySession class. Don\u2019t assume the presence of specific cameras. AVFoundation. Identify what cameras are available using the AVCaptureDevice.DiscoverySession class. Don\u2019t assume the presence of specific cameras. Automatic Assessment Configuration. Check for error values when you configure an AEAssessmentSession object. Automatic Assessment Configuration. Check for error values when you configure an AEAssessmentSession object. Contacts. Use the CNContactStore class to determine your app\u2019s authorization status. Contacts. Use the CNContactStore class to determine your app\u2019s authorization status. Core Bluetooth. Use the CBCentralManager and CBPeripheralManager classes to determine feature availability and your app\u2019s authorization status. Core Bluetooth. Use the CBCentralManager and CBPeripheralManager classes to determine feature availability and your app\u2019s authorization status. Core Haptics. Call the capabilitiesForHardware() method of the haptic engine to determine the available features. Core Haptics. Call the capabilitiesForHardware() method of the haptic engine to determine the available features. Core Location. Check the properties of CLLocationManager to determine the availability of location services. Core Location. Check the properties of CLLocationManager to determine the availability of location services. Core Motion. Check the properties of CMMotionManager to determine the availability of accelerometers, gyroscopes, magnetometers, and other hardware sensors. Core Motion. Check the properties of CMMotionManager to determine the availability of accelerometers, gyroscopes, magnetometers, and other hardware sensors. Core NFC. Check the readingAvailable property of your reader session to determine if NFC tag reading is available. Core NFC. Check the readingAvailable property of your reader session to determine if NFC tag reading is available. EventKit. Use the EKEventStore class to determine your app\u2019s authorization status. EventKit. Use the EKEventStore class to determine your app\u2019s authorization status. ExposureNotification. Use the ENManager class to determine your app\u2019s authorization status. ExposureNotification. Use the ENManager class to determine your app\u2019s authorization status. HealthKit. Use the HKHealthStore class to determine if health-related data is available. HealthKit. Use the HKHealthStore class to determine if health-related data is available. HomeKit. Check the properties of HMHomeManager to determine your app\u2019s authorization status. HomeKit. Check the properties of HMHomeManager to determine your app\u2019s authorization status. Local Authentication. Use the LAContext class to determine the authentication policies you can use. Local Authentication. Use the LAContext class to determine the authentication policies you can use. Media Player. Use the MPMediaLibrary class to determine your app\u2019s authorization status. Media Player. Use the MPMediaLibrary class to determine your app\u2019s authorization status. Nearby Interaction. Check the deviceCapabilities property of your session to determine whether features are available. Nearby Interaction. Check the deviceCapabilities property of your session to determine whether features are available. PhotoKit. Use the PHPhotoLibrary class to determine your app\u2019s authorization status. PhotoKit. Use the PHPhotoLibrary class to determine your app\u2019s authorization status. ProximityReader. Check the isSupported property of the card reader object to determine if Tap to Pay on iPhone is available. ProximityReader. Check the isSupported property of the card reader object to determine if Tap to Pay on iPhone is available. ReplayKit. Check the isAvailable property of RPScreenRecorder to determine if screen recording support is available. ReplayKit. Check the isAvailable property of RPScreenRecorder to determine if screen recording support is available. RoomPlan. Check the isSupported property of the RoomCaptureSession object to determine if LiDAR scanning is available on the device. RoomPlan. Check the isSupported property of the RoomCaptureSession object to determine if LiDAR scanning is available on the device. SensorKit. Use the SRSensorReader class to determine your app\u2019s authorization status. SensorKit. Use the SRSensorReader class to determine your app\u2019s authorization status. Speech. Use the SFSpeechRecognizer class to determine if speech recognition is available. Speech. Use the SFSpeechRecognizer class to determine if speech recognition is available. User Notifications. Use the getNotificationSettings(completionHandler:) method of UNUserNotificationCenter to determine your app\u2019s authorization status. User Notifications. Use the getNotificationSettings(completionHandler:) method of UNUserNotificationCenter to determine your app\u2019s authorization status. WatchConnectivity. Call the  isSupported() method of the WCSession object to determine if the framework is available. WatchConnectivity. Call the  isSupported() method of the WCSession object to determine if the framework is available. Apple frameworks take a device-agnostic approach whenever possible to minimize issues when you use them on different device types. Apple devices come in a variety of shapes and sizes, and with different sets of features. Rather than build your app for a specific device, make sure it adapts to any device and can gracefully handle differences. Build robustness into your app during the design process. Avoid assumptions that might cause your app to break when it runs on a new device, and make sure your app adapts dynamically to different conditions. For example: Don\u2019t assume the device type or idiom is always iPhone, iPad, or iPod Touch. Avoid decisions based on the current idiom. If you do rely on the current idiom, provide reasonable defaults for unknown idioms. Don\u2019t assume the device type or idiom is always iPhone, iPad, or iPod Touch. Avoid decisions based on the current idiom. If you do rely on the current idiom, provide reasonable defaults for unknown idioms. Design your app to handle unavailable hardware or features. Specific hardware and features might be unavailable for many different reasons. For example, a feature might be unavailable when your app runs in Simulator. Perform availability checks whenever possible, and handle missing features gracefully. Design your app to handle unavailable hardware or features. Specific hardware and features might be unavailable for many different reasons. For example, a feature might be unavailable when your app runs in Simulator. Perform availability checks whenever possible, and handle missing features gracefully. Design your windows and views to adapt dynamically. Build your interface to adapt dynamically to any size using SwiftUI or Auto Layout. Assume the size of your app can change dynamically. Design your windows and views to adapt dynamically. Build your interface to adapt dynamically to any size using SwiftUI or Auto Layout. Assume the size of your app can change dynamically. Don\u2019t assume the device has a specific number of displays. People can connect iPad and iPhone to an external display, and visionOS devices use two displays to create a stereoscopic version of your app\u2019s content. Don\u2019t assume the device has a specific number of displays. People can connect iPad and iPhone to an external display, and visionOS devices use two displays to create a stereoscopic version of your app\u2019s content. Don\u2019t make assumptions based on the available frameworks or symbols. The presence or absence of frameworks or code symbols is an unreliable way to identify a device type, and can change in later software updates. Don\u2019t make assumptions based on the available frameworks or symbols. The presence or absence of frameworks or code symbols is an unreliable way to identify a device type, and can change in later software updates. Don\u2019t assume your app runs in the background. visionOS doesn\u2019t support the location, external accessory, or Bluetooth-peripheral background execution modes. Don\u2019t assume your app runs in the background. visionOS doesn\u2019t support the location, external accessory, or Bluetooth-peripheral background execution modes. Don\u2019t assume that background apps are hidden. In visionOS, the windows of background apps remain visible, but are dimmed when no one looks at them. The only time app windows disappear is when one app presents an immersive space. Don\u2019t assume that background apps are hidden. In visionOS, the windows of background apps remain visible, but are dimmed when no one looks at them. The only time app windows disappear is when one app presents an immersive space. When you make decisions using device details, your app might produce inconsistent or erroneous results on an unknown device type, or it might fail altogether. Find solutions that rely on environmental information, rather than the device type. For example, SwiftUI and UIKit start layout using the app\u2019s window size, which isn\u2019t necessarily the same size as the device\u2019s display. Note Device-specific information is available when you absolutely need it, but validate the information you receive and provide reasonable default behavior for unexpected values. To minimize disruptions, visionOS runs your compatible iPad or iPhone app in an environment that matches an iPad as much as possible. Windows and views retain the same appearance that they have in iPadOS or iOS, and the system sizes your app\u2019s window to fit an iPad whenever possible. When building your app\u2019s interface, make choices that ensure your app runs well in visionOS too. Adopt the following best practices for your interface-related code: Support iPad and iPhone in the same app. Create one app that supports both device types, rather than separate apps for each device. SwiftUI and UIKit support adaptable interfaces, and Xcode provides tools to help you visualize your interface at different supported sizes. Support iPad and iPhone in the same app. Create one app that supports both device types, rather than separate apps for each device. SwiftUI and UIKit support adaptable interfaces, and Xcode provides tools to help you visualize your interface at different supported sizes. Organize your interface using scenes. Scenes are a fundamental tool for managing your app\u2019s interface. Use the scene types in SwiftUI and UIKit to assemble and manage the views you display in windows. Organize your interface using scenes. Scenes are a fundamental tool for managing your app\u2019s interface. Use the scene types in SwiftUI and UIKit to assemble and manage the views you display in windows. Adapt your interface to any size. Design your interface to adapt naturally to different sizes. For an introduction to SwiftUI views and layout, see Declaring a custom view. For information about laying out views in UIKit, see View layout. Adapt your interface to any size. Design your interface to adapt naturally to different sizes. For an introduction to SwiftUI views and layout, see Declaring a custom view. For information about laying out views in UIKit, see View layout. Don\u2019t access screen details. visionOS provides reasonable values for UIScreen objects, but don\u2019t use those values to make decisions. Don\u2019t access screen details. visionOS provides reasonable values for UIScreen objects, but don\u2019t use those values to make decisions. Specify the supported interface orientations. Add the UISupportedInterfaceOrientations key to your app\u2019s Info.plist file to specify the interface orientations it supports. Support all interface orientations whenever possible. visionOS adds an interface rotation for your app button only when this key is present. Specify the supported interface orientations. Add the UISupportedInterfaceOrientations key to your app\u2019s Info.plist file to specify the interface orientations it supports. Support all interface orientations whenever possible. visionOS adds an interface rotation for your app button only when this key is present. Update hover effects in custom views. Hover effects convey the focused view or control in your interface. Standard system views apply hover effects as needed. For custom views and controls, verify that the hover effects look appropriate in visionOS. Add or update the content shape for your hover effects if needed. Update hover effects in custom views. Hover effects convey the focused view or control in your interface. Standard system views apply hover effects as needed. For custom views and controls, verify that the hover effects look appropriate in visionOS. Add or update the content shape for your hover effects if needed. Adopt vector-based images when possible. Vector-based images scale well to different sizes while retaining a crisp appearance. If you use bitmap-based assets, make them the exact size you need. Don\u2019t use oversized assets, which require extra work to display at the correct size. Adopt vector-based images when possible. Vector-based images scale well to different sizes while retaining a crisp appearance. If you use bitmap-based assets, make them the exact size you need. Don\u2019t use oversized assets, which require extra work to display at the correct size. If you want visionOS to display your app\u2019s interface in a particular orientation at launch, add the UIPreferredDefaultInterfaceOrientation key to your app\u2019s Info.plist file. Set the value of the key to one of the values in your app\u2019s UISupportedInterfaceOrientations key. For example, to specify a preference for a portrait orientation, set the value to UIInterfaceOrientationPortrait. Add ~ipad or ~iphone to the key name to specify device-specific orientation preferences. If your app relies on frameworks that behave differently in visionOS, update your code to handle those differences. Availability checks give you a clear indication when you can\u2019t use a feature, but some frameworks might have more subtle behavior. Throughout your code, make sure you respond to unusual situations: Handle error conditions. If a function throws an exception or returns an error, handle the error. Use error information to adjust your app\u2019s behavior or provide an explanation of why it can\u2019t perform certain operations. Handle error conditions. If a function throws an exception or returns an error, handle the error. Use error information to adjust your app\u2019s behavior or provide an explanation of why it can\u2019t perform certain operations. Handle nil or empty values gracefully. Validate objects and return values before you try to use them. Handle nil or empty values gracefully. Validate objects and return values before you try to use them. Update your interface. Provide appropriate messaging in your interface when a feature is missing, or remove feature-specific views entirely if you can do so cleanly. Don\u2019t leave empty views where the feature was. Update your interface. Provide appropriate messaging in your interface when a feature is missing, or remove feature-specific views entirely if you can do so cleanly. Don\u2019t leave empty views where the feature was. For information about frameworks that behave differently in visionOS, see Checking whether your existing app is compatible with visionOS. If your app currently uses deprecated APIs or frameworks, update your code to use appropriate replacements. Deprecated symbols represent outdated features, and in some cases might not do anything when you call them. To prevent potential issues, replace them with modern equivalents to ensure your code behaves as expected. The following frameworks are deprecated in their entirety in iPadOS, iOS, and visionOS. If your app still uses these frameworks, move off of them immediately. The reference documentation for each framework includes information about how to update your code. Accounts Accounts Address Book Address Book Address Book UI Address Book UI Assets Library Assets Library iAd iAd Newsstand Kit Newsstand Kit NotificationCenter NotificationCenter OpenGL ES OpenGL ES Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos#app-top", "title": "visionOS | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space  / visionOS API Changes: None visionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.  Developing for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app\u2019s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS. Start with a familiar window-based experience to introduce people to your content. From there, add SwiftUI scene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences. Build your app\u2019s 3D content with RealityKit and Reality Composer Pro, and display it with a RealityView. In an immersive experience, use ARKit to integrate your content with the person\u2019s surroundings.  People can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures. SwiftUI provides built-in support for these standard gestures, so rely on them for most of your app\u2019s input. When you want to go beyond the standard gestures, use ARKit to create custom gestures. Tap to select Pinch to rotate Manipulate objects Create custom gestures Explore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash. visionOS Overview Expand your app into immersive spaces Explore new kinds of interaction Dive into featured sample apps Topics Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos#Overview", "title": "visionOS | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space  / visionOS API Changes: None visionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.  Developing for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app\u2019s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS. Start with a familiar window-based experience to introduce people to your content. From there, add SwiftUI scene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences. Build your app\u2019s 3D content with RealityKit and Reality Composer Pro, and display it with a RealityView. In an immersive experience, use ARKit to integrate your content with the person\u2019s surroundings.  People can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures. SwiftUI provides built-in support for these standard gestures, so rely on them for most of your app\u2019s input. When you want to go beyond the standard gestures, use ARKit to create custom gestures. Tap to select Pinch to rotate Manipulate objects Create custom gestures Explore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash. visionOS Overview Expand your app into immersive spaces Explore new kinds of interaction Dive into featured sample apps Topics Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos#Expand-your-app-into-immersive-spaces", "title": "visionOS | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space  / visionOS API Changes: None visionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.  Developing for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app\u2019s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS. Start with a familiar window-based experience to introduce people to your content. From there, add SwiftUI scene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences. Build your app\u2019s 3D content with RealityKit and Reality Composer Pro, and display it with a RealityView. In an immersive experience, use ARKit to integrate your content with the person\u2019s surroundings.  People can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures. SwiftUI provides built-in support for these standard gestures, so rely on them for most of your app\u2019s input. When you want to go beyond the standard gestures, use ARKit to create custom gestures. Tap to select Pinch to rotate Manipulate objects Create custom gestures Explore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash. visionOS Overview Expand your app into immersive spaces Explore new kinds of interaction Dive into featured sample apps Topics Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos#Explore-new-kinds-of-interaction", "title": "visionOS | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space  / visionOS API Changes: None visionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.  Developing for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app\u2019s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS. Start with a familiar window-based experience to introduce people to your content. From there, add SwiftUI scene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences. Build your app\u2019s 3D content with RealityKit and Reality Composer Pro, and display it with a RealityView. In an immersive experience, use ARKit to integrate your content with the person\u2019s surroundings.  People can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures. SwiftUI provides built-in support for these standard gestures, so rely on them for most of your app\u2019s input. When you want to go beyond the standard gestures, use ARKit to create custom gestures. Tap to select Pinch to rotate Manipulate objects Create custom gestures Explore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash. visionOS Overview Expand your app into immersive spaces Explore new kinds of interaction Dive into featured sample apps Topics Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos#Dive-into-featured-sample-apps", "title": "visionOS | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space  / visionOS API Changes: None visionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.  Developing for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app\u2019s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS. Start with a familiar window-based experience to introduce people to your content. From there, add SwiftUI scene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences. Build your app\u2019s 3D content with RealityKit and Reality Composer Pro, and display it with a RealityView. In an immersive experience, use ARKit to integrate your content with the person\u2019s surroundings.  People can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures. SwiftUI provides built-in support for these standard gestures, so rely on them for most of your app\u2019s input. When you want to go beyond the standard gestures, use ARKit to create custom gestures. Tap to select Pinch to rotate Manipulate objects Create custom gestures Explore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash. visionOS Overview Expand your app into immersive spaces Explore new kinds of interaction Dive into featured sample apps Topics Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC", "code_examples": []}, {"url": "https://developer.apple.com/documentation/visionos#topics", "title": "visionOS | Apple Developer Documentation", "content": "Global Nav Open MenuGlobal Nav Close Menu Apple Developer Apple Developer News Discover Design Develop Distribute Support Account  Downloads Documentation Videos Forums Xcode App construction Creating your first visionOS app Adding 3D content to your app Creating fully immersive experiences in your app Drawing sharp layer-based content in visionOS Design Designing for visionOS Adopting best practices for privacy and user preferences Improving accessibility support in your visionOS app SwiftUI Hello World Presenting windows and spaces Positioning and sizing windows RealityKit and Reality Composer Pro Swift Splash Diorama Understanding RealityKit\u2019s modular architecture Designing RealityKit content with Reality Composer Pro Capturing screenshots and video from Apple Vision Pro for 2D viewing ARKit Happy Beam Setting up access to ARKit data Incorporating real-world surroundings in an immersive experience Placing content on detected planes Tracking specific points in world space  / visionOS API Changes: None visionOS is the operating system that powers Apple Vision Pro. Use visionOS together with familiar tools and technologies to build immersive apps and games for spatial computing.  Developing for visionOS requires a Mac with Apple silicon. Create new apps using SwiftUI to take full advantage of the spectrum of immersion available in visionOS. If you have an existing iPad or iPhone app, add the visionOS destination to your app\u2019s target to gain access to the standard system appearance, and add platform-specific features to create a compelling experience. To provide continuous access to your content in the meantime, deliver a compatible version of your app that runs in visionOS. Start with a familiar window-based experience to introduce people to your content. From there, add SwiftUI scene types specific to visionOS, such as volumes and spaces. These scene types let you incorporate depth, 3D objects, and immersive experiences. Build your app\u2019s 3D content with RealityKit and Reality Composer Pro, and display it with a RealityView. In an immersive experience, use ARKit to integrate your content with the person\u2019s surroundings.  People can select an element by looking at it and tapping their fingers together. They can also pinch, drag, zoom, and rotate objects using specific hand gestures. SwiftUI provides built-in support for these standard gestures, so rely on them for most of your app\u2019s input. When you want to go beyond the standard gestures, use ARKit to create custom gestures. Tap to select Pinch to rotate Manipulate objects Create custom gestures Explore the core concepts for all visionOS apps with Hello World. Understand how to detect custom gestures using ARKit with Happy Beam. Discover streaming 2D and stereoscopic media with Destination Video. And learn how to build 3D scenes with RealityKit and Reality Composer Pro with Diorama and Swift Splash. visionOS Overview Expand your app into immersive spaces Explore new kinds of interaction Dive into featured sample apps Topics Developer Documentation iOS iPadOS macOS tvOS visionOS watchOS Swift SwiftUI Swift Playgrounds TestFlight Xcode Xcode Cloud SF Symbols Accessibility Accessories App Extension App Store Audio & Video Augmented Reality Business Design Distribution Education Fonts Games Health & Fitness In-App Purchase Localization Maps & Location Machine Learning Security Safari & Web  Documentation Curriculum Downloads Forums Videos Support Articles Contact Us Bug Reporting System Status Apple Developer App Store Connect Certificates, IDs, & Profiles Feedback Assistant Apple Developer Program Apple Developer Enterprise Program App Store Small Business Program MFi Program News Partner Program Video Partner Program Security Bounty Program Security Research Device Program Events Overview App Accelerators App Store Awards Apple Design Awards Apple Developer Academies Entrepreneur Camp WWDC", "code_examples": []}]}